{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T48_v3 - Recursive FBSDE (terminal-first, normalized)\n",
    "Questa versione introduce normalizzazione input per blocco, schedule terminale aggressivo e refinement automatico.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e5b60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Recursive training (backward in time) ===\n",
      "[RecursiveConfig] label=main:terminal_first, N_total=480, n_blocks=24 (req=24), min_steps_per_block=12, terminal_min_steps=28, iters_per_block=280, lr=8.0e-05, warm_start=True, train_scen=8, eval_scen=3\n",
      "[RecursivePartition] label=main:terminal_first, steps=[20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 12, 28]\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "\n",
      "[InitOutputBias] main:terminal_first b=23, bias_old=0.000, bias_target=-70.715, bias_new=-70.715\n",
      "[RecursiveBlock] main:terminal_first b=23, idx=[452,480], steps=28, terminal=g, iters=2800, lr=2.8e-05\n",
      "[TerminalSchedule] main:terminal_first b=23: stage1(iters=1540, lr=4.5e-05), stage2(iters=1260, lr=2.8e-05)\n",
      "It: 0, Loss: 1.080e+07, Y0_mean: -70.712, Y0_std: 0.011, Time: 7.75, Learning Rate: 4.480e-05\n",
      "It: 10, Loss: 1.080e+07, Y0_mean: -70.712, Y0_std: 0.022, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 20, Loss: 1.069e+07, Y0_mean: -70.710, Y0_std: 0.045, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 30, Loss: 1.080e+07, Y0_mean: -70.709, Y0_std: 0.072, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 40, Loss: 1.080e+07, Y0_mean: -70.707, Y0_std: 0.104, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 50, Loss: 1.096e+07, Y0_mean: -70.704, Y0_std: 0.148, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 60, Loss: 1.068e+07, Y0_mean: -70.704, Y0_std: 0.193, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 70, Loss: 1.063e+07, Y0_mean: -70.704, Y0_std: 0.249, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 80, Loss: 1.063e+07, Y0_mean: -70.702, Y0_std: 0.318, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 90, Loss: 1.066e+07, Y0_mean: -70.700, Y0_std: 0.400, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 100, Loss: 1.051e+07, Y0_mean: -70.698, Y0_std: 0.479, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 110, Loss: 1.076e+07, Y0_mean: -70.697, Y0_std: 0.599, Time: 0.77, Learning Rate: 4.480e-05\n",
      "It: 120, Loss: 1.126e+07, Y0_mean: -70.693, Y0_std: 0.750, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 130, Loss: 1.091e+07, Y0_mean: -70.689, Y0_std: 0.895, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 140, Loss: 9.964e+06, Y0_mean: -70.699, Y0_std: 1.000, Time: 0.77, Learning Rate: 4.480e-05\n",
      "It: 150, Loss: 1.060e+07, Y0_mean: -70.692, Y0_std: 1.219, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 160, Loss: 1.121e+07, Y0_mean: -70.684, Y0_std: 1.461, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 170, Loss: 1.128e+07, Y0_mean: -70.690, Y0_std: 1.674, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 180, Loss: 1.117e+07, Y0_mean: -70.679, Y0_std: 1.954, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 190, Loss: 1.124e+07, Y0_mean: -70.687, Y0_std: 2.211, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 200, Loss: 9.865e+06, Y0_mean: -70.695, Y0_std: 2.398, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 210, Loss: 1.119e+07, Y0_mean: -70.680, Y0_std: 2.858, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 220, Loss: 1.073e+07, Y0_mean: -70.651, Y0_std: 3.264, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 230, Loss: 1.070e+07, Y0_mean: -70.641, Y0_std: 3.672, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 240, Loss: 1.051e+07, Y0_mean: -70.656, Y0_std: 3.974, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 250, Loss: 1.032e+07, Y0_mean: -70.659, Y0_std: 4.454, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 260, Loss: 1.044e+07, Y0_mean: -70.633, Y0_std: 4.936, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 270, Loss: 1.015e+07, Y0_mean: -70.619, Y0_std: 5.337, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 280, Loss: 1.025e+07, Y0_mean: -70.598, Y0_std: 6.128, Time: 0.77, Learning Rate: 4.480e-05\n",
      "It: 290, Loss: 9.573e+06, Y0_mean: -70.618, Y0_std: 6.513, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 300, Loss: 1.026e+07, Y0_mean: -70.545, Y0_std: 7.353, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 310, Loss: 1.069e+07, Y0_mean: -70.470, Y0_std: 8.377, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 320, Loss: 1.015e+07, Y0_mean: -70.471, Y0_std: 8.839, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 330, Loss: 1.009e+07, Y0_mean: -70.424, Y0_std: 9.663, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 340, Loss: 9.773e+06, Y0_mean: -70.379, Y0_std: 10.259, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 350, Loss: 9.956e+06, Y0_mean: -70.309, Y0_std: 11.479, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 360, Loss: 9.727e+06, Y0_mean: -70.269, Y0_std: 12.528, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 370, Loss: 9.648e+06, Y0_mean: -70.189, Y0_std: 13.593, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 380, Loss: 9.564e+06, Y0_mean: -70.099, Y0_std: 14.721, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 390, Loss: 9.631e+06, Y0_mean: -69.959, Y0_std: 15.849, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 400, Loss: 9.652e+06, Y0_mean: -69.692, Y0_std: 17.743, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 410, Loss: 9.440e+06, Y0_mean: -69.722, Y0_std: 18.447, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 420, Loss: 8.657e+06, Y0_mean: -69.746, Y0_std: 19.332, Time: 0.77, Learning Rate: 4.480e-05\n",
      "It: 430, Loss: 9.328e+06, Y0_mean: -69.239, Y0_std: 22.126, Time: 0.77, Learning Rate: 4.480e-05\n",
      "It: 440, Loss: 9.208e+06, Y0_mean: -69.061, Y0_std: 23.740, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 450, Loss: 8.335e+06, Y0_mean: -69.302, Y0_std: 23.887, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 460, Loss: 8.771e+06, Y0_mean: -68.907, Y0_std: 26.594, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 470, Loss: 8.095e+06, Y0_mean: -68.945, Y0_std: 27.344, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 480, Loss: 7.967e+06, Y0_mean: -68.753, Y0_std: 29.200, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 490, Loss: 8.214e+06, Y0_mean: -68.298, Y0_std: 31.124, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 500, Loss: 7.695e+06, Y0_mean: -68.326, Y0_std: 33.197, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 510, Loss: 8.520e+06, Y0_mean: -67.512, Y0_std: 37.733, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 520, Loss: 7.904e+06, Y0_mean: -67.523, Y0_std: 39.070, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 530, Loss: 7.595e+06, Y0_mean: -67.262, Y0_std: 39.884, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 540, Loss: 8.002e+06, Y0_mean: -66.616, Y0_std: 45.216, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 550, Loss: 7.819e+06, Y0_mean: -66.311, Y0_std: 47.931, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 560, Loss: 7.315e+06, Y0_mean: -66.386, Y0_std: 48.851, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 570, Loss: 6.891e+06, Y0_mean: -66.109, Y0_std: 50.297, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 580, Loss: 7.380e+06, Y0_mean: -65.732, Y0_std: 55.926, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 590, Loss: 6.645e+06, Y0_mean: -65.572, Y0_std: 58.328, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 600, Loss: 6.384e+06, Y0_mean: -65.379, Y0_std: 61.175, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 610, Loss: 6.344e+06, Y0_mean: -65.001, Y0_std: 64.217, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 620, Loss: 6.094e+06, Y0_mean: -64.189, Y0_std: 70.010, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 630, Loss: 6.311e+06, Y0_mean: -64.455, Y0_std: 72.857, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 640, Loss: 5.544e+06, Y0_mean: -64.499, Y0_std: 75.257, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 650, Loss: 5.042e+06, Y0_mean: -64.870, Y0_std: 76.466, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 660, Loss: 5.619e+06, Y0_mean: -64.053, Y0_std: 84.561, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 670, Loss: 5.243e+06, Y0_mean: -63.434, Y0_std: 90.158, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 680, Loss: 4.411e+06, Y0_mean: -64.763, Y0_std: 88.352, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 690, Loss: 4.592e+06, Y0_mean: -64.046, Y0_std: 95.413, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 700, Loss: 4.524e+06, Y0_mean: -63.612, Y0_std: 103.767, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 710, Loss: 4.026e+06, Y0_mean: -64.639, Y0_std: 105.374, Time: 0.77, Learning Rate: 4.480e-05\n",
      "It: 720, Loss: 3.552e+06, Y0_mean: -65.550, Y0_std: 106.065, Time: 0.77, Learning Rate: 4.480e-05\n",
      "It: 730, Loss: 3.805e+06, Y0_mean: -64.553, Y0_std: 118.655, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 740, Loss: 3.328e+06, Y0_mean: -65.152, Y0_std: 123.183, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 750, Loss: 3.068e+06, Y0_mean: -66.414, Y0_std: 124.986, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 760, Loss: 3.225e+06, Y0_mean: -67.227, Y0_std: 132.592, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 770, Loss: 2.807e+06, Y0_mean: -67.599, Y0_std: 135.354, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 780, Loss: 2.604e+06, Y0_mean: -68.551, Y0_std: 140.972, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 790, Loss: 2.156e+06, Y0_mean: -70.696, Y0_std: 142.211, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 800, Loss: 1.987e+06, Y0_mean: -71.874, Y0_std: 147.921, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 810, Loss: 1.963e+06, Y0_mean: -72.741, Y0_std: 159.646, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 820, Loss: 1.732e+06, Y0_mean: -72.241, Y0_std: 161.244, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 830, Loss: 1.674e+06, Y0_mean: -75.605, Y0_std: 172.144, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 840, Loss: 1.520e+06, Y0_mean: -76.818, Y0_std: 182.881, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 850, Loss: 1.573e+06, Y0_mean: -78.348, Y0_std: 190.659, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 860, Loss: 1.467e+06, Y0_mean: -79.842, Y0_std: 189.527, Time: 0.77, Learning Rate: 4.480e-05\n",
      "It: 870, Loss: 1.419e+06, Y0_mean: -82.056, Y0_std: 202.137, Time: 0.77, Learning Rate: 4.480e-05\n",
      "It: 880, Loss: 1.231e+06, Y0_mean: -84.355, Y0_std: 204.267, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 890, Loss: 1.306e+06, Y0_mean: -84.766, Y0_std: 207.842, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 900, Loss: 1.087e+06, Y0_mean: -86.325, Y0_std: 194.221, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 910, Loss: 1.210e+06, Y0_mean: -88.249, Y0_std: 205.026, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 920, Loss: 1.004e+06, Y0_mean: -84.513, Y0_std: 196.982, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 930, Loss: 9.629e+05, Y0_mean: -84.732, Y0_std: 197.059, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 940, Loss: 1.055e+06, Y0_mean: -87.022, Y0_std: 201.214, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 950, Loss: 1.022e+06, Y0_mean: -88.753, Y0_std: 209.184, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 960, Loss: 9.809e+05, Y0_mean: -89.386, Y0_std: 209.129, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 970, Loss: 8.155e+05, Y0_mean: -85.936, Y0_std: 197.290, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 980, Loss: 7.838e+05, Y0_mean: -89.396, Y0_std: 201.745, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 990, Loss: 8.635e+05, Y0_mean: -88.331, Y0_std: 201.456, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1000, Loss: 7.191e+05, Y0_mean: -89.117, Y0_std: 202.474, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1010, Loss: 7.833e+05, Y0_mean: -91.686, Y0_std: 206.393, Time: 0.77, Learning Rate: 4.480e-05\n",
      "It: 1020, Loss: 7.673e+05, Y0_mean: -90.545, Y0_std: 209.557, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1030, Loss: 7.308e+05, Y0_mean: -88.502, Y0_std: 201.599, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1040, Loss: 6.990e+05, Y0_mean: -88.610, Y0_std: 201.547, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1050, Loss: 6.747e+05, Y0_mean: -90.831, Y0_std: 209.670, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1060, Loss: 5.753e+05, Y0_mean: -91.962, Y0_std: 203.605, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1070, Loss: 6.129e+05, Y0_mean: -88.445, Y0_std: 202.384, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1080, Loss: 4.999e+05, Y0_mean: -85.897, Y0_std: 199.218, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1090, Loss: 5.352e+05, Y0_mean: -92.769, Y0_std: 207.620, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1100, Loss: 4.736e+05, Y0_mean: -92.029, Y0_std: 204.136, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1110, Loss: 4.314e+05, Y0_mean: -89.369, Y0_std: 203.401, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1120, Loss: 4.873e+05, Y0_mean: -87.927, Y0_std: 202.594, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1130, Loss: 4.639e+05, Y0_mean: -88.052, Y0_std: 203.011, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1140, Loss: 3.798e+05, Y0_mean: -94.066, Y0_std: 210.129, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1150, Loss: 3.355e+05, Y0_mean: -90.184, Y0_std: 196.870, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1160, Loss: 3.480e+05, Y0_mean: -91.697, Y0_std: 205.345, Time: 0.77, Learning Rate: 4.480e-05\n",
      "It: 1170, Loss: 3.551e+05, Y0_mean: -92.490, Y0_std: 209.563, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1180, Loss: 3.731e+05, Y0_mean: -90.703, Y0_std: 212.358, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1190, Loss: 3.195e+05, Y0_mean: -93.408, Y0_std: 210.619, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1200, Loss: 2.569e+05, Y0_mean: -89.990, Y0_std: 197.824, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 1210, Loss: 2.592e+05, Y0_mean: -88.005, Y0_std: 206.097, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 1220, Loss: 2.461e+05, Y0_mean: -88.376, Y0_std: 206.451, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 1230, Loss: 2.861e+05, Y0_mean: -87.068, Y0_std: 205.645, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 1240, Loss: 2.226e+05, Y0_mean: -82.891, Y0_std: 203.960, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1250, Loss: 2.089e+05, Y0_mean: -93.131, Y0_std: 213.532, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1260, Loss: 2.004e+05, Y0_mean: -82.749, Y0_std: 204.774, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1270, Loss: 1.930e+05, Y0_mean: -90.761, Y0_std: 209.182, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 1280, Loss: 2.027e+05, Y0_mean: -91.414, Y0_std: 213.600, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 1290, Loss: 2.234e+05, Y0_mean: -89.025, Y0_std: 215.429, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1300, Loss: 1.828e+05, Y0_mean: -91.228, Y0_std: 214.783, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1310, Loss: 2.032e+05, Y0_mean: -89.067, Y0_std: 216.077, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 1320, Loss: 1.458e+05, Y0_mean: -81.786, Y0_std: 208.067, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 1330, Loss: 1.409e+05, Y0_mean: -86.810, Y0_std: 211.115, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 1340, Loss: 1.786e+05, Y0_mean: -88.912, Y0_std: 217.265, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 1350, Loss: 1.219e+05, Y0_mean: -92.974, Y0_std: 218.329, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1360, Loss: 1.229e+05, Y0_mean: -86.129, Y0_std: 212.466, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 1370, Loss: 1.110e+05, Y0_mean: -92.774, Y0_std: 219.286, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 1380, Loss: 1.018e+05, Y0_mean: -87.749, Y0_std: 205.174, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1390, Loss: 1.231e+05, Y0_mean: -91.093, Y0_std: 219.942, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1400, Loss: 9.906e+04, Y0_mean: -93.309, Y0_std: 222.473, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1410, Loss: 1.013e+05, Y0_mean: -85.838, Y0_std: 215.822, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1420, Loss: 1.318e+05, Y0_mean: -89.110, Y0_std: 222.091, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1430, Loss: 1.277e+05, Y0_mean: -89.126, Y0_std: 223.522, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1440, Loss: 8.067e+04, Y0_mean: -87.416, Y0_std: 208.684, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1450, Loss: 1.003e+05, Y0_mean: -90.651, Y0_std: 224.419, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1460, Loss: 7.996e+04, Y0_mean: -80.585, Y0_std: 218.097, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 1470, Loss: 1.137e+05, Y0_mean: -83.490, Y0_std: 218.633, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1480, Loss: 7.444e+04, Y0_mean: -91.434, Y0_std: 227.861, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 1490, Loss: 7.221e+04, Y0_mean: -91.052, Y0_std: 228.798, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 1500, Loss: 8.638e+04, Y0_mean: -89.630, Y0_std: 227.862, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 1510, Loss: 6.984e+04, Y0_mean: -88.623, Y0_std: 225.264, Time: 0.76, Learning Rate: 4.480e-05\n",
      "It: 1520, Loss: 6.631e+04, Y0_mean: -91.486, Y0_std: 230.915, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 1530, Loss: 9.812e+04, Y0_mean: -87.305, Y0_std: 230.592, Time: 0.75, Learning Rate: 4.480e-05\n",
      "It: 0, Loss: 2.801e+04, Y0_mean: -85.628, Y0_std: 216.898, Time: 0.08, Learning Rate: 2.800e-05\n",
      "It: 10, Loss: 3.719e+04, Y0_mean: -89.467, Y0_std: 232.249, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 20, Loss: 2.691e+04, Y0_mean: -85.837, Y0_std: 217.742, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 30, Loss: 3.227e+04, Y0_mean: -83.894, Y0_std: 227.201, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 40, Loss: 3.184e+04, Y0_mean: -83.739, Y0_std: 228.743, Time: 0.77, Learning Rate: 2.800e-05\n",
      "It: 50, Loss: 2.741e+04, Y0_mean: -78.783, Y0_std: 226.281, Time: 0.77, Learning Rate: 2.800e-05\n",
      "It: 60, Loss: 3.078e+04, Y0_mean: -84.013, Y0_std: 229.614, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 70, Loss: 4.275e+04, Y0_mean: -86.561, Y0_std: 234.595, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 80, Loss: 2.703e+04, Y0_mean: -90.651, Y0_std: 236.597, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 90, Loss: 2.936e+04, Y0_mean: -83.441, Y0_std: 230.264, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 100, Loss: 2.504e+04, Y0_mean: -77.935, Y0_std: 228.225, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 110, Loss: 2.467e+04, Y0_mean: -78.085, Y0_std: 228.628, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 120, Loss: 2.539e+04, Y0_mean: -89.885, Y0_std: 237.805, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 130, Loss: 3.074e+04, Y0_mean: -88.351, Y0_std: 236.613, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 140, Loss: 3.012e+04, Y0_mean: -88.669, Y0_std: 237.545, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 150, Loss: 2.436e+04, Y0_mean: -89.593, Y0_std: 238.822, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 160, Loss: 2.663e+04, Y0_mean: -82.680, Y0_std: 233.127, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 170, Loss: 2.211e+04, Y0_mean: -77.220, Y0_std: 230.038, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 180, Loss: 3.729e+04, Y0_mean: -81.403, Y0_std: 231.006, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 190, Loss: 2.297e+04, Y0_mean: -86.717, Y0_std: 234.791, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 200, Loss: 1.936e+04, Y0_mean: -84.325, Y0_std: 224.353, Time: 0.77, Learning Rate: 2.800e-05\n",
      "It: 210, Loss: 2.257e+04, Y0_mean: -90.011, Y0_std: 242.162, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 220, Loss: 3.508e+04, Y0_mean: -85.516, Y0_std: 240.237, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 230, Loss: 3.519e+04, Y0_mean: -81.191, Y0_std: 232.926, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 240, Loss: 2.129e+04, Y0_mean: -85.865, Y0_std: 236.536, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 250, Loss: 3.450e+04, Y0_mean: -80.542, Y0_std: 233.830, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 260, Loss: 2.345e+04, Y0_mean: -82.027, Y0_std: 236.341, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 270, Loss: 2.067e+04, Y0_mean: -88.720, Y0_std: 243.145, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 280, Loss: 2.542e+04, Y0_mean: -87.541, Y0_std: 242.520, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 290, Loss: 3.306e+04, Y0_mean: -80.513, Y0_std: 235.191, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 300, Loss: 1.868e+04, Y0_mean: -76.070, Y0_std: 235.155, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 310, Loss: 3.159e+04, Y0_mean: -85.026, Y0_std: 243.849, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 320, Loss: 2.177e+04, Y0_mean: -81.362, Y0_std: 238.405, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 330, Loss: 3.095e+04, Y0_mean: -84.763, Y0_std: 244.567, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 340, Loss: 1.775e+04, Y0_mean: -75.703, Y0_std: 237.008, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 350, Loss: 3.049e+04, Y0_mean: -84.478, Y0_std: 244.768, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 360, Loss: 3.067e+04, Y0_mean: -80.130, Y0_std: 238.015, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 370, Loss: 2.265e+04, Y0_mean: -87.269, Y0_std: 246.564, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 380, Loss: 1.794e+04, Y0_mean: -84.902, Y0_std: 241.789, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 390, Loss: 2.995e+04, Y0_mean: -79.695, Y0_std: 238.026, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 400, Loss: 1.757e+04, Y0_mean: -87.971, Y0_std: 248.454, Time: 0.74, Learning Rate: 2.800e-05\n",
      "It: 410, Loss: 1.749e+04, Y0_mean: -88.481, Y0_std: 249.428, Time: 0.74, Learning Rate: 2.800e-05\n",
      "It: 420, Loss: 1.716e+04, Y0_mean: -88.424, Y0_std: 249.092, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 430, Loss: 1.915e+04, Y0_mean: -81.277, Y0_std: 242.280, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 440, Loss: 2.105e+04, Y0_mean: -86.945, Y0_std: 249.600, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 450, Loss: 2.084e+04, Y0_mean: -86.725, Y0_std: 249.156, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 460, Loss: 2.770e+04, Y0_mean: -79.506, Y0_std: 241.103, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 470, Loss: 1.604e+04, Y0_mean: -87.919, Y0_std: 250.971, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 480, Loss: 1.600e+04, Y0_mean: -84.433, Y0_std: 244.648, Time: 0.77, Learning Rate: 2.800e-05\n",
      "It: 490, Loss: 1.576e+04, Y0_mean: -84.212, Y0_std: 245.101, Time: 0.77, Learning Rate: 2.800e-05\n",
      "It: 500, Loss: 1.761e+04, Y0_mean: -80.029, Y0_std: 245.223, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 510, Loss: 1.258e+04, Y0_mean: -82.160, Y0_std: 235.407, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 520, Loss: 2.607e+04, Y0_mean: -78.838, Y0_std: 244.075, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 530, Loss: 1.414e+04, Y0_mean: -74.620, Y0_std: 244.085, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 540, Loss: 1.485e+04, Y0_mean: -87.690, Y0_std: 253.914, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 550, Loss: 1.875e+04, Y0_mean: -86.021, Y0_std: 252.725, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 560, Loss: 1.443e+04, Y0_mean: -87.448, Y0_std: 254.768, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 570, Loss: 1.343e+04, Y0_mean: -74.144, Y0_std: 245.113, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 580, Loss: 1.320e+04, Y0_mean: -73.840, Y0_std: 245.359, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 590, Loss: 1.790e+04, Y0_mean: -85.193, Y0_std: 254.232, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 600, Loss: 1.277e+04, Y0_mean: -73.799, Y0_std: 246.498, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 610, Loss: 2.382e+04, Y0_mean: -78.358, Y0_std: 247.164, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 620, Loss: 1.312e+04, Y0_mean: -87.449, Y0_std: 256.865, Time: 0.77, Learning Rate: 2.800e-05\n",
      "It: 630, Loss: 1.289e+04, Y0_mean: -87.100, Y0_std: 256.899, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 640, Loss: 2.224e+04, Y0_mean: -83.209, Y0_std: 257.054, Time: 0.77, Learning Rate: 2.800e-05\n",
      "It: 650, Loss: 1.286e+04, Y0_mean: -83.048, Y0_std: 251.401, Time: 0.77, Learning Rate: 2.800e-05\n",
      "It: 660, Loss: 1.240e+04, Y0_mean: -86.347, Y0_std: 257.400, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 670, Loss: 1.171e+04, Y0_mean: -73.006, Y0_std: 249.546, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 680, Loss: 2.227e+04, Y0_mean: -78.037, Y0_std: 249.513, Time: 0.77, Learning Rate: 2.800e-05\n",
      "It: 690, Loss: 1.241e+04, Y0_mean: -83.454, Y0_std: 253.767, Time: 0.77, Learning Rate: 2.800e-05\n",
      "It: 700, Loss: 1.200e+04, Y0_mean: -86.673, Y0_std: 260.375, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 710, Loss: 1.358e+04, Y0_mean: -79.263, Y0_std: 252.891, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 720, Loss: 1.096e+04, Y0_mean: -73.243, Y0_std: 251.319, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 730, Loss: 1.132e+04, Y0_mean: -86.219, Y0_std: 260.779, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 740, Loss: 1.072e+04, Y0_mean: -73.553, Y0_std: 252.226, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 750, Loss: 1.112e+04, Y0_mean: -86.269, Y0_std: 261.878, Time: 0.77, Learning Rate: 2.800e-05\n",
      "It: 760, Loss: 1.481e+04, Y0_mean: -85.194, Y0_std: 261.426, Time: 0.77, Learning Rate: 2.800e-05\n",
      "It: 770, Loss: 1.958e+04, Y0_mean: -82.500, Y0_std: 262.469, Time: 0.77, Learning Rate: 2.800e-05\n",
      "It: 780, Loss: 1.246e+04, Y0_mean: -79.439, Y0_std: 255.873, Time: 0.77, Learning Rate: 2.800e-05\n",
      "It: 790, Loss: 8.545e+03, Y0_mean: -81.431, Y0_std: 246.450, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 800, Loss: 1.214e+04, Y0_mean: -79.064, Y0_std: 256.519, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 810, Loss: 9.855e+03, Y0_mean: -73.438, Y0_std: 255.423, Time: 0.77, Learning Rate: 2.800e-05\n",
      "It: 820, Loss: 1.941e+04, Y0_mean: -77.787, Y0_std: 254.888, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 830, Loss: 1.174e+04, Y0_mean: -79.364, Y0_std: 258.673, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 840, Loss: 1.879e+04, Y0_mean: -77.461, Y0_std: 256.854, Time: 0.77, Learning Rate: 2.800e-05\n",
      "It: 850, Loss: 7.801e+03, Y0_mean: -81.156, Y0_std: 248.878, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 860, Loss: 1.867e+04, Y0_mean: -77.482, Y0_std: 256.216, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 870, Loss: 7.440e+03, Y0_mean: -80.949, Y0_std: 249.366, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 880, Loss: 1.750e+04, Y0_mean: -82.559, Y0_std: 265.754, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 890, Loss: 1.792e+04, Y0_mean: -77.569, Y0_std: 258.622, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 900, Loss: 1.767e+04, Y0_mean: -77.876, Y0_std: 258.794, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 910, Loss: 1.044e+04, Y0_mean: -79.356, Y0_std: 262.421, Time: 0.78, Learning Rate: 2.800e-05\n",
      "It: 920, Loss: 8.669e+03, Y0_mean: -86.799, Y0_std: 269.008, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 930, Loss: 1.632e+04, Y0_mean: -82.654, Y0_std: 268.715, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 940, Loss: 6.706e+03, Y0_mean: -81.354, Y0_std: 253.100, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 950, Loss: 1.658e+04, Y0_mean: -77.593, Y0_std: 261.254, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 960, Loss: 6.505e+03, Y0_mean: -80.823, Y0_std: 253.197, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 970, Loss: 8.616e+03, Y0_mean: -82.681, Y0_std: 264.951, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 980, Loss: 7.884e+03, Y0_mean: -86.335, Y0_std: 271.033, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 990, Loss: 8.432e+03, Y0_mean: -82.353, Y0_std: 265.833, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 1000, Loss: 1.523e+04, Y0_mean: -82.375, Y0_std: 271.661, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 1010, Loss: 1.505e+04, Y0_mean: -82.461, Y0_std: 272.030, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 1020, Loss: 7.387e+03, Y0_mean: -72.752, Y0_std: 263.423, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 1030, Loss: 7.270e+03, Y0_mean: -72.778, Y0_std: 263.825, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 1040, Loss: 5.735e+03, Y0_mean: -80.936, Y0_std: 255.805, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 1050, Loss: 7.266e+03, Y0_mean: -86.207, Y0_std: 274.900, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 1060, Loss: 1.443e+04, Y0_mean: -82.284, Y0_std: 273.590, Time: 0.77, Learning Rate: 2.800e-05\n",
      "It: 1070, Loss: 6.887e+03, Y0_mean: -72.847, Y0_std: 265.698, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 1080, Loss: 8.367e+03, Y0_mean: -78.977, Y0_std: 268.275, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 1090, Loss: 5.251e+03, Y0_mean: -81.064, Y0_std: 257.760, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 1100, Loss: 9.838e+03, Y0_mean: -84.774, Y0_std: 275.492, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 1110, Loss: 6.583e+03, Y0_mean: -86.135, Y0_std: 277.340, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 1120, Loss: 9.497e+03, Y0_mean: -84.712, Y0_std: 276.147, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 1130, Loss: 6.336e+03, Y0_mean: -72.460, Y0_std: 268.611, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 1140, Loss: 4.916e+03, Y0_mean: -80.794, Y0_std: 259.703, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 1150, Loss: 6.836e+03, Y0_mean: -83.152, Y0_std: 271.727, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 1160, Loss: 6.045e+03, Y0_mean: -86.621, Y0_std: 278.499, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 1170, Loss: 5.900e+03, Y0_mean: -72.622, Y0_std: 269.702, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 1180, Loss: 6.527e+03, Y0_mean: -82.970, Y0_std: 272.513, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 1190, Loss: 5.931e+03, Y0_mean: -86.501, Y0_std: 280.789, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 1200, Loss: 8.662e+03, Y0_mean: -84.914, Y0_std: 279.041, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 1210, Loss: 5.641e+03, Y0_mean: -72.763, Y0_std: 271.560, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 1220, Loss: 6.820e+03, Y0_mean: -79.348, Y0_std: 274.535, Time: 0.76, Learning Rate: 2.800e-05\n",
      "It: 1230, Loss: 4.263e+03, Y0_mean: -81.357, Y0_std: 263.326, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 1240, Loss: 1.166e+04, Y0_mean: -82.820, Y0_std: 281.631, Time: 0.75, Learning Rate: 2.800e-05\n",
      "It: 1250, Loss: 1.204e+04, Y0_mean: -77.640, Y0_std: 273.068, Time: 0.76, Learning Rate: 2.800e-05\n",
      "[TerminalRefine] main:terminal_first b=23, round=1, iters=1120, lr=1.4e-05, rel_delta=0.233\n",
      "It: 0, Loss: 1.280e+04, Y0_mean: -79.173, Y0_std: 275.910, Time: 0.07, Learning Rate: 1.400e-05\n",
      "It: 10, Loss: 2.359e+04, Y0_mean: -77.684, Y0_std: 273.738, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 20, Loss: 2.242e+04, Y0_mean: -82.819, Y0_std: 283.426, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 30, Loss: 2.346e+04, Y0_mean: -77.466, Y0_std: 273.728, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 40, Loss: 1.002e+04, Y0_mean: -86.670, Y0_std: 283.636, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 50, Loss: 2.226e+04, Y0_mean: -82.597, Y0_std: 283.531, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 60, Loss: 2.299e+04, Y0_mean: -77.511, Y0_std: 274.763, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 70, Loss: 9.895e+03, Y0_mean: -86.893, Y0_std: 284.969, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 80, Loss: 2.182e+04, Y0_mean: -82.988, Y0_std: 283.667, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 90, Loss: 7.571e+03, Y0_mean: -81.625, Y0_std: 267.192, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 100, Loss: 1.183e+04, Y0_mean: -79.287, Y0_std: 277.958, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 110, Loss: 9.650e+03, Y0_mean: -72.996, Y0_std: 275.689, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 120, Loss: 9.643e+03, Y0_mean: -72.967, Y0_std: 276.233, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 130, Loss: 1.453e+04, Y0_mean: -85.192, Y0_std: 284.475, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 140, Loss: 9.245e+03, Y0_mean: -86.771, Y0_std: 285.740, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 150, Loss: 1.046e+04, Y0_mean: -83.232, Y0_std: 278.580, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 160, Loss: 2.195e+04, Y0_mean: -77.568, Y0_std: 276.638, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 170, Loss: 8.957e+03, Y0_mean: -86.894, Y0_std: 286.522, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 180, Loss: 1.125e+04, Y0_mean: -79.429, Y0_std: 279.315, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 190, Loss: 8.977e+03, Y0_mean: -73.081, Y0_std: 277.409, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 200, Loss: 2.118e+04, Y0_mean: -77.719, Y0_std: 277.933, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 210, Loss: 2.111e+04, Y0_mean: -77.717, Y0_std: 277.721, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 220, Loss: 2.120e+04, Y0_mean: -77.726, Y0_std: 277.523, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 230, Loss: 1.347e+04, Y0_mean: -85.513, Y0_std: 287.447, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 240, Loss: 1.338e+04, Y0_mean: -85.613, Y0_std: 287.415, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 250, Loss: 2.085e+04, Y0_mean: -77.851, Y0_std: 278.017, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 260, Loss: 1.323e+04, Y0_mean: -85.342, Y0_std: 287.453, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 270, Loss: 1.044e+04, Y0_mean: -79.049, Y0_std: 281.430, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 280, Loss: 6.239e+03, Y0_mean: -81.377, Y0_std: 270.266, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 290, Loss: 1.944e+04, Y0_mean: -82.958, Y0_std: 288.070, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 300, Loss: 9.293e+03, Y0_mean: -83.439, Y0_std: 282.190, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 310, Loss: 1.917e+04, Y0_mean: -83.055, Y0_std: 288.481, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 320, Loss: 1.900e+04, Y0_mean: -83.126, Y0_std: 289.046, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 330, Loss: 9.932e+03, Y0_mean: -79.726, Y0_std: 283.060, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 340, Loss: 1.971e+04, Y0_mean: -77.847, Y0_std: 279.945, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 350, Loss: 8.979e+03, Y0_mean: -83.516, Y0_std: 282.878, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 360, Loss: 1.858e+04, Y0_mean: -83.065, Y0_std: 289.871, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 370, Loss: 7.853e+03, Y0_mean: -72.893, Y0_std: 280.697, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 380, Loss: 5.744e+03, Y0_mean: -81.598, Y0_std: 272.128, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 390, Loss: 1.896e+04, Y0_mean: -77.758, Y0_std: 281.491, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 400, Loss: 8.634e+03, Y0_mean: -83.530, Y0_std: 284.115, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 410, Loss: 7.617e+03, Y0_mean: -72.678, Y0_std: 281.411, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 420, Loss: 7.346e+03, Y0_mean: -86.970, Y0_std: 291.356, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 430, Loss: 1.855e+04, Y0_mean: -77.797, Y0_std: 282.413, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 440, Loss: 8.446e+03, Y0_mean: -83.753, Y0_std: 285.200, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 450, Loss: 8.360e+03, Y0_mean: -83.492, Y0_std: 284.439, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 460, Loss: 1.784e+04, Y0_mean: -82.526, Y0_std: 291.933, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 470, Loss: 1.817e+04, Y0_mean: -77.254, Y0_std: 283.070, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 480, Loss: 7.075e+03, Y0_mean: -87.020, Y0_std: 292.542, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 490, Loss: 8.901e+03, Y0_mean: -79.472, Y0_std: 285.187, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 500, Loss: 1.798e+04, Y0_mean: -77.794, Y0_std: 283.537, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 510, Loss: 5.252e+03, Y0_mean: -81.742, Y0_std: 274.923, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 520, Loss: 5.125e+03, Y0_mean: -81.647, Y0_std: 274.128, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 530, Loss: 5.084e+03, Y0_mean: -81.683, Y0_std: 274.875, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 540, Loss: 6.932e+03, Y0_mean: -72.812, Y0_std: 284.116, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 550, Loss: 8.458e+03, Y0_mean: -79.293, Y0_std: 286.792, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 560, Loss: 8.354e+03, Y0_mean: -79.568, Y0_std: 287.330, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 570, Loss: 1.655e+04, Y0_mean: -83.078, Y0_std: 294.206, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 580, Loss: 1.707e+04, Y0_mean: -77.764, Y0_std: 284.778, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 590, Loss: 1.619e+04, Y0_mean: -83.419, Y0_std: 294.794, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 600, Loss: 6.823e+03, Y0_mean: -73.007, Y0_std: 285.625, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 610, Loss: 1.667e+04, Y0_mean: -77.949, Y0_std: 285.577, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 620, Loss: 1.656e+04, Y0_mean: -77.836, Y0_std: 285.833, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 630, Loss: 7.307e+03, Y0_mean: -83.704, Y0_std: 288.418, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 640, Loss: 1.638e+04, Y0_mean: -77.499, Y0_std: 286.300, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 650, Loss: 6.431e+03, Y0_mean: -72.774, Y0_std: 286.046, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 660, Loss: 6.124e+03, Y0_mean: -87.216, Y0_std: 295.757, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 670, Loss: 1.637e+04, Y0_mean: -77.882, Y0_std: 286.603, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 680, Loss: 1.638e+04, Y0_mean: -77.926, Y0_std: 286.448, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 690, Loss: 4.446e+03, Y0_mean: -81.897, Y0_std: 277.305, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 700, Loss: 7.595e+03, Y0_mean: -79.730, Y0_std: 289.834, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 710, Loss: 5.897e+03, Y0_mean: -87.644, Y0_std: 297.137, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 720, Loss: 1.524e+04, Y0_mean: -83.375, Y0_std: 297.174, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 730, Loss: 4.487e+03, Y0_mean: -81.657, Y0_std: 278.637, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 740, Loss: 7.364e+03, Y0_mean: -79.542, Y0_std: 289.988, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 750, Loss: 4.405e+03, Y0_mean: -81.525, Y0_std: 278.957, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 760, Loss: 1.497e+04, Y0_mean: -83.315, Y0_std: 297.560, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 770, Loss: 5.697e+03, Y0_mean: -87.473, Y0_std: 298.315, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 780, Loss: 4.246e+03, Y0_mean: -81.758, Y0_std: 279.246, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 790, Loss: 5.544e+03, Y0_mean: -87.561, Y0_std: 298.522, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 800, Loss: 6.498e+03, Y0_mean: -83.817, Y0_std: 291.870, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 810, Loss: 5.746e+03, Y0_mean: -72.881, Y0_std: 288.853, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 820, Loss: 4.124e+03, Y0_mean: -81.777, Y0_std: 279.861, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 830, Loss: 8.966e+03, Y0_mean: -85.512, Y0_std: 298.593, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 840, Loss: 1.479e+04, Y0_mean: -77.906, Y0_std: 289.849, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 850, Loss: 5.417e+03, Y0_mean: -87.714, Y0_std: 300.051, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 860, Loss: 5.597e+03, Y0_mean: -73.168, Y0_std: 289.764, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 870, Loss: 8.748e+03, Y0_mean: -85.806, Y0_std: 299.339, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 880, Loss: 4.012e+03, Y0_mean: -81.727, Y0_std: 281.147, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 890, Loss: 8.698e+03, Y0_mean: -85.786, Y0_std: 299.269, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 900, Loss: 3.994e+03, Y0_mean: -81.916, Y0_std: 281.837, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 910, Loss: 6.468e+03, Y0_mean: -79.989, Y0_std: 294.031, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 920, Loss: 5.418e+03, Y0_mean: -73.301, Y0_std: 290.834, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 930, Loss: 6.389e+03, Y0_mean: -79.583, Y0_std: 293.835, Time: 0.78, Learning Rate: 1.400e-05\n",
      "It: 940, Loss: 1.406e+04, Y0_mean: -77.749, Y0_std: 291.438, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 950, Loss: 3.790e+03, Y0_mean: -81.900, Y0_std: 282.304, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 960, Loss: 1.350e+04, Y0_mean: -83.639, Y0_std: 300.931, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 970, Loss: 1.398e+04, Y0_mean: -78.037, Y0_std: 291.608, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 980, Loss: 8.189e+03, Y0_mean: -86.020, Y0_std: 301.423, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 990, Loss: 6.114e+03, Y0_mean: -79.641, Y0_std: 295.080, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 1000, Loss: 5.095e+03, Y0_mean: -73.174, Y0_std: 292.404, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 1010, Loss: 6.050e+03, Y0_mean: -79.976, Y0_std: 294.991, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 1020, Loss: 7.994e+03, Y0_mean: -86.033, Y0_std: 301.720, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 1030, Loss: 5.967e+03, Y0_mean: -79.568, Y0_std: 295.808, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 1040, Loss: 5.939e+03, Y0_mean: -79.534, Y0_std: 295.419, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 1050, Loss: 3.516e+03, Y0_mean: -82.003, Y0_std: 283.937, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 1060, Loss: 5.406e+03, Y0_mean: -84.139, Y0_std: 296.013, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 1070, Loss: 1.309e+04, Y0_mean: -78.146, Y0_std: 293.968, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 1080, Loss: 5.357e+03, Y0_mean: -84.213, Y0_std: 296.437, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 1090, Loss: 5.332e+03, Y0_mean: -84.252, Y0_std: 296.600, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 1100, Loss: 5.291e+03, Y0_mean: -84.163, Y0_std: 296.656, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 1110, Loss: 3.356e+03, Y0_mean: -82.080, Y0_std: 284.649, Time: 0.75, Learning Rate: 1.400e-05\n",
      "[TerminalRefine] main:terminal_first b=23, round=2, iters=1120, lr=1.4e-05, rel_delta=0.246\n",
      "It: 0, Loss: 1.243e+04, Y0_mean: -84.049, Y0_std: 304.216, Time: 0.07, Learning Rate: 1.400e-05\n",
      "It: 10, Loss: 3.299e+03, Y0_mean: -82.345, Y0_std: 284.952, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 20, Loss: 3.304e+03, Y0_mean: -82.292, Y0_std: 285.262, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 30, Loss: 5.537e+03, Y0_mean: -79.762, Y0_std: 297.381, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 40, Loss: 1.257e+04, Y0_mean: -77.865, Y0_std: 295.317, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 50, Loss: 4.309e+03, Y0_mean: -87.961, Y0_std: 305.151, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 60, Loss: 3.323e+03, Y0_mean: -81.998, Y0_std: 286.202, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 70, Loss: 5.357e+03, Y0_mean: -79.881, Y0_std: 298.140, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 80, Loss: 4.520e+03, Y0_mean: -73.128, Y0_std: 295.953, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 90, Loss: 5.251e+03, Y0_mean: -80.079, Y0_std: 298.591, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 100, Loss: 3.203e+03, Y0_mean: -82.191, Y0_std: 286.721, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 110, Loss: 1.197e+04, Y0_mean: -83.862, Y0_std: 305.350, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 120, Loss: 4.800e+03, Y0_mean: -84.048, Y0_std: 298.843, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 130, Loss: 3.154e+03, Y0_mean: -82.179, Y0_std: 287.384, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 140, Loss: 1.197e+04, Y0_mean: -78.221, Y0_std: 296.761, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 150, Loss: 5.082e+03, Y0_mean: -80.026, Y0_std: 299.271, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 160, Loss: 5.051e+03, Y0_mean: -80.179, Y0_std: 300.360, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 170, Loss: 5.020e+03, Y0_mean: -80.118, Y0_std: 299.545, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 180, Loss: 4.231e+03, Y0_mean: -73.403, Y0_std: 297.351, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 190, Loss: 4.978e+03, Y0_mean: -80.173, Y0_std: 299.829, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 200, Loss: 6.710e+03, Y0_mean: -86.503, Y0_std: 306.848, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 210, Loss: 1.138e+04, Y0_mean: -83.772, Y0_std: 307.563, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 220, Loss: 3.873e+03, Y0_mean: -88.187, Y0_std: 307.547, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 230, Loss: 6.545e+03, Y0_mean: -86.297, Y0_std: 307.488, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 240, Loss: 2.956e+03, Y0_mean: -82.254, Y0_std: 288.944, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 250, Loss: 4.045e+03, Y0_mean: -73.075, Y0_std: 298.542, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 260, Loss: 4.760e+03, Y0_mean: -80.009, Y0_std: 301.071, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 270, Loss: 6.377e+03, Y0_mean: -86.332, Y0_std: 308.005, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 280, Loss: 3.748e+03, Y0_mean: -88.179, Y0_std: 309.275, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 290, Loss: 6.300e+03, Y0_mean: -86.415, Y0_std: 308.582, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 300, Loss: 1.101e+04, Y0_mean: -78.107, Y0_std: 299.444, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 310, Loss: 3.686e+03, Y0_mean: -88.325, Y0_std: 309.944, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 320, Loss: 6.184e+03, Y0_mean: -86.422, Y0_std: 308.914, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 330, Loss: 4.240e+03, Y0_mean: -84.522, Y0_std: 302.357, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 340, Loss: 3.608e+03, Y0_mean: -88.618, Y0_std: 310.604, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 350, Loss: 4.532e+03, Y0_mean: -80.200, Y0_std: 302.266, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 360, Loss: 3.854e+03, Y0_mean: -73.159, Y0_std: 300.701, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 370, Loss: 3.736e+03, Y0_mean: -73.026, Y0_std: 300.140, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 380, Loss: 6.019e+03, Y0_mean: -86.404, Y0_std: 309.669, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 390, Loss: 3.709e+03, Y0_mean: -73.285, Y0_std: 300.655, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 400, Loss: 4.362e+03, Y0_mean: -80.084, Y0_std: 303.374, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 410, Loss: 4.060e+03, Y0_mean: -84.568, Y0_std: 303.847, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 420, Loss: 4.273e+03, Y0_mean: -80.266, Y0_std: 304.069, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 430, Loss: 4.062e+03, Y0_mean: -84.834, Y0_std: 304.080, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 440, Loss: 1.009e+04, Y0_mean: -84.401, Y0_std: 311.324, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 450, Loss: 1.029e+04, Y0_mean: -78.155, Y0_std: 301.677, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 460, Loss: 4.229e+03, Y0_mean: -79.942, Y0_std: 305.291, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 470, Loss: 4.139e+03, Y0_mean: -80.137, Y0_std: 304.618, Time: 0.74, Learning Rate: 1.400e-05\n",
      "It: 480, Loss: 1.002e+04, Y0_mean: -78.521, Y0_std: 302.587, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 490, Loss: 3.289e+03, Y0_mean: -88.642, Y0_std: 312.493, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 500, Loss: 4.069e+03, Y0_mean: -80.207, Y0_std: 305.058, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 510, Loss: 9.950e+03, Y0_mean: -83.947, Y0_std: 312.399, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 520, Loss: 4.027e+03, Y0_mean: -80.126, Y0_std: 305.793, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 530, Loss: 5.522e+03, Y0_mean: -86.744, Y0_std: 311.901, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 540, Loss: 2.481e+03, Y0_mean: -82.898, Y0_std: 293.589, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 550, Loss: 5.438e+03, Y0_mean: -86.594, Y0_std: 312.367, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 560, Loss: 9.862e+03, Y0_mean: -78.238, Y0_std: 303.066, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 570, Loss: 9.536e+03, Y0_mean: -84.440, Y0_std: 313.521, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 580, Loss: 3.176e+03, Y0_mean: -89.074, Y0_std: 314.229, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 590, Loss: 3.376e+03, Y0_mean: -73.332, Y0_std: 303.710, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 600, Loss: 5.281e+03, Y0_mean: -86.382, Y0_std: 313.227, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 610, Loss: 3.601e+03, Y0_mean: -84.685, Y0_std: 306.499, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 620, Loss: 9.477e+03, Y0_mean: -78.461, Y0_std: 304.255, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 630, Loss: 3.773e+03, Y0_mean: -80.116, Y0_std: 307.022, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 640, Loss: 3.517e+03, Y0_mean: -84.659, Y0_std: 306.909, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 650, Loss: 3.490e+03, Y0_mean: -84.613, Y0_std: 306.766, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 660, Loss: 3.501e+03, Y0_mean: -84.608, Y0_std: 307.589, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 670, Loss: 5.098e+03, Y0_mean: -86.807, Y0_std: 314.131, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 680, Loss: 3.280e+03, Y0_mean: -73.561, Y0_std: 305.671, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 690, Loss: 2.264e+03, Y0_mean: -82.912, Y0_std: 295.143, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 700, Loss: 2.276e+03, Y0_mean: -82.889, Y0_std: 295.895, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 710, Loss: 3.477e+03, Y0_mean: -85.191, Y0_std: 307.893, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 720, Loss: 3.550e+03, Y0_mean: -80.437, Y0_std: 308.463, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 730, Loss: 4.932e+03, Y0_mean: -86.903, Y0_std: 315.058, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 740, Loss: 8.884e+03, Y0_mean: -84.619, Y0_std: 315.808, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 750, Loss: 2.834e+03, Y0_mean: -88.942, Y0_std: 316.392, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 760, Loss: 8.813e+03, Y0_mean: -84.642, Y0_std: 316.177, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 770, Loss: 2.232e+03, Y0_mean: -82.932, Y0_std: 296.951, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 780, Loss: 3.020e+03, Y0_mean: -73.471, Y0_std: 306.421, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 790, Loss: 8.727e+03, Y0_mean: -78.459, Y0_std: 307.073, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 800, Loss: 3.431e+03, Y0_mean: -80.380, Y0_std: 309.419, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 810, Loss: 3.005e+03, Y0_mean: -73.674, Y0_std: 306.950, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 820, Loss: 8.751e+03, Y0_mean: -78.611, Y0_std: 307.014, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 830, Loss: 3.220e+03, Y0_mean: -85.082, Y0_std: 309.349, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 840, Loss: 2.751e+03, Y0_mean: -88.914, Y0_std: 317.891, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 850, Loss: 8.472e+03, Y0_mean: -84.699, Y0_std: 317.542, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 860, Loss: 2.906e+03, Y0_mean: -73.563, Y0_std: 307.321, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 870, Loss: 2.054e+03, Y0_mean: -83.007, Y0_std: 297.989, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 880, Loss: 3.109e+03, Y0_mean: -85.082, Y0_std: 310.033, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 890, Loss: 2.632e+03, Y0_mean: -89.412, Y0_std: 318.527, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 900, Loss: 4.533e+03, Y0_mean: -87.352, Y0_std: 317.607, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 910, Loss: 4.510e+03, Y0_mean: -87.050, Y0_std: 317.354, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 920, Loss: 3.211e+03, Y0_mean: -80.575, Y0_std: 310.956, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 930, Loss: 2.798e+03, Y0_mean: -73.641, Y0_std: 308.594, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 940, Loss: 2.993e+03, Y0_mean: -85.114, Y0_std: 310.825, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 950, Loss: 3.105e+03, Y0_mean: -80.765, Y0_std: 311.796, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 960, Loss: 4.391e+03, Y0_mean: -87.183, Y0_std: 317.938, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 970, Loss: 4.332e+03, Y0_mean: -87.028, Y0_std: 318.577, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 980, Loss: 2.911e+03, Y0_mean: -84.910, Y0_std: 311.092, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 990, Loss: 4.320e+03, Y0_mean: -87.222, Y0_std: 318.878, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 1000, Loss: 2.753e+03, Y0_mean: -74.266, Y0_std: 309.597, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 1010, Loss: 2.919e+03, Y0_mean: -85.428, Y0_std: 311.721, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 1020, Loss: 2.457e+03, Y0_mean: -89.075, Y0_std: 320.218, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 1030, Loss: 4.219e+03, Y0_mean: -87.139, Y0_std: 318.897, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 1040, Loss: 2.848e+03, Y0_mean: -85.365, Y0_std: 312.292, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 1050, Loss: 2.376e+03, Y0_mean: -89.360, Y0_std: 320.256, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 1060, Loss: 7.966e+03, Y0_mean: -79.007, Y0_std: 310.267, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 1070, Loss: 4.149e+03, Y0_mean: -87.514, Y0_std: 319.582, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 1080, Loss: 7.819e+03, Y0_mean: -85.183, Y0_std: 320.360, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 1090, Loss: 2.877e+03, Y0_mean: -81.127, Y0_std: 313.768, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 1100, Loss: 7.822e+03, Y0_mean: -79.367, Y0_std: 310.669, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 1110, Loss: 7.615e+03, Y0_mean: -85.232, Y0_std: 321.699, Time: 0.77, Learning Rate: 1.400e-05\n",
      "[TerminalRefine] main:terminal_first b=23, round=3, iters=1120, lr=1.4e-05, rel_delta=0.257\n",
      "It: 0, Loss: 4.038e+03, Y0_mean: -87.517, Y0_std: 320.605, Time: 0.08, Learning Rate: 1.400e-05\n",
      "It: 10, Loss: 2.739e+03, Y0_mean: -85.297, Y0_std: 313.681, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 20, Loss: 1.774e+03, Y0_mean: -83.284, Y0_std: 301.481, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 30, Loss: 1.725e+03, Y0_mean: -83.266, Y0_std: 301.305, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 40, Loss: 1.729e+03, Y0_mean: -83.476, Y0_std: 301.670, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 50, Loss: 7.580e+03, Y0_mean: -78.959, Y0_std: 311.474, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 60, Loss: 7.467e+03, Y0_mean: -85.170, Y0_std: 321.627, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 70, Loss: 3.890e+03, Y0_mean: -87.528, Y0_std: 321.407, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 80, Loss: 7.464e+03, Y0_mean: -79.087, Y0_std: 312.032, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 90, Loss: 2.456e+03, Y0_mean: -73.628, Y0_std: 312.379, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 100, Loss: 2.399e+03, Y0_mean: -73.691, Y0_std: 312.374, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 110, Loss: 2.201e+03, Y0_mean: -89.286, Y0_std: 322.920, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 120, Loss: 7.324e+03, Y0_mean: -79.073, Y0_std: 312.606, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 130, Loss: 2.393e+03, Y0_mean: -73.940, Y0_std: 312.971, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 140, Loss: 2.533e+03, Y0_mean: -85.380, Y0_std: 315.181, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 150, Loss: 1.619e+03, Y0_mean: -83.307, Y0_std: 302.778, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 160, Loss: 2.362e+03, Y0_mean: -74.087, Y0_std: 313.227, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 170, Loss: 7.114e+03, Y0_mean: -85.541, Y0_std: 323.222, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 180, Loss: 7.095e+03, Y0_mean: -85.298, Y0_std: 323.598, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 190, Loss: 3.660e+03, Y0_mean: -87.608, Y0_std: 323.131, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 200, Loss: 1.580e+03, Y0_mean: -83.407, Y0_std: 303.309, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 210, Loss: 2.055e+03, Y0_mean: -89.688, Y0_std: 324.191, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 220, Loss: 2.521e+03, Y0_mean: -81.206, Y0_std: 316.860, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 230, Loss: 7.031e+03, Y0_mean: -79.297, Y0_std: 313.813, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 240, Loss: 6.896e+03, Y0_mean: -85.417, Y0_std: 324.431, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 250, Loss: 2.493e+03, Y0_mean: -86.048, Y0_std: 316.712, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 260, Loss: 2.007e+03, Y0_mean: -89.949, Y0_std: 324.736, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 270, Loss: 6.823e+03, Y0_mean: -85.299, Y0_std: 324.763, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 280, Loss: 2.356e+03, Y0_mean: -85.487, Y0_std: 316.712, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 290, Loss: 2.435e+03, Y0_mean: -80.971, Y0_std: 317.552, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 300, Loss: 2.204e+03, Y0_mean: -74.169, Y0_std: 314.894, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 310, Loss: 1.925e+03, Y0_mean: -90.013, Y0_std: 325.572, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 320, Loss: 2.328e+03, Y0_mean: -85.862, Y0_std: 317.331, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 330, Loss: 2.405e+03, Y0_mean: -80.919, Y0_std: 317.988, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 340, Loss: 1.464e+03, Y0_mean: -83.529, Y0_std: 305.442, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 350, Loss: 1.898e+03, Y0_mean: -89.933, Y0_std: 326.081, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 360, Loss: 6.604e+03, Y0_mean: -85.565, Y0_std: 325.937, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 370, Loss: 6.631e+03, Y0_mean: -85.426, Y0_std: 325.837, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 380, Loss: 2.241e+03, Y0_mean: -85.743, Y0_std: 318.245, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 390, Loss: 2.054e+03, Y0_mean: -73.917, Y0_std: 316.027, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 400, Loss: 2.298e+03, Y0_mean: -81.239, Y0_std: 318.946, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 410, Loss: 6.501e+03, Y0_mean: -85.700, Y0_std: 326.642, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 420, Loss: 1.820e+03, Y0_mean: -89.917, Y0_std: 326.669, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 430, Loss: 3.266e+03, Y0_mean: -87.924, Y0_std: 326.705, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 440, Loss: 1.356e+03, Y0_mean: -83.590, Y0_std: 306.047, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 450, Loss: 1.804e+03, Y0_mean: -90.068, Y0_std: 327.634, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 460, Loss: 2.176e+03, Y0_mean: -86.005, Y0_std: 319.014, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 470, Loss: 2.153e+03, Y0_mean: -85.906, Y0_std: 318.978, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 480, Loss: 3.173e+03, Y0_mean: -87.890, Y0_std: 326.375, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 490, Loss: 6.381e+03, Y0_mean: -79.434, Y0_std: 317.132, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 500, Loss: 6.275e+03, Y0_mean: -85.907, Y0_std: 327.739, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 510, Loss: 2.173e+03, Y0_mean: -81.273, Y0_std: 320.134, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 520, Loss: 2.179e+03, Y0_mean: -81.166, Y0_std: 320.145, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 530, Loss: 3.095e+03, Y0_mean: -87.980, Y0_std: 326.915, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 540, Loss: 3.052e+03, Y0_mean: -87.931, Y0_std: 327.422, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 550, Loss: 2.120e+03, Y0_mean: -81.178, Y0_std: 320.817, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 560, Loss: 1.924e+03, Y0_mean: -74.060, Y0_std: 318.380, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 570, Loss: 2.019e+03, Y0_mean: -85.652, Y0_std: 320.477, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 580, Loss: 2.124e+03, Y0_mean: -80.934, Y0_std: 321.071, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 590, Loss: 1.868e+03, Y0_mean: -73.872, Y0_std: 318.409, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 600, Loss: 6.123e+03, Y0_mean: -85.682, Y0_std: 328.797, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 610, Loss: 1.223e+03, Y0_mean: -84.022, Y0_std: 308.406, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 620, Loss: 1.641e+03, Y0_mean: -90.027, Y0_std: 329.173, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 630, Loss: 6.057e+03, Y0_mean: -78.962, Y0_std: 318.825, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 640, Loss: 1.949e+03, Y0_mean: -85.820, Y0_std: 321.332, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 650, Loss: 2.909e+03, Y0_mean: -88.218, Y0_std: 329.209, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 660, Loss: 1.964e+03, Y0_mean: -85.916, Y0_std: 321.658, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 670, Loss: 1.612e+03, Y0_mean: -90.039, Y0_std: 330.041, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 680, Loss: 1.585e+03, Y0_mean: -90.089, Y0_std: 329.808, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 690, Loss: 5.876e+03, Y0_mean: -79.391, Y0_std: 319.797, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 700, Loss: 1.762e+03, Y0_mean: -73.832, Y0_std: 319.831, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 710, Loss: 2.851e+03, Y0_mean: -87.926, Y0_std: 330.206, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 720, Loss: 1.196e+03, Y0_mean: -83.690, Y0_std: 309.650, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 730, Loss: 2.754e+03, Y0_mean: -87.792, Y0_std: 329.834, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 740, Loss: 1.727e+03, Y0_mean: -73.820, Y0_std: 320.310, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 750, Loss: 1.711e+03, Y0_mean: -73.818, Y0_std: 320.639, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 760, Loss: 1.932e+03, Y0_mean: -81.190, Y0_std: 323.263, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 770, Loss: 1.860e+03, Y0_mean: -86.176, Y0_std: 323.001, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 780, Loss: 2.702e+03, Y0_mean: -88.222, Y0_std: 330.538, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 790, Loss: 2.693e+03, Y0_mean: -88.210, Y0_std: 330.680, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 800, Loss: 1.674e+03, Y0_mean: -74.051, Y0_std: 321.047, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 810, Loss: 1.694e+03, Y0_mean: -74.161, Y0_std: 321.672, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 820, Loss: 1.482e+03, Y0_mean: -90.448, Y0_std: 331.838, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 830, Loss: 5.579e+03, Y0_mean: -86.087, Y0_std: 331.784, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 840, Loss: 1.431e+03, Y0_mean: -90.579, Y0_std: 332.063, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 850, Loss: 5.693e+03, Y0_mean: -85.836, Y0_std: 331.691, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 860, Loss: 1.019e+03, Y0_mean: -84.074, Y0_std: 311.163, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 870, Loss: 5.577e+03, Y0_mean: -79.572, Y0_std: 321.707, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 880, Loss: 5.451e+03, Y0_mean: -79.331, Y0_std: 322.109, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 890, Loss: 1.635e+03, Y0_mean: -73.936, Y0_std: 322.556, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 900, Loss: 1.603e+03, Y0_mean: -74.119, Y0_std: 322.360, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 910, Loss: 5.453e+03, Y0_mean: -85.984, Y0_std: 333.139, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 920, Loss: 1.592e+03, Y0_mean: -73.989, Y0_std: 322.537, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 930, Loss: 2.497e+03, Y0_mean: -87.923, Y0_std: 332.374, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 940, Loss: 1.597e+03, Y0_mean: -74.194, Y0_std: 322.884, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 950, Loss: 1.744e+03, Y0_mean: -81.364, Y0_std: 325.790, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 960, Loss: 1.749e+03, Y0_mean: -81.371, Y0_std: 325.610, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 970, Loss: 5.286e+03, Y0_mean: -79.326, Y0_std: 323.116, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 980, Loss: 5.272e+03, Y0_mean: -79.083, Y0_std: 322.949, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 990, Loss: 1.578e+03, Y0_mean: -73.934, Y0_std: 323.800, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 1000, Loss: 1.360e+03, Y0_mean: -90.322, Y0_std: 333.970, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 1010, Loss: 9.988e+02, Y0_mean: -83.956, Y0_std: 312.980, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 1020, Loss: 1.629e+03, Y0_mean: -85.976, Y0_std: 325.750, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 1030, Loss: 1.617e+03, Y0_mean: -85.996, Y0_std: 325.850, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 1040, Loss: 1.011e+03, Y0_mean: -83.674, Y0_std: 313.395, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 1050, Loss: 2.350e+03, Y0_mean: -88.019, Y0_std: 333.735, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 1060, Loss: 5.107e+03, Y0_mean: -79.211, Y0_std: 323.975, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 1070, Loss: 1.316e+03, Y0_mean: -90.023, Y0_std: 334.596, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 1080, Loss: 5.106e+03, Y0_mean: -79.524, Y0_std: 324.488, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 1090, Loss: 1.492e+03, Y0_mean: -74.201, Y0_std: 324.684, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 1100, Loss: 5.187e+03, Y0_mean: -85.700, Y0_std: 334.961, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 1110, Loss: 9.588e+02, Y0_mean: -83.808, Y0_std: 314.355, Time: 0.75, Learning Rate: 1.400e-05\n",
      "[TerminalRefine] main:terminal_first b=23, round=4, iters=1120, lr=1.4e-05, rel_delta=0.261\n",
      "It: 0, Loss: 5.157e+03, Y0_mean: -85.809, Y0_std: 334.975, Time: 0.07, Learning Rate: 1.400e-05\n",
      "It: 10, Loss: 4.989e+03, Y0_mean: -79.451, Y0_std: 324.996, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 20, Loss: 1.623e+03, Y0_mean: -81.319, Y0_std: 327.733, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 30, Loss: 1.425e+03, Y0_mean: -73.703, Y0_std: 325.135, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 40, Loss: 1.589e+03, Y0_mean: -81.394, Y0_std: 328.222, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 50, Loss: 2.233e+03, Y0_mean: -88.121, Y0_std: 335.110, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 60, Loss: 1.246e+03, Y0_mean: -90.636, Y0_std: 336.254, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 70, Loss: 2.224e+03, Y0_mean: -88.264, Y0_std: 335.324, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 80, Loss: 1.573e+03, Y0_mean: -81.265, Y0_std: 328.588, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 90, Loss: 1.541e+03, Y0_mean: -81.423, Y0_std: 328.822, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 100, Loss: 4.944e+03, Y0_mean: -85.832, Y0_std: 336.476, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 110, Loss: 1.225e+03, Y0_mean: -90.445, Y0_std: 336.729, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 120, Loss: 2.169e+03, Y0_mean: -88.378, Y0_std: 336.040, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 130, Loss: 9.042e+02, Y0_mean: -84.124, Y0_std: 315.795, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 140, Loss: 4.774e+03, Y0_mean: -79.452, Y0_std: 326.407, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 150, Loss: 2.138e+03, Y0_mean: -88.327, Y0_std: 336.153, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 160, Loss: 8.884e+02, Y0_mean: -84.021, Y0_std: 316.100, Time: 0.74, Learning Rate: 1.400e-05\n",
      "It: 170, Loss: 1.355e+03, Y0_mean: -74.081, Y0_std: 326.897, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 180, Loss: 1.423e+03, Y0_mean: -86.170, Y0_std: 328.941, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 190, Loss: 1.420e+03, Y0_mean: -86.119, Y0_std: 328.999, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 200, Loss: 4.576e+03, Y0_mean: -79.373, Y0_std: 327.788, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 210, Loss: 1.350e+03, Y0_mean: -74.042, Y0_std: 327.462, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 220, Loss: 1.430e+03, Y0_mean: -86.248, Y0_std: 329.998, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 230, Loss: 1.309e+03, Y0_mean: -73.764, Y0_std: 327.615, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 240, Loss: 2.058e+03, Y0_mean: -88.227, Y0_std: 337.090, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 250, Loss: 1.465e+03, Y0_mean: -81.422, Y0_std: 330.613, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 260, Loss: 1.284e+03, Y0_mean: -73.811, Y0_std: 328.029, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 270, Loss: 1.297e+03, Y0_mean: -74.018, Y0_std: 328.339, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 280, Loss: 8.325e+02, Y0_mean: -84.022, Y0_std: 317.522, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 290, Loss: 1.101e+03, Y0_mean: -90.628, Y0_std: 338.823, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 300, Loss: 8.041e+02, Y0_mean: -84.265, Y0_std: 317.505, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 310, Loss: 4.493e+03, Y0_mean: -79.461, Y0_std: 328.486, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 320, Loss: 1.114e+03, Y0_mean: -90.799, Y0_std: 339.412, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 330, Loss: 4.454e+03, Y0_mean: -79.602, Y0_std: 328.587, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 340, Loss: 1.076e+03, Y0_mean: -90.758, Y0_std: 339.236, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 350, Loss: 1.276e+03, Y0_mean: -74.201, Y0_std: 329.213, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 360, Loss: 1.366e+03, Y0_mean: -81.593, Y0_std: 331.839, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 370, Loss: 4.478e+03, Y0_mean: -86.339, Y0_std: 339.626, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 380, Loss: 4.406e+03, Y0_mean: -79.834, Y0_std: 329.100, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 390, Loss: 1.248e+03, Y0_mean: -74.318, Y0_std: 329.464, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 400, Loss: 1.344e+03, Y0_mean: -81.748, Y0_std: 332.089, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 410, Loss: 4.334e+03, Y0_mean: -79.578, Y0_std: 329.479, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 420, Loss: 4.448e+03, Y0_mean: -86.118, Y0_std: 340.013, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 430, Loss: 1.341e+03, Y0_mean: -81.450, Y0_std: 332.666, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 440, Loss: 1.903e+03, Y0_mean: -88.648, Y0_std: 339.444, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 450, Loss: 1.296e+03, Y0_mean: -86.536, Y0_std: 332.099, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 460, Loss: 8.017e+02, Y0_mean: -84.040, Y0_std: 319.568, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 470, Loss: 4.207e+03, Y0_mean: -79.411, Y0_std: 330.158, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 480, Loss: 1.220e+03, Y0_mean: -74.172, Y0_std: 330.609, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 490, Loss: 4.313e+03, Y0_mean: -86.092, Y0_std: 340.989, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 500, Loss: 1.028e+03, Y0_mean: -90.713, Y0_std: 341.123, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 510, Loss: 4.196e+03, Y0_mean: -79.431, Y0_std: 330.393, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 520, Loss: 4.230e+03, Y0_mean: -79.579, Y0_std: 330.386, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 530, Loss: 1.273e+03, Y0_mean: -81.639, Y0_std: 333.733, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 540, Loss: 1.259e+03, Y0_mean: -81.691, Y0_std: 333.771, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 550, Loss: 1.796e+03, Y0_mean: -88.351, Y0_std: 340.771, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 560, Loss: 1.794e+03, Y0_mean: -88.427, Y0_std: 340.502, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 570, Loss: 4.233e+03, Y0_mean: -86.256, Y0_std: 342.092, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 580, Loss: 4.150e+03, Y0_mean: -79.678, Y0_std: 331.036, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 590, Loss: 1.170e+03, Y0_mean: -74.274, Y0_std: 331.731, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 600, Loss: 1.236e+03, Y0_mean: -81.719, Y0_std: 334.337, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 610, Loss: 7.174e+02, Y0_mean: -84.416, Y0_std: 320.840, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 620, Loss: 6.934e+02, Y0_mean: -84.489, Y0_std: 320.775, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 630, Loss: 4.140e+03, Y0_mean: -79.619, Y0_std: 331.476, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 640, Loss: 1.235e+03, Y0_mean: -81.456, Y0_std: 334.856, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 650, Loss: 1.204e+03, Y0_mean: -81.589, Y0_std: 335.071, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 660, Loss: 4.051e+03, Y0_mean: -86.618, Y0_std: 342.764, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 670, Loss: 4.095e+03, Y0_mean: -86.285, Y0_std: 342.716, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 680, Loss: 4.082e+03, Y0_mean: -86.188, Y0_std: 342.845, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 690, Loss: 1.690e+03, Y0_mean: -88.703, Y0_std: 342.496, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 700, Loss: 1.125e+03, Y0_mean: -74.114, Y0_std: 333.129, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 710, Loss: 1.167e+03, Y0_mean: -81.737, Y0_std: 335.732, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 720, Loss: 1.160e+03, Y0_mean: -81.974, Y0_std: 336.241, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 730, Loss: 1.119e+03, Y0_mean: -74.190, Y0_std: 333.310, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 740, Loss: 7.111e+02, Y0_mean: -83.981, Y0_std: 322.113, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 750, Loss: 3.891e+03, Y0_mean: -79.684, Y0_std: 333.099, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 760, Loss: 3.817e+03, Y0_mean: -79.743, Y0_std: 333.453, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 770, Loss: 1.168e+03, Y0_mean: -81.398, Y0_std: 336.428, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 780, Loss: 1.615e+03, Y0_mean: -88.510, Y0_std: 343.447, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 790, Loss: 1.618e+03, Y0_mean: -88.774, Y0_std: 343.574, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 800, Loss: 6.870e+02, Y0_mean: -84.084, Y0_std: 322.829, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 810, Loss: 1.169e+03, Y0_mean: -81.399, Y0_std: 336.609, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 820, Loss: 3.818e+03, Y0_mean: -79.774, Y0_std: 333.970, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 830, Loss: 3.716e+03, Y0_mean: -79.674, Y0_std: 334.237, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 840, Loss: 1.116e+03, Y0_mean: -86.827, Y0_std: 336.463, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 850, Loss: 3.794e+03, Y0_mean: -79.522, Y0_std: 334.052, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 860, Loss: 1.023e+03, Y0_mean: -73.852, Y0_std: 334.691, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 870, Loss: 3.759e+03, Y0_mean: -79.669, Y0_std: 334.271, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 880, Loss: 1.079e+03, Y0_mean: -86.655, Y0_std: 336.736, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 890, Loss: 6.609e+02, Y0_mean: -84.167, Y0_std: 323.870, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 900, Loss: 1.015e+03, Y0_mean: -74.265, Y0_std: 334.938, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 910, Loss: 1.570e+03, Y0_mean: -88.952, Y0_std: 344.853, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 920, Loss: 1.050e+03, Y0_mean: -86.579, Y0_std: 336.944, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 930, Loss: 6.527e+02, Y0_mean: -84.156, Y0_std: 324.224, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 940, Loss: 1.072e+03, Y0_mean: -81.855, Y0_std: 338.185, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 950, Loss: 8.356e+02, Y0_mean: -91.073, Y0_std: 345.961, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 960, Loss: 6.183e+02, Y0_mean: -84.360, Y0_std: 324.396, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 970, Loss: 1.535e+03, Y0_mean: -88.881, Y0_std: 345.225, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 980, Loss: 6.155e+02, Y0_mean: -84.446, Y0_std: 324.524, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 990, Loss: 9.676e+02, Y0_mean: -73.889, Y0_std: 335.801, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 1000, Loss: 3.790e+03, Y0_mean: -86.191, Y0_std: 346.197, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 1010, Loss: 1.016e+03, Y0_mean: -86.558, Y0_std: 337.684, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 1020, Loss: 6.275e+02, Y0_mean: -84.152, Y0_std: 325.167, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 1030, Loss: 9.516e+02, Y0_mean: -74.079, Y0_std: 336.324, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 1040, Loss: 9.508e+02, Y0_mean: -74.142, Y0_std: 336.240, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 1050, Loss: 1.037e+03, Y0_mean: -81.764, Y0_std: 339.177, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 1060, Loss: 6.101e+02, Y0_mean: -84.425, Y0_std: 325.429, Time: 0.77, Learning Rate: 1.400e-05\n",
      "It: 1070, Loss: 9.332e+02, Y0_mean: -73.913, Y0_std: 336.684, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 1080, Loss: 3.644e+03, Y0_mean: -86.633, Y0_std: 347.164, Time: 0.76, Learning Rate: 1.400e-05\n",
      "It: 1090, Loss: 7.884e+02, Y0_mean: -91.307, Y0_std: 347.463, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 1100, Loss: 9.286e+02, Y0_mean: -74.044, Y0_std: 336.753, Time: 0.75, Learning Rate: 1.400e-05\n",
      "It: 1110, Loss: 9.332e+02, Y0_mean: -74.037, Y0_std: 337.252, Time: 0.75, Learning Rate: 1.400e-05\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "\n",
      "[InitOutputBias] main:terminal_first b=22, bias_old=-70.719, bias_target=-84.244, bias_new=-84.244\n",
      "[RecursiveBlock] main:terminal_first b=22, idx=[440,452], steps=12, terminal=external, iters=280, lr=8.0e-05\n",
      "It: 0, Loss: 5.373e+04, Y0_mean: -90.958, Y0_std: 346.958, Time: 4.75, Learning Rate: 8.000e-05\n",
      "It: 10, Loss: 1.029e+04, Y0_mean: -108.229, Y0_std: 390.466, Time: 0.34, Learning Rate: 8.000e-05\n",
      "It: 20, Loss: 4.328e+03, Y0_mean: -105.172, Y0_std: 407.316, Time: 0.34, Learning Rate: 8.000e-05\n",
      "It: 30, Loss: 2.783e+03, Y0_mean: -92.628, Y0_std: 388.862, Time: 0.33, Learning Rate: 8.000e-05\n",
      "It: 40, Loss: 1.770e+03, Y0_mean: -105.402, Y0_std: 397.194, Time: 0.33, Learning Rate: 8.000e-05\n",
      "It: 50, Loss: 1.547e+03, Y0_mean: -100.260, Y0_std: 394.353, Time: 0.34, Learning Rate: 8.000e-05\n",
      "It: 60, Loss: 1.556e+03, Y0_mean: -91.703, Y0_std: 385.738, Time: 0.34, Learning Rate: 8.000e-05\n",
      "It: 70, Loss: 1.215e+03, Y0_mean: -101.592, Y0_std: 392.560, Time: 0.34, Learning Rate: 8.000e-05\n",
      "It: 80, Loss: 1.065e+03, Y0_mean: -96.824, Y0_std: 386.297, Time: 0.34, Learning Rate: 8.000e-05\n",
      "It: 90, Loss: 6.840e+02, Y0_mean: -101.853, Y0_std: 398.076, Time: 0.33, Learning Rate: 8.000e-05\n",
      "It: 100, Loss: 7.469e+02, Y0_mean: -93.484, Y0_std: 377.090, Time: 0.33, Learning Rate: 8.000e-05\n",
      "It: 110, Loss: 5.437e+02, Y0_mean: -100.833, Y0_std: 394.597, Time: 0.33, Learning Rate: 8.000e-05\n",
      "It: 120, Loss: 6.256e+02, Y0_mean: -93.014, Y0_std: 374.312, Time: 0.34, Learning Rate: 8.000e-05\n",
      "It: 130, Loss: 5.735e+02, Y0_mean: -107.627, Y0_std: 388.429, Time: 0.34, Learning Rate: 8.000e-05\n",
      "It: 140, Loss: 4.268e+02, Y0_mean: -99.774, Y0_std: 391.023, Time: 0.34, Learning Rate: 8.000e-05\n",
      "It: 150, Loss: 5.351e+02, Y0_mean: -102.726, Y0_std: 380.751, Time: 0.34, Learning Rate: 8.000e-05\n",
      "It: 160, Loss: 5.406e+02, Y0_mean: -90.115, Y0_std: 371.797, Time: 0.34, Learning Rate: 8.000e-05\n",
      "It: 170, Loss: 4.693e+02, Y0_mean: -105.719, Y0_std: 384.137, Time: 0.34, Learning Rate: 8.000e-05\n",
      "It: 180, Loss: 3.406e+02, Y0_mean: -98.064, Y0_std: 385.776, Time: 0.34, Learning Rate: 8.000e-05\n",
      "It: 190, Loss: 3.299e+02, Y0_mean: -97.708, Y0_std: 384.667, Time: 0.34, Learning Rate: 8.000e-05\n",
      "It: 200, Loss: 3.159e+02, Y0_mean: -96.536, Y0_std: 383.468, Time: 0.34, Learning Rate: 8.000e-05\n",
      "It: 210, Loss: 3.143e+02, Y0_mean: -97.000, Y0_std: 382.215, Time: 0.33, Learning Rate: 8.000e-05\n",
      "It: 220, Loss: 3.893e+02, Y0_mean: -104.058, Y0_std: 378.752, Time: 0.33, Learning Rate: 8.000e-05\n",
      "It: 230, Loss: 3.881e+02, Y0_mean: -103.796, Y0_std: 377.637, Time: 0.33, Learning Rate: 8.000e-05\n",
      "It: 240, Loss: 3.543e+02, Y0_mean: -95.818, Y0_std: 371.680, Time: 0.33, Learning Rate: 8.000e-05\n",
      "It: 250, Loss: 3.832e+02, Y0_mean: -99.334, Y0_std: 370.109, Time: 0.33, Learning Rate: 8.000e-05\n",
      "It: 260, Loss: 2.815e+02, Y0_mean: -95.849, Y0_std: 378.390, Time: 0.33, Learning Rate: 8.000e-05\n",
      "It: 270, Loss: 2.744e+02, Y0_mean: -95.564, Y0_std: 377.682, Time: 0.33, Learning Rate: 8.000e-05\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "\n",
      "[InitOutputBias] main:terminal_first b=21, bias_old=-84.244, bias_target=-94.816, bias_new=-94.816\n",
      "[RecursiveBlock] main:terminal_first b=21, idx=[420,440], steps=20, terminal=external, iters=280, lr=8.0e-05\n",
      "It: 0, Loss: 9.061e+03, Y0_mean: -93.585, Y0_std: 362.619, Time: 6.57, Learning Rate: 8.000e-05\n",
      "It: 10, Loss: 1.685e+03, Y0_mean: -103.964, Y0_std: 386.929, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 20, Loss: 7.461e+02, Y0_mean: -102.437, Y0_std: 378.377, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 30, Loss: 4.561e+02, Y0_mean: -103.384, Y0_std: 361.287, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 40, Loss: 3.889e+02, Y0_mean: -111.083, Y0_std: 381.428, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 50, Loss: 3.341e+02, Y0_mean: -102.620, Y0_std: 378.653, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 60, Loss: 2.514e+02, Y0_mean: -105.430, Y0_std: 377.887, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 70, Loss: 2.633e+02, Y0_mean: -103.252, Y0_std: 377.055, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 80, Loss: 1.990e+02, Y0_mean: -105.413, Y0_std: 376.236, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 90, Loss: 1.559e+02, Y0_mean: -96.413, Y0_std: 374.809, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 100, Loss: 1.713e+02, Y0_mean: -103.900, Y0_std: 356.893, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 110, Loss: 1.596e+02, Y0_mean: -104.679, Y0_std: 373.535, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 120, Loss: 1.481e+02, Y0_mean: -95.316, Y0_std: 372.613, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 130, Loss: 1.824e+02, Y0_mean: -103.389, Y0_std: 364.732, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 140, Loss: 1.247e+02, Y0_mean: -95.491, Y0_std: 370.642, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 150, Loss: 1.319e+02, Y0_mean: -102.912, Y0_std: 353.304, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 160, Loss: 1.381e+02, Y0_mean: -109.542, Y0_std: 372.034, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 170, Loss: 1.337e+02, Y0_mean: -109.308, Y0_std: 371.522, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 180, Loss: 1.305e+02, Y0_mean: -109.084, Y0_std: 370.812, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 190, Loss: 1.274e+02, Y0_mean: -109.119, Y0_std: 370.538, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 200, Loss: 1.389e+02, Y0_mean: -102.718, Y0_std: 360.281, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 210, Loss: 1.350e+02, Y0_mean: -102.398, Y0_std: 360.002, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 220, Loss: 1.313e+02, Y0_mean: -103.504, Y0_std: 374.603, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 230, Loss: 1.106e+02, Y0_mean: -102.913, Y0_std: 365.971, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 240, Loss: 1.097e+02, Y0_mean: -101.502, Y0_std: 348.458, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 250, Loss: 1.050e+02, Y0_mean: -102.525, Y0_std: 365.590, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 260, Loss: 9.637e+01, Y0_mean: -93.640, Y0_std: 364.424, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 270, Loss: 9.733e+01, Y0_mean: -93.439, Y0_std: 363.434, Time: 0.56, Learning Rate: 8.000e-05\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "\n",
      "[InitOutputBias] main:terminal_first b=20, bias_old=-94.815, bias_target=-100.125, bias_new=-100.125\n",
      "[RecursiveBlock] main:terminal_first b=20, idx=[400,420], steps=20, terminal=external, iters=280, lr=8.0e-05\n",
      "It: 0, Loss: 5.903e+02, Y0_mean: -111.103, Y0_std: 372.071, Time: 6.36, Learning Rate: 8.000e-05\n",
      "It: 10, Loss: 3.266e+02, Y0_mean: -112.139, Y0_std: 366.461, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 20, Loss: 1.908e+02, Y0_mean: -108.684, Y0_std: 342.928, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 30, Loss: 1.527e+02, Y0_mean: -106.706, Y0_std: 352.510, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 40, Loss: 1.521e+02, Y0_mean: -113.312, Y0_std: 363.278, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 50, Loss: 1.380e+02, Y0_mean: -113.278, Y0_std: 362.945, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 60, Loss: 1.110e+02, Y0_mean: -112.433, Y0_std: 370.099, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 70, Loss: 1.065e+02, Y0_mean: -102.631, Y0_std: 358.751, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 80, Loss: 9.071e+01, Y0_mean: -107.093, Y0_std: 349.880, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 90, Loss: 9.268e+01, Y0_mean: -108.400, Y0_std: 358.809, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 100, Loss: 8.804e+01, Y0_mean: -108.219, Y0_std: 357.784, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 110, Loss: 8.235e+01, Y0_mean: -106.620, Y0_std: 348.374, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 120, Loss: 8.972e+01, Y0_mean: -112.275, Y0_std: 358.374, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 130, Loss: 9.405e+01, Y0_mean: -106.434, Y0_std: 353.769, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 140, Loss: 8.241e+01, Y0_mean: -112.230, Y0_std: 357.971, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 150, Loss: 8.836e+01, Y0_mean: -106.484, Y0_std: 352.996, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 160, Loss: 8.095e+01, Y0_mean: -112.159, Y0_std: 357.501, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 170, Loss: 7.621e+01, Y0_mean: -111.377, Y0_std: 365.243, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 180, Loss: 6.389e+01, Y0_mean: -107.775, Y0_std: 336.528, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 190, Loss: 7.543e+01, Y0_mean: -110.891, Y0_std: 359.595, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 200, Loss: 7.478e+01, Y0_mean: -110.695, Y0_std: 358.975, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 210, Loss: 7.221e+01, Y0_mean: -110.569, Y0_std: 358.769, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 220, Loss: 7.043e+01, Y0_mean: -100.757, Y0_std: 352.408, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 230, Loss: 7.035e+01, Y0_mean: -110.296, Y0_std: 357.898, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 240, Loss: 6.859e+01, Y0_mean: -106.485, Y0_std: 353.025, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 250, Loss: 7.122e+01, Y0_mean: -110.589, Y0_std: 353.886, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 260, Loss: 6.638e+01, Y0_mean: -104.569, Y0_std: 342.992, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 270, Loss: 7.280e+01, Y0_mean: -100.528, Y0_std: 351.960, Time: 0.55, Learning Rate: 8.000e-05\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "\n",
      "[InitOutputBias] main:terminal_first b=19, bias_old=-100.124, bias_target=-106.538, bias_new=-106.538\n",
      "[RecursiveBlock] main:terminal_first b=19, idx=[380,400], steps=20, terminal=external, iters=280, lr=8.0e-05\n",
      "It: 0, Loss: 3.553e+03, Y0_mean: -114.013, Y0_std: 351.726, Time: 6.32, Learning Rate: 8.000e-05\n",
      "It: 10, Loss: 4.008e+02, Y0_mean: -108.217, Y0_std: 333.552, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 20, Loss: 2.204e+02, Y0_mean: -108.149, Y0_std: 334.784, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 30, Loss: 1.929e+02, Y0_mean: -109.736, Y0_std: 327.210, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 40, Loss: 1.214e+02, Y0_mean: -110.638, Y0_std: 339.623, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 50, Loss: 1.014e+02, Y0_mean: -116.114, Y0_std: 339.900, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 60, Loss: 8.969e+01, Y0_mean: -108.894, Y0_std: 335.117, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 70, Loss: 8.944e+01, Y0_mean: -115.984, Y0_std: 338.431, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 80, Loss: 7.515e+01, Y0_mean: -115.764, Y0_std: 340.181, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 90, Loss: 7.949e+01, Y0_mean: -110.544, Y0_std: 337.349, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 100, Loss: 7.166e+01, Y0_mean: -119.254, Y0_std: 337.184, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 110, Loss: 7.409e+01, Y0_mean: -109.510, Y0_std: 324.864, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 120, Loss: 5.581e+01, Y0_mean: -115.121, Y0_std: 317.731, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 130, Loss: 6.675e+01, Y0_mean: -108.150, Y0_std: 332.661, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 140, Loss: 6.621e+01, Y0_mean: -114.931, Y0_std: 336.065, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 150, Loss: 5.588e+01, Y0_mean: -114.704, Y0_std: 316.741, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 160, Loss: 5.652e+01, Y0_mean: -118.339, Y0_std: 335.277, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 170, Loss: 6.135e+01, Y0_mean: -115.354, Y0_std: 338.039, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 180, Loss: 6.403e+01, Y0_mean: -108.532, Y0_std: 323.158, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 190, Loss: 5.287e+01, Y0_mean: -107.276, Y0_std: 330.802, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 200, Loss: 5.638e+01, Y0_mean: -109.182, Y0_std: 334.215, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 210, Loss: 5.262e+01, Y0_mean: -107.042, Y0_std: 330.478, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 220, Loss: 5.241e+01, Y0_mean: -106.943, Y0_std: 329.959, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 230, Loss: 5.595e+01, Y0_mean: -108.046, Y0_std: 321.435, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 240, Loss: 5.348e+01, Y0_mean: -114.476, Y0_std: 336.151, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 250, Loss: 5.225e+01, Y0_mean: -108.552, Y0_std: 333.288, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 260, Loss: 5.403e+01, Y0_mean: -114.216, Y0_std: 335.544, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 270, Loss: 5.451e+01, Y0_mean: -113.380, Y0_std: 332.792, Time: 0.55, Learning Rate: 8.000e-05\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "\n",
      "[InitOutputBias] main:terminal_first b=18, bias_old=-106.537, bias_target=-111.596, bias_new=-111.596\n",
      "[RecursiveBlock] main:terminal_first b=18, idx=[360,380], steps=20, terminal=external, iters=280, lr=8.0e-05\n",
      "It: 0, Loss: 8.458e+03, Y0_mean: -115.798, Y0_std: 329.945, Time: 6.46, Learning Rate: 8.000e-05\n",
      "It: 10, Loss: 9.128e+02, Y0_mean: -123.289, Y0_std: 293.080, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 20, Loss: 2.366e+02, Y0_mean: -116.078, Y0_std: 310.873, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 30, Loss: 1.186e+02, Y0_mean: -118.953, Y0_std: 316.117, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 40, Loss: 8.611e+01, Y0_mean: -116.252, Y0_std: 310.237, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 50, Loss: 7.810e+01, Y0_mean: -119.576, Y0_std: 315.186, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 60, Loss: 6.695e+01, Y0_mean: -116.788, Y0_std: 309.892, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 70, Loss: 6.136e+01, Y0_mean: -119.733, Y0_std: 314.771, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 80, Loss: 5.416e+01, Y0_mean: -118.882, Y0_std: 314.203, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 90, Loss: 5.088e+01, Y0_mean: -120.778, Y0_std: 300.169, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 100, Loss: 4.832e+01, Y0_mean: -116.315, Y0_std: 308.259, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 110, Loss: 4.968e+01, Y0_mean: -118.897, Y0_std: 313.542, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 120, Loss: 4.407e+01, Y0_mean: -123.524, Y0_std: 296.079, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 130, Loss: 4.432e+01, Y0_mean: -118.545, Y0_std: 312.951, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 140, Loss: 4.296e+01, Y0_mean: -118.015, Y0_std: 312.177, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 150, Loss: 4.582e+01, Y0_mean: -118.329, Y0_std: 312.222, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 160, Loss: 4.458e+01, Y0_mean: -118.233, Y0_std: 312.338, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 170, Loss: 4.360e+01, Y0_mean: -115.689, Y0_std: 311.764, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 180, Loss: 4.989e+01, Y0_mean: -114.332, Y0_std: 309.376, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 190, Loss: 4.442e+01, Y0_mean: -117.680, Y0_std: 311.956, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 200, Loss: 4.424e+01, Y0_mean: -117.703, Y0_std: 311.403, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 210, Loss: 4.238e+01, Y0_mean: -114.842, Y0_std: 306.257, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 220, Loss: 4.001e+01, Y0_mean: -118.859, Y0_std: 297.434, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 230, Loss: 4.986e+01, Y0_mean: -113.852, Y0_std: 308.692, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 240, Loss: 4.541e+01, Y0_mean: -117.141, Y0_std: 311.439, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 250, Loss: 3.983e+01, Y0_mean: -114.392, Y0_std: 305.386, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 260, Loss: 4.527e+01, Y0_mean: -113.668, Y0_std: 308.792, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 270, Loss: 4.307e+01, Y0_mean: -117.157, Y0_std: 310.780, Time: 0.54, Learning Rate: 8.000e-05\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "\n",
      "[InitOutputBias] main:terminal_first b=17, bias_old=-111.595, bias_target=-116.605, bias_new=-116.605\n",
      "[RecursiveBlock] main:terminal_first b=17, idx=[340,360], steps=20, terminal=external, iters=280, lr=8.0e-05\n",
      "It: 0, Loss: 1.067e+04, Y0_mean: -120.974, Y0_std: 305.145, Time: 6.17, Learning Rate: 8.000e-05\n",
      "It: 10, Loss: 1.062e+03, Y0_mean: -115.881, Y0_std: 284.054, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 20, Loss: 2.467e+02, Y0_mean: -122.965, Y0_std: 287.508, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 30, Loss: 9.070e+01, Y0_mean: -121.853, Y0_std: 288.452, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 40, Loss: 7.818e+01, Y0_mean: -123.193, Y0_std: 286.586, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 50, Loss: 5.561e+01, Y0_mean: -118.797, Y0_std: 278.948, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 60, Loss: 6.546e+01, Y0_mean: -123.393, Y0_std: 285.273, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 70, Loss: 5.396e+01, Y0_mean: -121.330, Y0_std: 285.746, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 80, Loss: 5.121e+01, Y0_mean: -123.173, Y0_std: 284.982, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 90, Loss: 4.701e+01, Y0_mean: -121.013, Y0_std: 285.303, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 100, Loss: 4.373e+01, Y0_mean: -116.226, Y0_std: 285.894, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 110, Loss: 4.040e+01, Y0_mean: -122.616, Y0_std: 284.572, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 120, Loss: 4.429e+01, Y0_mean: -123.532, Y0_std: 286.706, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 130, Loss: 3.873e+01, Y0_mean: -120.219, Y0_std: 284.113, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 140, Loss: 4.120e+01, Y0_mean: -123.327, Y0_std: 286.327, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 150, Loss: 3.829e+01, Y0_mean: -115.358, Y0_std: 284.788, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 160, Loss: 3.893e+01, Y0_mean: -115.642, Y0_std: 287.644, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 170, Loss: 3.806e+01, Y0_mean: -115.034, Y0_std: 284.390, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 180, Loss: 3.775e+01, Y0_mean: -121.672, Y0_std: 283.496, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 190, Loss: 3.990e+01, Y0_mean: -120.424, Y0_std: 285.857, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 200, Loss: 3.939e+01, Y0_mean: -122.632, Y0_std: 285.674, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 210, Loss: 3.722e+01, Y0_mean: -114.631, Y0_std: 284.014, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 220, Loss: 3.737e+01, Y0_mean: -114.617, Y0_std: 283.934, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 230, Loss: 3.445e+01, Y0_mean: -119.141, Y0_std: 273.365, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 240, Loss: 3.421e+01, Y0_mean: -119.086, Y0_std: 273.306, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 250, Loss: 3.549e+01, Y0_mean: -118.766, Y0_std: 282.639, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 260, Loss: 4.830e+01, Y0_mean: -121.890, Y0_std: 285.483, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 270, Loss: 3.788e+01, Y0_mean: -118.600, Y0_std: 282.623, Time: 0.55, Learning Rate: 8.000e-05\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "\n",
      "[InitOutputBias] main:terminal_first b=16, bias_old=-116.603, bias_target=-117.841, bias_new=-117.841\n",
      "[RecursiveBlock] main:terminal_first b=16, idx=[320,340], steps=20, terminal=external, iters=280, lr=8.0e-05\n",
      "It: 0, Loss: 1.213e+04, Y0_mean: -121.102, Y0_std: 285.471, Time: 6.39, Learning Rate: 8.000e-05\n",
      "It: 10, Loss: 8.518e+02, Y0_mean: -121.347, Y0_std: 260.240, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 20, Loss: 1.883e+02, Y0_mean: -119.330, Y0_std: 263.851, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 30, Loss: 1.207e+02, Y0_mean: -123.402, Y0_std: 247.754, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 40, Loss: 7.281e+01, Y0_mean: -119.575, Y0_std: 263.245, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 50, Loss: 5.085e+01, Y0_mean: -119.802, Y0_std: 263.470, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 60, Loss: 4.985e+01, Y0_mean: -120.212, Y0_std: 262.905, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 70, Loss: 4.105e+01, Y0_mean: -122.675, Y0_std: 264.615, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 80, Loss: 3.715e+01, Y0_mean: -121.943, Y0_std: 266.394, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 90, Loss: 3.632e+01, Y0_mean: -122.208, Y0_std: 264.208, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 100, Loss: 3.274e+01, Y0_mean: -119.279, Y0_std: 264.345, Time: 0.57, Learning Rate: 8.000e-05\n",
      "It: 110, Loss: 3.368e+01, Y0_mean: -119.403, Y0_std: 262.149, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 120, Loss: 3.258e+01, Y0_mean: -118.961, Y0_std: 264.109, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 130, Loss: 3.161e+01, Y0_mean: -118.003, Y0_std: 261.487, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 140, Loss: 3.459e+01, Y0_mean: -121.742, Y0_std: 263.619, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 150, Loss: 3.517e+01, Y0_mean: -121.158, Y0_std: 264.990, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 160, Loss: 3.228e+01, Y0_mean: -119.018, Y0_std: 261.673, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 170, Loss: 3.370e+01, Y0_mean: -118.601, Y0_std: 263.801, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 180, Loss: 4.103e+01, Y0_mean: -119.132, Y0_std: 260.814, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 190, Loss: 3.445e+01, Y0_mean: -118.483, Y0_std: 263.721, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 200, Loss: 3.649e+01, Y0_mean: -118.547, Y0_std: 263.079, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 210, Loss: 3.367e+01, Y0_mean: -120.760, Y0_std: 264.609, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 220, Loss: 3.351e+01, Y0_mean: -121.273, Y0_std: 262.571, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 230, Loss: 3.464e+01, Y0_mean: -121.154, Y0_std: 262.972, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 240, Loss: 3.280e+01, Y0_mean: -122.538, Y0_std: 246.314, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 250, Loss: 3.071e+01, Y0_mean: -122.463, Y0_std: 246.362, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 260, Loss: 3.431e+01, Y0_mean: -118.377, Y0_std: 261.044, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 270, Loss: 3.180e+01, Y0_mean: -118.099, Y0_std: 262.787, Time: 0.54, Learning Rate: 8.000e-05\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "\n",
      "[InitOutputBias] main:terminal_first b=15, bias_old=-117.841, bias_target=-119.744, bias_new=-119.744\n",
      "[RecursiveBlock] main:terminal_first b=15, idx=[300,320], steps=20, terminal=external, iters=280, lr=8.0e-05\n",
      "It: 0, Loss: 1.256e+04, Y0_mean: -119.186, Y0_std: 256.751, Time: 6.48, Learning Rate: 8.000e-05\n",
      "It: 10, Loss: 7.176e+02, Y0_mean: -118.828, Y0_std: 234.361, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 20, Loss: 2.144e+02, Y0_mean: -121.057, Y0_std: 236.179, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 30, Loss: 8.802e+01, Y0_mean: -122.911, Y0_std: 223.492, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 40, Loss: 6.227e+01, Y0_mean: -118.766, Y0_std: 233.369, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 50, Loss: 4.777e+01, Y0_mean: -122.741, Y0_std: 242.654, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 60, Loss: 4.073e+01, Y0_mean: -120.523, Y0_std: 238.132, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 70, Loss: 3.706e+01, Y0_mean: -122.614, Y0_std: 242.138, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 80, Loss: 3.291e+01, Y0_mean: -116.176, Y0_std: 235.434, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 90, Loss: 3.274e+01, Y0_mean: -118.196, Y0_std: 239.237, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 100, Loss: 3.190e+01, Y0_mean: -122.379, Y0_std: 241.845, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 110, Loss: 2.914e+01, Y0_mean: -124.140, Y0_std: 228.773, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 120, Loss: 2.988e+01, Y0_mean: -121.272, Y0_std: 234.580, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 130, Loss: 2.978e+01, Y0_mean: -123.902, Y0_std: 228.385, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 140, Loss: 2.866e+01, Y0_mean: -123.820, Y0_std: 228.494, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 150, Loss: 2.972e+01, Y0_mean: -120.958, Y0_std: 234.245, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 160, Loss: 3.108e+01, Y0_mean: -119.392, Y0_std: 237.412, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 170, Loss: 2.943e+01, Y0_mean: -123.547, Y0_std: 228.475, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 180, Loss: 3.135e+01, Y0_mean: -118.142, Y0_std: 232.112, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 190, Loss: 3.308e+01, Y0_mean: -119.314, Y0_std: 237.395, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 200, Loss: 3.190e+01, Y0_mean: -119.335, Y0_std: 236.741, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 210, Loss: 3.517e+01, Y0_mean: -123.361, Y0_std: 227.695, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 220, Loss: 3.356e+01, Y0_mean: -121.393, Y0_std: 241.243, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 230, Loss: 3.462e+01, Y0_mean: -115.048, Y0_std: 234.754, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 240, Loss: 3.170e+01, Y0_mean: -115.098, Y0_std: 234.018, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 250, Loss: 3.415e+01, Y0_mean: -120.536, Y0_std: 233.622, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 260, Loss: 3.121e+01, Y0_mean: -120.414, Y0_std: 234.189, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 270, Loss: 3.198e+01, Y0_mean: -122.075, Y0_std: 223.213, Time: 0.54, Learning Rate: 8.000e-05\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "\n",
      "[InitOutputBias] main:terminal_first b=14, bias_old=-119.743, bias_target=-119.458, bias_new=-119.458\n",
      "[RecursiveBlock] main:terminal_first b=14, idx=[280,300], steps=20, terminal=external, iters=280, lr=8.0e-05\n",
      "It: 0, Loss: 1.139e+04, Y0_mean: -119.020, Y0_std: 235.561, Time: 6.72, Learning Rate: 8.000e-05\n",
      "It: 10, Loss: 6.279e+02, Y0_mean: -118.716, Y0_std: 214.569, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 20, Loss: 1.411e+02, Y0_mean: -119.152, Y0_std: 218.355, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 30, Loss: 8.130e+01, Y0_mean: -120.310, Y0_std: 208.669, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 40, Loss: 5.161e+01, Y0_mean: -117.870, Y0_std: 204.732, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 50, Loss: 3.555e+01, Y0_mean: -118.055, Y0_std: 204.783, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 60, Loss: 3.588e+01, Y0_mean: -120.483, Y0_std: 217.759, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 70, Loss: 2.900e+01, Y0_mean: -120.976, Y0_std: 209.235, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 80, Loss: 2.921e+01, Y0_mean: -115.796, Y0_std: 214.613, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 90, Loss: 2.565e+01, Y0_mean: -119.070, Y0_std: 214.073, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 100, Loss: 2.522e+01, Y0_mean: -119.044, Y0_std: 214.020, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 110, Loss: 2.451e+01, Y0_mean: -120.666, Y0_std: 209.108, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 120, Loss: 2.434e+01, Y0_mean: -120.610, Y0_std: 209.007, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 130, Loss: 2.760e+01, Y0_mean: -120.049, Y0_std: 217.150, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 140, Loss: 2.599e+01, Y0_mean: -115.476, Y0_std: 214.337, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 150, Loss: 2.407e+01, Y0_mean: -120.398, Y0_std: 208.842, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 160, Loss: 2.456e+01, Y0_mean: -117.644, Y0_std: 203.853, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 170, Loss: 2.380e+01, Y0_mean: -120.386, Y0_std: 208.727, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 180, Loss: 2.679e+01, Y0_mean: -120.635, Y0_std: 216.195, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 190, Loss: 2.592e+01, Y0_mean: -119.241, Y0_std: 216.447, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 200, Loss: 2.715e+01, Y0_mean: -115.259, Y0_std: 214.163, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 210, Loss: 2.608e+01, Y0_mean: -119.311, Y0_std: 217.142, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 220, Loss: 2.419e+01, Y0_mean: -117.440, Y0_std: 203.796, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 230, Loss: 2.668e+01, Y0_mean: -119.623, Y0_std: 216.638, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 240, Loss: 2.387e+01, Y0_mean: -120.113, Y0_std: 208.289, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 250, Loss: 3.132e+01, Y0_mean: -119.707, Y0_std: 216.485, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 260, Loss: 2.557e+01, Y0_mean: -115.148, Y0_std: 213.714, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 270, Loss: 2.719e+01, Y0_mean: -119.626, Y0_std: 216.306, Time: 0.56, Learning Rate: 8.000e-05\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "\n",
      "[InitOutputBias] main:terminal_first b=13, bias_old=-119.459, bias_target=-118.635, bias_new=-118.635\n",
      "[RecursiveBlock] main:terminal_first b=13, idx=[260,280], steps=20, terminal=external, iters=280, lr=8.0e-05\n",
      "It: 0, Loss: 1.134e+04, Y0_mean: -122.338, Y0_std: 217.152, Time: 6.57, Learning Rate: 8.000e-05\n",
      "It: 10, Loss: 5.648e+02, Y0_mean: -117.441, Y0_std: 190.627, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 20, Loss: 1.380e+02, Y0_mean: -113.440, Y0_std: 193.304, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 30, Loss: 8.435e+01, Y0_mean: -113.648, Y0_std: 192.848, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 40, Loss: 5.839e+01, Y0_mean: -116.268, Y0_std: 191.825, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 50, Loss: 3.859e+01, Y0_mean: -118.805, Y0_std: 194.107, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 60, Loss: 2.954e+01, Y0_mean: -120.305, Y0_std: 186.774, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 70, Loss: 2.900e+01, Y0_mean: -123.007, Y0_std: 199.492, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 80, Loss: 2.506e+01, Y0_mean: -120.451, Y0_std: 186.546, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 90, Loss: 2.359e+01, Y0_mean: -120.377, Y0_std: 186.506, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 100, Loss: 2.239e+01, Y0_mean: -122.787, Y0_std: 184.359, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 110, Loss: 2.398e+01, Y0_mean: -118.816, Y0_std: 193.609, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 120, Loss: 2.365e+01, Y0_mean: -116.432, Y0_std: 193.979, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 130, Loss: 2.457e+01, Y0_mean: -116.301, Y0_std: 190.659, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 140, Loss: 2.217e+01, Y0_mean: -122.671, Y0_std: 184.325, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 150, Loss: 2.437e+01, Y0_mean: -113.911, Y0_std: 193.147, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 160, Loss: 2.417e+01, Y0_mean: -114.002, Y0_std: 193.210, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 170, Loss: 2.274e+01, Y0_mean: -120.139, Y0_std: 186.090, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 180, Loss: 2.326e+01, Y0_mean: -120.109, Y0_std: 186.242, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 190, Loss: 2.434e+01, Y0_mean: -116.294, Y0_std: 190.610, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 200, Loss: 2.470e+01, Y0_mean: -122.662, Y0_std: 198.717, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 210, Loss: 2.398e+01, Y0_mean: -119.200, Y0_std: 195.863, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 220, Loss: 2.260e+01, Y0_mean: -120.012, Y0_std: 186.031, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 230, Loss: 2.236e+01, Y0_mean: -116.226, Y0_std: 193.465, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 240, Loss: 2.744e+01, Y0_mean: -122.693, Y0_std: 198.573, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 250, Loss: 2.464e+01, Y0_mean: -119.918, Y0_std: 185.913, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 260, Loss: 2.330e+01, Y0_mean: -116.170, Y0_std: 190.214, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 270, Loss: 2.346e+01, Y0_mean: -116.114, Y0_std: 190.159, Time: 0.54, Learning Rate: 8.000e-05\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "\n",
      "[InitOutputBias] main:terminal_first b=12, bias_old=-118.637, bias_target=-118.613, bias_new=-118.613\n",
      "[RecursiveBlock] main:terminal_first b=12, idx=[240,260], steps=20, terminal=external, iters=280, lr=8.0e-05\n",
      "It: 0, Loss: 1.020e+04, Y0_mean: -119.782, Y0_std: 187.667, Time: 7.08, Learning Rate: 8.000e-05\n",
      "It: 10, Loss: 4.651e+02, Y0_mean: -118.997, Y0_std: 168.434, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 20, Loss: 1.484e+02, Y0_mean: -115.514, Y0_std: 170.251, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 30, Loss: 6.791e+01, Y0_mean: -117.310, Y0_std: 172.218, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 40, Loss: 4.400e+01, Y0_mean: -119.621, Y0_std: 174.450, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 50, Loss: 3.036e+01, Y0_mean: -116.133, Y0_std: 170.806, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 60, Loss: 2.432e+01, Y0_mean: -122.126, Y0_std: 169.633, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 70, Loss: 2.247e+01, Y0_mean: -122.107, Y0_std: 169.403, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 80, Loss: 2.153e+01, Y0_mean: -117.852, Y0_std: 172.574, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 90, Loss: 2.253e+01, Y0_mean: -117.867, Y0_std: 172.840, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 100, Loss: 2.154e+01, Y0_mean: -122.084, Y0_std: 169.213, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 110, Loss: 2.210e+01, Y0_mean: -120.427, Y0_std: 170.831, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 120, Loss: 2.260e+01, Y0_mean: -119.727, Y0_std: 171.702, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 130, Loss: 2.122e+01, Y0_mean: -119.760, Y0_std: 174.058, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 140, Loss: 2.360e+01, Y0_mean: -119.619, Y0_std: 171.723, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 150, Loss: 2.386e+01, Y0_mean: -120.353, Y0_std: 170.653, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 160, Loss: 2.068e+01, Y0_mean: -119.691, Y0_std: 174.149, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 170, Loss: 2.063e+01, Y0_mean: -121.792, Y0_std: 169.204, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 180, Loss: 2.080e+01, Y0_mean: -124.325, Y0_std: 164.667, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 190, Loss: 2.017e+01, Y0_mean: -124.307, Y0_std: 164.865, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 200, Loss: 2.131e+01, Y0_mean: -119.578, Y0_std: 174.108, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 210, Loss: 2.022e+01, Y0_mean: -124.139, Y0_std: 164.609, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 220, Loss: 2.130e+01, Y0_mean: -119.532, Y0_std: 174.058, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 230, Loss: 2.004e+01, Y0_mean: -117.454, Y0_std: 172.136, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 240, Loss: 1.952e+01, Y0_mean: -124.098, Y0_std: 164.496, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 250, Loss: 2.005e+01, Y0_mean: -124.104, Y0_std: 164.408, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 260, Loss: 1.971e+01, Y0_mean: -121.642, Y0_std: 168.899, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 270, Loss: 2.092e+01, Y0_mean: -119.439, Y0_std: 173.630, Time: 0.55, Learning Rate: 8.000e-05\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "\n",
      "[InitOutputBias] main:terminal_first b=11, bias_old=-118.612, bias_target=-119.212, bias_new=-119.212\n",
      "[RecursiveBlock] main:terminal_first b=11, idx=[220,240], steps=20, terminal=external, iters=280, lr=8.0e-05\n",
      "It: 0, Loss: 8.670e+03, Y0_mean: -118.839, Y0_std: 161.571, Time: 6.72, Learning Rate: 8.000e-05\n",
      "It: 10, Loss: 3.921e+02, Y0_mean: -118.703, Y0_std: 151.252, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 20, Loss: 1.401e+02, Y0_mean: -121.522, Y0_std: 153.643, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 30, Loss: 6.263e+01, Y0_mean: -119.548, Y0_std: 153.015, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 40, Loss: 3.879e+01, Y0_mean: -119.737, Y0_std: 153.426, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 50, Loss: 2.710e+01, Y0_mean: -119.252, Y0_std: 150.505, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 60, Loss: 2.083e+01, Y0_mean: -119.622, Y0_std: 146.150, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 70, Loss: 1.994e+01, Y0_mean: -118.912, Y0_std: 154.803, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 80, Loss: 1.870e+01, Y0_mean: -118.893, Y0_std: 154.819, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 90, Loss: 1.876e+01, Y0_mean: -119.270, Y0_std: 150.163, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 100, Loss: 2.026e+01, Y0_mean: -118.462, Y0_std: 154.518, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 110, Loss: 1.848e+01, Y0_mean: -119.128, Y0_std: 150.070, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 120, Loss: 1.856e+01, Y0_mean: -122.542, Y0_std: 156.646, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 130, Loss: 1.862e+01, Y0_mean: -119.090, Y0_std: 149.903, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 140, Loss: 1.877e+01, Y0_mean: -121.470, Y0_std: 155.084, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 150, Loss: 1.842e+01, Y0_mean: -121.447, Y0_std: 154.923, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 160, Loss: 1.845e+01, Y0_mean: -119.728, Y0_std: 153.022, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 170, Loss: 1.883e+01, Y0_mean: -118.994, Y0_std: 149.876, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 180, Loss: 1.841e+01, Y0_mean: -118.931, Y0_std: 149.885, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 190, Loss: 1.854e+01, Y0_mean: -118.961, Y0_std: 149.880, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 200, Loss: 2.139e+01, Y0_mean: -118.153, Y0_std: 154.053, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 210, Loss: 1.840e+01, Y0_mean: -121.258, Y0_std: 154.687, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 220, Loss: 1.775e+01, Y0_mean: -119.600, Y0_std: 152.969, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 230, Loss: 1.805e+01, Y0_mean: -119.580, Y0_std: 152.883, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 240, Loss: 1.802e+01, Y0_mean: -122.245, Y0_std: 156.149, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 250, Loss: 1.758e+01, Y0_mean: -118.357, Y0_std: 154.147, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 260, Loss: 1.913e+01, Y0_mean: -118.085, Y0_std: 154.037, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 270, Loss: 1.838e+01, Y0_mean: -121.141, Y0_std: 154.553, Time: 0.56, Learning Rate: 8.000e-05\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "\n",
      "[InitOutputBias] main:terminal_first b=10, bias_old=-119.211, bias_target=-119.865, bias_new=-119.865\n",
      "[RecursiveBlock] main:terminal_first b=10, idx=[200,220], steps=20, terminal=external, iters=280, lr=8.0e-05\n",
      "It: 0, Loss: 9.174e+03, Y0_mean: -119.314, Y0_std: 149.391, Time: 7.09, Learning Rate: 8.000e-05\n",
      "It: 10, Loss: 3.065e+02, Y0_mean: -118.054, Y0_std: 129.758, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 20, Loss: 1.441e+02, Y0_mean: -118.308, Y0_std: 128.106, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 30, Loss: 5.075e+01, Y0_mean: -122.086, Y0_std: 134.492, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 40, Loss: 3.377e+01, Y0_mean: -119.679, Y0_std: 133.849, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 50, Loss: 2.252e+01, Y0_mean: -122.118, Y0_std: 137.006, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 60, Loss: 2.113e+01, Y0_mean: -122.207, Y0_std: 136.740, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 70, Loss: 1.819e+01, Y0_mean: -122.161, Y0_std: 136.726, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 80, Loss: 1.649e+01, Y0_mean: -121.824, Y0_std: 129.506, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 90, Loss: 1.741e+01, Y0_mean: -119.239, Y0_std: 131.376, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 100, Loss: 1.708e+01, Y0_mean: -119.163, Y0_std: 131.198, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 110, Loss: 1.694e+01, Y0_mean: -118.344, Y0_std: 134.368, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 120, Loss: 1.769e+01, Y0_mean: -119.694, Y0_std: 133.819, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 130, Loss: 1.945e+01, Y0_mean: -119.071, Y0_std: 130.965, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 140, Loss: 1.753e+01, Y0_mean: -122.277, Y0_std: 134.840, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 150, Loss: 1.738e+01, Y0_mean: -118.983, Y0_std: 131.287, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 160, Loss: 1.704e+01, Y0_mean: -121.863, Y0_std: 136.501, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 170, Loss: 1.580e+01, Y0_mean: -121.503, Y0_std: 129.522, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 180, Loss: 1.726e+01, Y0_mean: -119.562, Y0_std: 133.747, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 190, Loss: 1.876e+01, Y0_mean: -122.440, Y0_std: 134.405, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 200, Loss: 1.576e+01, Y0_mean: -118.491, Y0_std: 128.584, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 210, Loss: 1.633e+01, Y0_mean: -121.696, Y0_std: 136.555, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 220, Loss: 1.637e+01, Y0_mean: -118.043, Y0_std: 134.113, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 230, Loss: 1.734e+01, Y0_mean: -122.325, Y0_std: 134.544, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 240, Loss: 1.688e+01, Y0_mean: -118.789, Y0_std: 131.045, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 250, Loss: 1.727e+01, Y0_mean: -122.252, Y0_std: 134.374, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 260, Loss: 1.638e+01, Y0_mean: -117.971, Y0_std: 134.196, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 270, Loss: 1.568e+01, Y0_mean: -118.338, Y0_std: 128.229, Time: 0.56, Learning Rate: 8.000e-05\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "\n",
      "[InitOutputBias] main:terminal_first b=9, bias_old=-119.865, bias_target=-120.122, bias_new=-120.122\n",
      "[RecursiveBlock] main:terminal_first b=9, idx=[180,200], steps=20, terminal=external, iters=280, lr=8.0e-05\n",
      "It: 0, Loss: 7.817e+03, Y0_mean: -120.241, Y0_std: 131.425, Time: 6.92, Learning Rate: 8.000e-05\n",
      "It: 10, Loss: 3.050e+02, Y0_mean: -120.375, Y0_std: 113.730, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 20, Loss: 1.461e+02, Y0_mean: -120.973, Y0_std: 114.845, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 30, Loss: 5.320e+01, Y0_mean: -117.073, Y0_std: 113.344, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 40, Loss: 3.246e+01, Y0_mean: -118.644, Y0_std: 117.368, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 50, Loss: 1.965e+01, Y0_mean: -122.362, Y0_std: 113.232, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 60, Loss: 1.773e+01, Y0_mean: -118.812, Y0_std: 117.254, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 70, Loss: 1.614e+01, Y0_mean: -118.859, Y0_std: 117.139, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 80, Loss: 1.551e+01, Y0_mean: -125.518, Y0_std: 120.677, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 90, Loss: 1.678e+01, Y0_mean: -121.592, Y0_std: 115.373, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 100, Loss: 1.444e+01, Y0_mean: -122.296, Y0_std: 113.015, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 110, Loss: 1.560e+01, Y0_mean: -125.354, Y0_std: 120.597, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 120, Loss: 1.534e+01, Y0_mean: -125.420, Y0_std: 120.693, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 130, Loss: 1.533e+01, Y0_mean: -121.429, Y0_std: 115.185, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 140, Loss: 1.516e+01, Y0_mean: -121.470, Y0_std: 115.273, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 150, Loss: 1.458e+01, Y0_mean: -120.339, Y0_std: 115.354, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 160, Loss: 1.451e+01, Y0_mean: -120.284, Y0_std: 113.888, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 170, Loss: 1.512e+01, Y0_mean: -121.345, Y0_std: 115.108, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 180, Loss: 1.557e+01, Y0_mean: -125.262, Y0_std: 120.656, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 190, Loss: 1.614e+01, Y0_mean: -120.574, Y0_std: 116.643, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 200, Loss: 1.557e+01, Y0_mean: -121.309, Y0_std: 115.179, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 210, Loss: 1.483e+01, Y0_mean: -117.157, Y0_std: 113.307, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 220, Loss: 1.549e+01, Y0_mean: -120.224, Y0_std: 114.992, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 230, Loss: 1.459e+01, Y0_mean: -120.533, Y0_std: 116.927, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 240, Loss: 1.486e+01, Y0_mean: -120.509, Y0_std: 116.937, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 250, Loss: 1.540e+01, Y0_mean: -117.139, Y0_std: 112.919, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 260, Loss: 1.455e+01, Y0_mean: -120.505, Y0_std: 116.867, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 270, Loss: 1.478e+01, Y0_mean: -120.181, Y0_std: 115.248, Time: 0.54, Learning Rate: 8.000e-05\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "\n",
      "[InitOutputBias] main:terminal_first b=8, bias_old=-120.122, bias_target=-120.590, bias_new=-120.590\n",
      "[RecursiveBlock] main:terminal_first b=8, idx=[160,180], steps=20, terminal=external, iters=280, lr=8.0e-05\n",
      "It: 0, Loss: 7.397e+03, Y0_mean: -119.128, Y0_std: 112.304, Time: 6.99, Learning Rate: 8.000e-05\n",
      "It: 10, Loss: 2.423e+02, Y0_mean: -117.899, Y0_std: 95.063, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 20, Loss: 1.707e+02, Y0_mean: -119.616, Y0_std: 98.242, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 30, Loss: 5.029e+01, Y0_mean: -122.065, Y0_std: 100.637, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 40, Loss: 2.476e+01, Y0_mean: -122.441, Y0_std: 100.854, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 50, Loss: 1.652e+01, Y0_mean: -120.236, Y0_std: 99.354, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 60, Loss: 1.360e+01, Y0_mean: -120.252, Y0_std: 97.220, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 70, Loss: 1.345e+01, Y0_mean: -119.027, Y0_std: 95.449, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 80, Loss: 1.370e+01, Y0_mean: -122.559, Y0_std: 100.716, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 90, Loss: 1.281e+01, Y0_mean: -119.246, Y0_std: 93.362, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 100, Loss: 1.290e+01, Y0_mean: -120.194, Y0_std: 99.236, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 110, Loss: 1.374e+01, Y0_mean: -119.605, Y0_std: 98.470, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 120, Loss: 1.256e+01, Y0_mean: -119.103, Y0_std: 93.242, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 130, Loss: 1.409e+01, Y0_mean: -122.431, Y0_std: 100.785, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 140, Loss: 1.376e+01, Y0_mean: -119.502, Y0_std: 98.438, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 150, Loss: 1.266e+01, Y0_mean: -118.839, Y0_std: 95.579, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 160, Loss: 1.294e+01, Y0_mean: -120.007, Y0_std: 99.056, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 170, Loss: 1.237e+01, Y0_mean: -120.067, Y0_std: 97.138, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 180, Loss: 1.231e+01, Y0_mean: -118.980, Y0_std: 93.126, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 190, Loss: 1.259e+01, Y0_mean: -118.253, Y0_std: 100.326, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 200, Loss: 1.347e+01, Y0_mean: -122.215, Y0_std: 100.480, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 210, Loss: 1.373e+01, Y0_mean: -122.260, Y0_std: 100.579, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 220, Loss: 1.205e+01, Y0_mean: -119.945, Y0_std: 96.995, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 230, Loss: 1.224e+01, Y0_mean: -119.966, Y0_std: 96.977, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 240, Loss: 1.238e+01, Y0_mean: -119.896, Y0_std: 96.941, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 250, Loss: 1.200e+01, Y0_mean: -118.877, Y0_std: 93.025, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 260, Loss: 1.199e+01, Y0_mean: -119.872, Y0_std: 96.863, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 270, Loss: 1.420e+01, Y0_mean: -121.519, Y0_std: 98.626, Time: 0.55, Learning Rate: 8.000e-05\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "\n",
      "[InitOutputBias] main:terminal_first b=7, bias_old=-120.589, bias_target=-119.758, bias_new=-119.758\n",
      "[RecursiveBlock] main:terminal_first b=7, idx=[140,160], steps=20, terminal=external, iters=280, lr=8.0e-05\n",
      "It: 0, Loss: 6.436e+03, Y0_mean: -117.882, Y0_std: 93.478, Time: 7.16, Learning Rate: 8.000e-05\n",
      "It: 10, Loss: 2.092e+02, Y0_mean: -117.425, Y0_std: 80.604, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 20, Loss: 1.785e+02, Y0_mean: -118.481, Y0_std: 78.914, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 30, Loss: 3.945e+01, Y0_mean: -119.473, Y0_std: 81.408, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 40, Loss: 2.507e+01, Y0_mean: -118.681, Y0_std: 83.593, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 50, Loss: 1.540e+01, Y0_mean: -118.371, Y0_std: 80.852, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 60, Loss: 1.216e+01, Y0_mean: -119.761, Y0_std: 80.951, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 70, Loss: 1.168e+01, Y0_mean: -118.631, Y0_std: 81.037, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 80, Loss: 1.130e+01, Y0_mean: -117.852, Y0_std: 83.289, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 90, Loss: 1.193e+01, Y0_mean: -118.852, Y0_std: 83.186, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 100, Loss: 1.140e+01, Y0_mean: -118.425, Y0_std: 80.793, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 110, Loss: 1.133e+01, Y0_mean: -118.779, Y0_std: 82.546, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 120, Loss: 1.165e+01, Y0_mean: -118.825, Y0_std: 83.130, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 130, Loss: 1.108e+01, Y0_mean: -118.437, Y0_std: 80.762, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 140, Loss: 1.089e+01, Y0_mean: -119.807, Y0_std: 80.983, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 150, Loss: 1.082e+01, Y0_mean: -118.380, Y0_std: 80.762, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 160, Loss: 1.078e+01, Y0_mean: -118.358, Y0_std: 80.595, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 170, Loss: 1.084e+01, Y0_mean: -119.785, Y0_std: 81.004, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 180, Loss: 1.107e+01, Y0_mean: -118.569, Y0_std: 80.955, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 190, Loss: 1.189e+01, Y0_mean: -118.783, Y0_std: 82.221, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 200, Loss: 1.078e+01, Y0_mean: -118.327, Y0_std: 80.529, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 210, Loss: 1.110e+01, Y0_mean: -118.337, Y0_std: 80.468, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 220, Loss: 1.195e+01, Y0_mean: -118.650, Y0_std: 82.544, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 230, Loss: 1.056e+01, Y0_mean: -118.513, Y0_std: 80.842, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 240, Loss: 1.067e+01, Y0_mean: -118.521, Y0_std: 80.830, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 250, Loss: 1.222e+01, Y0_mean: -118.718, Y0_std: 82.139, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 260, Loss: 1.086e+01, Y0_mean: -118.637, Y0_std: 82.286, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 270, Loss: 1.091e+01, Y0_mean: -118.479, Y0_std: 80.755, Time: 0.55, Learning Rate: 8.000e-05\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "\n",
      "[InitOutputBias] main:terminal_first b=6, bias_old=-119.760, bias_target=-118.646, bias_new=-118.646\n",
      "[RecursiveBlock] main:terminal_first b=6, idx=[120,140], steps=20, terminal=external, iters=280, lr=8.0e-05\n",
      "It: 0, Loss: 6.607e+03, Y0_mean: -116.978, Y0_std: 80.619, Time: 7.14, Learning Rate: 8.000e-05\n",
      "It: 10, Loss: 2.368e+02, Y0_mean: -117.133, Y0_std: 67.525, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 20, Loss: 2.346e+02, Y0_mean: -115.830, Y0_std: 65.392, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 30, Loss: 6.338e+01, Y0_mean: -117.606, Y0_std: 67.534, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 40, Loss: 1.717e+01, Y0_mean: -117.329, Y0_std: 68.582, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 50, Loss: 1.303e+01, Y0_mean: -118.615, Y0_std: 67.918, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 60, Loss: 1.113e+01, Y0_mean: -117.283, Y0_std: 68.143, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 70, Loss: 1.019e+01, Y0_mean: -116.743, Y0_std: 66.434, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 80, Loss: 1.045e+01, Y0_mean: -117.215, Y0_std: 66.117, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 90, Loss: 9.688e+00, Y0_mean: -116.356, Y0_std: 67.161, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 100, Loss: 1.002e+01, Y0_mean: -117.170, Y0_std: 66.179, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 110, Loss: 9.879e+00, Y0_mean: -118.660, Y0_std: 67.941, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 120, Loss: 9.227e+00, Y0_mean: -115.620, Y0_std: 66.658, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 130, Loss: 9.665e+00, Y0_mean: -117.979, Y0_std: 67.081, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 140, Loss: 9.636e+00, Y0_mean: -116.671, Y0_std: 66.354, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 150, Loss: 9.435e+00, Y0_mean: -116.288, Y0_std: 67.097, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 160, Loss: 9.578e+00, Y0_mean: -116.659, Y0_std: 66.333, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 170, Loss: 9.681e+00, Y0_mean: -117.845, Y0_std: 68.458, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 180, Loss: 9.468e+00, Y0_mean: -116.216, Y0_std: 67.013, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 190, Loss: 9.730e+00, Y0_mean: -117.124, Y0_std: 66.097, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 200, Loss: 9.689e+00, Y0_mean: -117.080, Y0_std: 66.111, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 210, Loss: 9.008e+00, Y0_mean: -115.575, Y0_std: 66.542, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 220, Loss: 9.238e+00, Y0_mean: -116.599, Y0_std: 66.268, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 230, Loss: 8.949e+00, Y0_mean: -115.552, Y0_std: 66.488, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 240, Loss: 8.818e+00, Y0_mean: -115.524, Y0_std: 66.520, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 250, Loss: 9.189e+00, Y0_mean: -116.569, Y0_std: 66.160, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 260, Loss: 9.679e+00, Y0_mean: -117.171, Y0_std: 67.980, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 270, Loss: 8.771e+00, Y0_mean: -115.493, Y0_std: 66.455, Time: 0.55, Learning Rate: 8.000e-05\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "\n",
      "[InitOutputBias] main:terminal_first b=5, bias_old=-118.646, bias_target=-117.028, bias_new=-117.028\n",
      "[RecursiveBlock] main:terminal_first b=5, idx=[100,120], steps=20, terminal=external, iters=280, lr=8.0e-05\n",
      "It: 0, Loss: 6.062e+03, Y0_mean: -115.887, Y0_std: 66.496, Time: 7.13, Learning Rate: 8.000e-05\n",
      "It: 10, Loss: 2.727e+02, Y0_mean: -115.330, Y0_std: 56.132, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 20, Loss: 2.408e+02, Y0_mean: -114.760, Y0_std: 52.842, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 30, Loss: 6.413e+01, Y0_mean: -114.641, Y0_std: 52.463, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 40, Loss: 1.654e+01, Y0_mean: -116.655, Y0_std: 54.477, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 50, Loss: 1.025e+01, Y0_mean: -114.992, Y0_std: 52.051, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 60, Loss: 9.875e+00, Y0_mean: -114.295, Y0_std: 54.011, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 70, Loss: 1.015e+01, Y0_mean: -116.768, Y0_std: 54.226, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 80, Loss: 9.917e+00, Y0_mean: -116.729, Y0_std: 54.110, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 90, Loss: 8.349e+00, Y0_mean: -113.787, Y0_std: 50.961, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 100, Loss: 8.444e+00, Y0_mean: -114.913, Y0_std: 51.992, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 110, Loss: 8.302e+00, Y0_mean: -114.956, Y0_std: 52.040, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 120, Loss: 8.694e+00, Y0_mean: -116.026, Y0_std: 55.047, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 130, Loss: 9.379e+00, Y0_mean: -116.654, Y0_std: 54.183, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 140, Loss: 8.519e+00, Y0_mean: -114.049, Y0_std: 53.368, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 150, Loss: 8.148e+00, Y0_mean: -114.887, Y0_std: 51.949, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 160, Loss: 7.716e+00, Y0_mean: -115.128, Y0_std: 54.519, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 170, Loss: 8.286e+00, Y0_mean: -116.760, Y0_std: 54.994, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 180, Loss: 8.452e+00, Y0_mean: -115.977, Y0_std: 54.993, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 190, Loss: 9.408e+00, Y0_mean: -116.561, Y0_std: 54.168, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 200, Loss: 9.183e+00, Y0_mean: -116.556, Y0_std: 54.060, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 210, Loss: 8.423e+00, Y0_mean: -115.927, Y0_std: 55.018, Time: 0.54, Learning Rate: 8.000e-05\n",
      "It: 220, Loss: 8.310e+00, Y0_mean: -114.151, Y0_std: 53.885, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 230, Loss: 8.369e+00, Y0_mean: -114.151, Y0_std: 53.912, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 240, Loss: 8.349e+00, Y0_mean: -113.922, Y0_std: 53.133, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 250, Loss: 7.525e+00, Y0_mean: -115.031, Y0_std: 54.427, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 260, Loss: 7.878e+00, Y0_mean: -114.731, Y0_std: 51.769, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 270, Loss: 7.939e+00, Y0_mean: -114.737, Y0_std: 51.788, Time: 0.55, Learning Rate: 8.000e-05\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "\n",
      "[InitOutputBias] main:terminal_first b=4, bias_old=-117.028, bias_target=-114.990, bias_new=-114.990\n",
      "[RecursiveBlock] main:terminal_first b=4, idx=[80,100], steps=20, terminal=external, iters=280, lr=8.0e-05\n",
      "It: 0, Loss: 5.834e+03, Y0_mean: -111.583, Y0_std: 49.909, Time: 7.50, Learning Rate: 8.000e-05\n",
      "It: 10, Loss: 3.341e+02, Y0_mean: -110.105, Y0_std: 40.914, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 20, Loss: 2.418e+02, Y0_mean: -111.887, Y0_std: 37.883, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 30, Loss: 7.091e+01, Y0_mean: -111.750, Y0_std: 40.274, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 40, Loss: 1.884e+01, Y0_mean: -112.410, Y0_std: 39.575, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 50, Loss: 1.003e+01, Y0_mean: -110.884, Y0_std: 39.624, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 60, Loss: 9.581e+00, Y0_mean: -112.293, Y0_std: 39.398, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 70, Loss: 7.621e+00, Y0_mean: -111.955, Y0_std: 39.802, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 80, Loss: 8.048e+00, Y0_mean: -112.034, Y0_std: 39.966, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 90, Loss: 7.731e+00, Y0_mean: -113.069, Y0_std: 39.900, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 100, Loss: 7.411e+00, Y0_mean: -110.969, Y0_std: 39.589, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 110, Loss: 6.819e+00, Y0_mean: -112.046, Y0_std: 38.925, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 120, Loss: 9.470e+00, Y0_mean: -112.119, Y0_std: 39.190, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 130, Loss: 7.724e+00, Y0_mean: -110.936, Y0_std: 39.635, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 140, Loss: 7.226e+00, Y0_mean: -112.020, Y0_std: 38.885, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 150, Loss: 7.625e+00, Y0_mean: -112.026, Y0_std: 40.012, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 160, Loss: 7.833e+00, Y0_mean: -112.416, Y0_std: 39.440, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 170, Loss: 7.190e+00, Y0_mean: -112.033, Y0_std: 39.914, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 180, Loss: 7.387e+00, Y0_mean: -112.998, Y0_std: 39.920, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 190, Loss: 7.341e+00, Y0_mean: -111.979, Y0_std: 39.878, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 200, Loss: 7.299e+00, Y0_mean: -110.885, Y0_std: 39.571, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 210, Loss: 7.592e+00, Y0_mean: -112.377, Y0_std: 39.358, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 220, Loss: 7.181e+00, Y0_mean: -110.849, Y0_std: 39.466, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 230, Loss: 6.654e+00, Y0_mean: -111.897, Y0_std: 38.844, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 240, Loss: 6.819e+00, Y0_mean: -111.914, Y0_std: 39.800, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 250, Loss: 7.063e+00, Y0_mean: -111.937, Y0_std: 39.819, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 260, Loss: 6.516e+00, Y0_mean: -111.869, Y0_std: 38.781, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 270, Loss: 7.203e+00, Y0_mean: -112.761, Y0_std: 39.164, Time: 0.55, Learning Rate: 8.000e-05\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "\n",
      "[InitOutputBias] main:terminal_first b=3, bias_old=-114.990, bias_target=-112.038, bias_new=-112.038\n",
      "[RecursiveBlock] main:terminal_first b=3, idx=[60,80], steps=20, terminal=external, iters=280, lr=8.0e-05\n",
      "It: 0, Loss: 5.167e+03, Y0_mean: -106.996, Y0_std: 36.056, Time: 7.91, Learning Rate: 8.000e-05\n",
      "It: 10, Loss: 3.644e+02, Y0_mean: -108.579, Y0_std: 29.428, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 20, Loss: 2.227e+02, Y0_mean: -107.792, Y0_std: 25.853, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 30, Loss: 5.923e+01, Y0_mean: -107.551, Y0_std: 28.003, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 40, Loss: 1.694e+01, Y0_mean: -108.405, Y0_std: 27.937, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 50, Loss: 9.326e+00, Y0_mean: -107.788, Y0_std: 27.749, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 60, Loss: 7.645e+00, Y0_mean: -108.535, Y0_std: 28.296, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 70, Loss: 6.943e+00, Y0_mean: -108.280, Y0_std: 27.247, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 80, Loss: 6.562e+00, Y0_mean: -108.295, Y0_std: 27.240, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 90, Loss: 6.703e+00, Y0_mean: -108.351, Y0_std: 27.998, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 100, Loss: 6.335e+00, Y0_mean: -108.401, Y0_std: 28.041, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 110, Loss: 7.051e+00, Y0_mean: -108.702, Y0_std: 28.261, Time: 0.57, Learning Rate: 8.000e-05\n",
      "It: 120, Loss: 6.460e+00, Y0_mean: -108.841, Y0_std: 28.652, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 130, Loss: 6.490e+00, Y0_mean: -109.214, Y0_std: 28.119, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 140, Loss: 6.419e+00, Y0_mean: -108.280, Y0_std: 27.245, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 150, Loss: 6.717e+00, Y0_mean: -108.363, Y0_std: 27.922, Time: 0.57, Learning Rate: 8.000e-05\n",
      "It: 160, Loss: 6.681e+00, Y0_mean: -108.661, Y0_std: 28.312, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 170, Loss: 6.256e+00, Y0_mean: -108.352, Y0_std: 27.948, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 180, Loss: 6.156e+00, Y0_mean: -108.352, Y0_std: 27.970, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 190, Loss: 6.621e+00, Y0_mean: -108.658, Y0_std: 28.283, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 200, Loss: 6.445e+00, Y0_mean: -108.641, Y0_std: 28.169, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 210, Loss: 6.009e+00, Y0_mean: -108.813, Y0_std: 28.497, Time: 0.57, Learning Rate: 8.000e-05\n",
      "It: 220, Loss: 6.286e+00, Y0_mean: -108.638, Y0_std: 28.210, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 230, Loss: 5.999e+00, Y0_mean: -108.762, Y0_std: 28.087, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 240, Loss: 6.303e+00, Y0_mean: -108.254, Y0_std: 27.155, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 250, Loss: 6.153e+00, Y0_mean: -108.337, Y0_std: 27.854, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 260, Loss: 6.082e+00, Y0_mean: -108.222, Y0_std: 27.125, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 270, Loss: 6.084e+00, Y0_mean: -108.240, Y0_std: 27.081, Time: 0.56, Learning Rate: 8.000e-05\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "\n",
      "[InitOutputBias] main:terminal_first b=2, bias_old=-112.039, bias_target=-108.534, bias_new=-108.534\n",
      "[RecursiveBlock] main:terminal_first b=2, idx=[40,60], steps=20, terminal=external, iters=280, lr=8.0e-05\n",
      "It: 0, Loss: 4.674e+03, Y0_mean: -102.940, Y0_std: 24.521, Time: 7.55, Learning Rate: 8.000e-05\n",
      "It: 10, Loss: 4.341e+02, Y0_mean: -103.131, Y0_std: 19.594, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 20, Loss: 2.343e+02, Y0_mean: -103.932, Y0_std: 17.678, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 30, Loss: 5.156e+01, Y0_mean: -103.214, Y0_std: 17.370, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 40, Loss: 2.138e+01, Y0_mean: -104.473, Y0_std: 18.507, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 50, Loss: 1.209e+01, Y0_mean: -104.529, Y0_std: 18.830, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 60, Loss: 7.322e+00, Y0_mean: -103.444, Y0_std: 17.386, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 70, Loss: 6.859e+00, Y0_mean: -104.448, Y0_std: 18.725, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 80, Loss: 6.532e+00, Y0_mean: -103.989, Y0_std: 18.845, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 90, Loss: 6.001e+00, Y0_mean: -104.421, Y0_std: 18.434, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 100, Loss: 6.004e+00, Y0_mean: -104.754, Y0_std: 18.423, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 110, Loss: 5.687e+00, Y0_mean: -104.416, Y0_std: 18.416, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 120, Loss: 5.800e+00, Y0_mean: -104.579, Y0_std: 18.691, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 130, Loss: 5.433e+00, Y0_mean: -104.631, Y0_std: 18.842, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 140, Loss: 5.251e+00, Y0_mean: -103.672, Y0_std: 17.323, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 150, Loss: 5.560e+00, Y0_mean: -104.446, Y0_std: 18.357, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 160, Loss: 5.320e+00, Y0_mean: -104.101, Y0_std: 18.320, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 170, Loss: 5.413e+00, Y0_mean: -104.647, Y0_std: 18.763, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 180, Loss: 5.764e+00, Y0_mean: -104.845, Y0_std: 18.802, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 190, Loss: 5.483e+00, Y0_mean: -104.597, Y0_std: 18.559, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 200, Loss: 5.538e+00, Y0_mean: -104.585, Y0_std: 18.521, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 210, Loss: 5.172e+00, Y0_mean: -104.127, Y0_std: 18.192, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 220, Loss: 5.508e+00, Y0_mean: -104.811, Y0_std: 18.220, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 230, Loss: 5.274e+00, Y0_mean: -104.652, Y0_std: 18.633, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 240, Loss: 5.067e+00, Y0_mean: -103.709, Y0_std: 17.180, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 250, Loss: 5.712e+00, Y0_mean: -104.845, Y0_std: 18.625, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 260, Loss: 5.594e+00, Y0_mean: -104.834, Y0_std: 18.660, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 270, Loss: 5.365e+00, Y0_mean: -104.804, Y0_std: 18.102, Time: 0.56, Learning Rate: 8.000e-05\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "\n",
      "[InitOutputBias] main:terminal_first b=1, bias_old=-108.535, bias_target=-104.425, bias_new=-104.425\n",
      "[RecursiveBlock] main:terminal_first b=1, idx=[20,40], steps=20, terminal=external, iters=280, lr=8.0e-05\n",
      "It: 0, Loss: 4.563e+03, Y0_mean: -96.510, Y0_std: 14.113, Time: 7.53, Learning Rate: 8.000e-05\n",
      "It: 10, Loss: 5.915e+02, Y0_mean: -96.933, Y0_std: 11.795, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 20, Loss: 2.614e+02, Y0_mean: -96.945, Y0_std: 9.976, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 30, Loss: 7.215e+01, Y0_mean: -97.951, Y0_std: 10.141, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 40, Loss: 2.292e+01, Y0_mean: -98.270, Y0_std: 10.330, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 50, Loss: 1.397e+01, Y0_mean: -98.419, Y0_std: 10.213, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 60, Loss: 9.042e+00, Y0_mean: -98.392, Y0_std: 10.901, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 70, Loss: 6.726e+00, Y0_mean: -97.956, Y0_std: 10.378, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 80, Loss: 5.988e+00, Y0_mean: -98.678, Y0_std: 11.019, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 90, Loss: 5.490e+00, Y0_mean: -98.489, Y0_std: 10.139, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 100, Loss: 4.956e+00, Y0_mean: -98.211, Y0_std: 10.275, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 110, Loss: 5.133e+00, Y0_mean: -98.800, Y0_std: 10.980, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 120, Loss: 5.139e+00, Y0_mean: -98.627, Y0_std: 10.456, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 130, Loss: 5.190e+00, Y0_mean: -98.935, Y0_std: 10.794, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 140, Loss: 4.705e+00, Y0_mean: -98.717, Y0_std: 10.066, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 150, Loss: 4.932e+00, Y0_mean: -99.007, Y0_std: 10.934, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 160, Loss: 4.953e+00, Y0_mean: -99.066, Y0_std: 10.916, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 170, Loss: 4.590e+00, Y0_mean: -98.876, Y0_std: 10.016, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 180, Loss: 4.807e+00, Y0_mean: -99.164, Y0_std: 10.897, Time: 0.57, Learning Rate: 8.000e-05\n",
      "It: 190, Loss: 4.922e+00, Y0_mean: -98.993, Y0_std: 10.356, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 200, Loss: 4.391e+00, Y0_mean: -98.721, Y0_std: 10.124, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 210, Loss: 4.307e+00, Y0_mean: -98.776, Y0_std: 10.109, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 220, Loss: 4.292e+00, Y0_mean: -98.841, Y0_std: 10.076, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 230, Loss: 4.605e+00, Y0_mean: -99.314, Y0_std: 10.713, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 240, Loss: 4.699e+00, Y0_mean: -99.262, Y0_std: 10.301, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 250, Loss: 4.606e+00, Y0_mean: -99.568, Y0_std: 10.611, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 260, Loss: 4.248e+00, Y0_mean: -99.059, Y0_std: 10.013, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 270, Loss: 4.423e+00, Y0_mean: -99.656, Y0_std: 10.704, Time: 0.55, Learning Rate: 8.000e-05\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "\n",
      "[InitOutputBias] main:terminal_first b=0, bias_old=-104.427, bias_target=-99.550, bias_new=-99.550\n",
      "[RecursiveBlock] main:terminal_first b=0, idx=[0,20], steps=20, terminal=external, iters=280, lr=8.0e-05\n",
      "It: 0, Loss: 5.178e+03, Y0_mean: -84.096, Y0_std: 0.000, Time: 7.65, Learning Rate: 8.000e-05\n",
      "It: 10, Loss: 1.363e+03, Y0_mean: -88.079, Y0_std: 0.000, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 20, Loss: 7.055e+02, Y0_mean: -89.786, Y0_std: 0.000, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 30, Loss: 2.717e+02, Y0_mean: -89.103, Y0_std: 0.000, Time: 0.55, Learning Rate: 8.000e-05\n",
      "It: 40, Loss: 8.781e+01, Y0_mean: -90.202, Y0_std: 0.000, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 50, Loss: 4.010e+01, Y0_mean: -90.807, Y0_std: 0.000, Time: 0.57, Learning Rate: 8.000e-05\n",
      "It: 60, Loss: 2.607e+01, Y0_mean: -90.754, Y0_std: 0.000, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 70, Loss: 1.669e+01, Y0_mean: -90.684, Y0_std: 0.000, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 80, Loss: 1.129e+01, Y0_mean: -91.089, Y0_std: 0.000, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 90, Loss: 1.099e+01, Y0_mean: -90.981, Y0_std: 0.000, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 100, Loss: 7.880e+00, Y0_mean: -90.891, Y0_std: 0.000, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 110, Loss: 7.065e+00, Y0_mean: -90.854, Y0_std: 0.000, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 120, Loss: 6.413e+00, Y0_mean: -90.801, Y0_std: 0.000, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 130, Loss: 6.156e+00, Y0_mean: -90.912, Y0_std: 0.000, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 140, Loss: 6.085e+00, Y0_mean: -90.880, Y0_std: 0.000, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 150, Loss: 5.966e+00, Y0_mean: -90.989, Y0_std: 0.000, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 160, Loss: 5.259e+00, Y0_mean: -91.036, Y0_std: 0.000, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 170, Loss: 4.966e+00, Y0_mean: -91.093, Y0_std: 0.000, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 180, Loss: 5.056e+00, Y0_mean: -91.171, Y0_std: 0.000, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 190, Loss: 5.193e+00, Y0_mean: -91.250, Y0_std: 0.000, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 200, Loss: 4.743e+00, Y0_mean: -91.375, Y0_std: 0.000, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 210, Loss: 4.645e+00, Y0_mean: -91.481, Y0_std: 0.000, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 220, Loss: 4.705e+00, Y0_mean: -91.484, Y0_std: 0.000, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 230, Loss: 4.857e+00, Y0_mean: -91.646, Y0_std: 0.000, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 240, Loss: 4.853e+00, Y0_mean: -91.704, Y0_std: 0.000, Time: 0.57, Learning Rate: 8.000e-05\n",
      "It: 250, Loss: 4.800e+00, Y0_mean: -91.780, Y0_std: 0.000, Time: 0.57, Learning Rate: 8.000e-05\n",
      "It: 260, Loss: 4.544e+00, Y0_mean: -91.904, Y0_std: 0.000, Time: 0.56, Learning Rate: 8.000e-05\n",
      "It: 270, Loss: 4.147e+00, Y0_mean: -91.971, Y0_std: 0.000, Time: 0.56, Learning Rate: 8.000e-05\n",
      "=== Recursive Log (compact) ===\n",
      "block=0, idx=[0,20], steps=20, terminal=external, eval_loss=4.167e+00, eval_y0=-92.020, exact_y0=-8.000, delta=-84.020, rel_delta=10.502, refine=0\n",
      "block=1, idx=[20,40], steps=20, terminal=external, eval_loss=4.256e+00, eval_y0=-99.437, exact_y0=-9.119, delta=-90.318, rel_delta=9.904, refine=0\n",
      "block=2, idx=[40,60], steps=20, terminal=external, eval_loss=5.062e+00, eval_y0=-104.105, exact_y0=-11.428, delta=-92.677, rel_delta=8.110, refine=0\n",
      "block=3, idx=[60,80], steps=20, terminal=external, eval_loss=6.018e+00, eval_y0=-108.523, exact_y0=-13.960, delta=-94.563, rel_delta=6.774, refine=0\n",
      "block=4, idx=[80,100], steps=20, terminal=external, eval_loss=7.050e+00, eval_y0=-112.528, exact_y0=-16.730, delta=-95.798, rel_delta=5.726, refine=0\n",
      "block=5, idx=[100,120], steps=20, terminal=external, eval_loss=8.008e+00, eval_y0=-115.242, exact_y0=-19.372, delta=-95.871, rel_delta=4.949, refine=0\n",
      "block=6, idx=[120,140], steps=20, terminal=external, eval_loss=9.045e+00, eval_y0=-117.300, exact_y0=-22.057, delta=-95.244, rel_delta=4.318, refine=0\n",
      "block=7, idx=[140,160], steps=20, terminal=external, eval_loss=1.056e+01, eval_y0=-119.259, exact_y0=-24.825, delta=-94.434, rel_delta=3.804, refine=0\n",
      "block=8, idx=[160,180], steps=20, terminal=external, eval_loss=1.278e+01, eval_y0=-119.033, exact_y0=-26.732, delta=-92.300, rel_delta=3.453, refine=0\n",
      "block=9, idx=[180,200], steps=20, terminal=external, eval_loss=1.425e+01, eval_y0=-118.612, exact_y0=-29.152, delta=-89.459, rel_delta=3.069, refine=0\n",
      "block=10, idx=[200,220], steps=20, terminal=external, eval_loss=1.608e+01, eval_y0=-119.424, exact_y0=-32.154, delta=-87.270, rel_delta=2.714, refine=0\n",
      "block=11, idx=[220,240], steps=20, terminal=external, eval_loss=1.817e+01, eval_y0=-118.120, exact_y0=-34.275, delta=-83.844, rel_delta=2.446, refine=0\n",
      "block=12, idx=[240,260], steps=20, terminal=external, eval_loss=1.997e+01, eval_y0=-118.615, exact_y0=-37.490, delta=-81.125, rel_delta=2.164, refine=0\n",
      "block=13, idx=[260,280], steps=20, terminal=external, eval_loss=2.219e+01, eval_y0=-118.471, exact_y0=-40.000, delta=-78.471, rel_delta=1.962, refine=0\n",
      "block=14, idx=[280,300], steps=20, terminal=external, eval_loss=2.575e+01, eval_y0=-118.547, exact_y0=-42.225, delta=-76.322, rel_delta=1.807, refine=0\n",
      "block=15, idx=[300,320], steps=20, terminal=external, eval_loss=2.797e+01, eval_y0=-118.606, exact_y0=-44.600, delta=-74.006, rel_delta=1.659, refine=0\n",
      "block=16, idx=[320,340], steps=20, terminal=external, eval_loss=2.951e+01, eval_y0=-120.469, exact_y0=-49.112, delta=-71.358, rel_delta=1.453, refine=0\n",
      "block=17, idx=[340,360], steps=20, terminal=external, eval_loss=3.405e+01, eval_y0=-116.145, exact_y0=-49.173, delta=-66.972, rel_delta=1.362, refine=0\n",
      "block=18, idx=[360,380], steps=20, terminal=external, eval_loss=3.862e+01, eval_y0=-116.668, exact_y0=-54.112, delta=-62.556, rel_delta=1.156, refine=0\n",
      "block=19, idx=[380,400], steps=20, terminal=external, eval_loss=4.731e+01, eval_y0=-114.470, exact_y0=-57.525, delta=-56.945, rel_delta=0.990, refine=0\n",
      "block=20, idx=[400,420], steps=20, terminal=external, eval_loss=6.708e+01, eval_y0=-106.401, exact_y0=-58.562, delta=-47.839, rel_delta=0.817, refine=0\n",
      "block=21, idx=[420,440], steps=20, terminal=external, eval_loss=1.055e+02, eval_y0=-101.333, exact_y0=-60.562, delta=-40.771, rel_delta=0.673, refine=0\n",
      "block=22, idx=[440,452], steps=12, terminal=external, eval_loss=3.507e+02, eval_y0=-95.255, exact_y0=-64.888, delta=-30.367, rel_delta=0.468, refine=0\n",
      "block=23, idx=[452,480], steps=28, terminal=g, eval_loss=1.246e+03, eval_y0=-79.139, exact_y0=-62.437, delta=-16.702, rel_delta=0.268, refine=4\n",
      "\n",
      "Predicted Y0 mean: -92.0199 (std 0.0000)\n",
      "Predicted Y0 path0: -92.0199\n",
      "Exact Y0:           -8.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "class FBSNN(ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        Xi,\n",
    "        T,\n",
    "        M,\n",
    "        N,\n",
    "        D,\n",
    "        layers,\n",
    "        t_start=0.0,\n",
    "        activation=\"sin\",\n",
    "        clip_grad_norm=1.0,\n",
    "        use_antithetic_sampling=True,\n",
    "        path_loss_weight=1.0,\n",
    "        terminal_y_loss_weight=10.0,\n",
    "        terminal_z_loss_weight=1.0,\n",
    "        x_norm_mean=None,\n",
    "        x_norm_std=None,\n",
    "        normalize_time=True,\n",
    "    ):\n",
    "        self.T = float(T)\n",
    "        self.t_start = np.float32(t_start)\n",
    "        self.M = int(M)\n",
    "        self.N = int(N)\n",
    "        self.D = int(D)\n",
    "        self.layers = layers\n",
    "\n",
    "        self.activation = activation\n",
    "        self.clip_grad_norm = clip_grad_norm\n",
    "        self.use_antithetic_sampling = bool(use_antithetic_sampling)\n",
    "        self.path_loss_weight = float(path_loss_weight)\n",
    "        self.terminal_y_loss_weight = float(terminal_y_loss_weight)\n",
    "        self.terminal_z_loss_weight = float(terminal_z_loss_weight)\n",
    "        self.normalize_time = bool(normalize_time)\n",
    "\n",
    "        if x_norm_mean is None:\n",
    "            x_norm_mean = np.zeros((1, self.D), dtype=np.float32)\n",
    "        if x_norm_std is None:\n",
    "            x_norm_std = np.ones((1, self.D), dtype=np.float32)\n",
    "\n",
    "        x_norm_mean = np.asarray(x_norm_mean, dtype=np.float32).reshape(1, self.D)\n",
    "        x_norm_std = np.asarray(x_norm_std, dtype=np.float32).reshape(1, self.D)\n",
    "        x_norm_std = np.maximum(x_norm_std, 1.0e-3).astype(np.float32)\n",
    "\n",
    "        self.x_norm_mean_np = x_norm_mean\n",
    "        self.x_norm_std_np = x_norm_std\n",
    "\n",
    "        self.Xi = self._prepare_Xi(Xi)\n",
    "\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "        self.sess = tf.Session(\n",
    "            config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "        )\n",
    "\n",
    "        self.learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "        self.t_tf = tf.placeholder(tf.float32, shape=[self.M, self.N + 1, 1])\n",
    "        self.W_tf = tf.placeholder(tf.float32, shape=[self.M, self.N + 1, self.D])\n",
    "        self.Xi_tf = tf.placeholder(tf.float32, shape=[self.M, self.D])\n",
    "        self.const_tf = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "        self.use_external_terminal_tf = tf.placeholder_with_default(np.float32(0.0), shape=[])\n",
    "        self.Y_terminal_tf = tf.placeholder_with_default(\n",
    "            tf.zeros([self.M, 1], dtype=tf.float32), shape=[self.M, 1]\n",
    "        )\n",
    "        self.Z_terminal_tf = tf.placeholder_with_default(\n",
    "            tf.zeros([self.M, self.D], dtype=tf.float32), shape=[self.M, self.D]\n",
    "        )\n",
    "        self.path_loss_weight_tf = tf.placeholder_with_default(\n",
    "            np.float32(self.path_loss_weight), shape=[]\n",
    "        )\n",
    "        self.terminal_y_loss_weight_tf = tf.placeholder_with_default(\n",
    "            np.float32(self.terminal_y_loss_weight), shape=[]\n",
    "        )\n",
    "        self.terminal_z_loss_weight_tf = tf.placeholder_with_default(\n",
    "            np.float32(self.terminal_z_loss_weight), shape=[]\n",
    "        )\n",
    "\n",
    "        self.x_norm_mean_tf = tf.constant(self.x_norm_mean_np, dtype=tf.float32)\n",
    "        self.x_norm_std_tf = tf.constant(self.x_norm_std_np, dtype=tf.float32)\n",
    "        self.t_start_tf = tf.constant(np.float32(self.t_start), dtype=tf.float32)\n",
    "        self.t_scale_tf = tf.constant(np.float32(max(self.T, 1.0e-6)), dtype=tf.float32)\n",
    "\n",
    "        self.loss, self.X_pred, self.Y_pred, self.Y0_pred, self.Z_pred = self.loss_function(\n",
    "            self.t_tf, self.W_tf, self.Xi_tf\n",
    "        )\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
    "        if self.clip_grad_norm is not None:\n",
    "            non_null = [(g, v) for g, v in grads_and_vars if g is not None]\n",
    "            if non_null:\n",
    "                grads, vars_ = zip(*non_null)\n",
    "                clipped_grads, _ = tf.clip_by_global_norm(grads, self.clip_grad_norm)\n",
    "                clipped = list(zip(clipped_grads, vars_))\n",
    "                untouched = [(g, v) for g, v in grads_and_vars if g is None]\n",
    "                grads_and_vars = clipped + untouched\n",
    "        self.train_op = self.optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(num_layers - 1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = np.sqrt(2.0 / (in_dim + out_dim))\n",
    "        return tf.Variable(\n",
    "            tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def _prepare_Xi(self, Xi_in):\n",
    "        Xi_arr = np.asarray(Xi_in, dtype=np.float32)\n",
    "        if Xi_arr.ndim == 1:\n",
    "            Xi_arr = Xi_arr.reshape(1, -1)\n",
    "\n",
    "        if Xi_arr.shape == (1, self.D):\n",
    "            Xi_arr = np.repeat(Xi_arr, self.M, axis=0)\n",
    "        elif Xi_arr.shape != (self.M, self.D):\n",
    "            raise ValueError(\n",
    "                f\"Xi must have shape (1,{self.D}) or ({self.M},{self.D}), got {Xi_arr.shape}\"\n",
    "            )\n",
    "        return Xi_arr\n",
    "\n",
    "    def _prepare_terminal_targets(self, Y_terminal, Z_terminal):\n",
    "        Y_arr = np.asarray(Y_terminal, dtype=np.float32)\n",
    "        Z_arr = np.asarray(Z_terminal, dtype=np.float32)\n",
    "\n",
    "        if Y_arr.ndim == 1:\n",
    "            Y_arr = Y_arr.reshape(-1, 1)\n",
    "        if Z_arr.ndim == 1:\n",
    "            Z_arr = Z_arr.reshape(1, -1)\n",
    "\n",
    "        if Y_arr.shape != (self.M, 1):\n",
    "            raise ValueError(f\"Y_terminal must have shape ({self.M},1), got {Y_arr.shape}\")\n",
    "        if Z_arr.shape != (self.M, self.D):\n",
    "            raise ValueError(f\"Z_terminal must have shape ({self.M},{self.D}), got {Z_arr.shape}\")\n",
    "\n",
    "        return Y_arr, Z_arr\n",
    "\n",
    "    def neural_net(self, X, weights, biases):\n",
    "        num_layers = len(weights) + 1\n",
    "\n",
    "        H = X\n",
    "        for l in range(num_layers - 2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            pre = tf.add(tf.matmul(H, W), b)\n",
    "            if self.activation == \"sin\":\n",
    "                H = tf.sin(pre)\n",
    "            elif self.activation == \"tanh\":\n",
    "                H = tf.tanh(pre)\n",
    "            elif self.activation == \"swish\":\n",
    "                H = pre * tf.sigmoid(pre)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Unsupported activation '{self.activation}'. Use: sin, tanh, swish\"\n",
    "                )\n",
    "\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        return Y\n",
    "\n",
    "    def net_u(self, t, X):\n",
    "        if self.normalize_time:\n",
    "            t_in = 2.0 * ((t - self.t_start_tf) / self.t_scale_tf) - 1.0\n",
    "        else:\n",
    "            t_in = t\n",
    "        X_in = (X - self.x_norm_mean_tf) / self.x_norm_std_tf\n",
    "        u = self.neural_net(tf.concat([t_in, X_in], 1), self.weights, self.biases)\n",
    "        Du = tf.gradients(u, X)[0]\n",
    "        return u, Du\n",
    "\n",
    "    def Dg_tf(self, X):\n",
    "        return tf.gradients(self.g_tf(X), X)[0]\n",
    "\n",
    "    def loss_function(self, t, W, Xi):\n",
    "        path_loss = 0.0\n",
    "        X_list = []\n",
    "        Y_list = []\n",
    "        Z_list = []\n",
    "\n",
    "        t0 = t[:, 0, :]\n",
    "        W0 = W[:, 0, :]\n",
    "        X0 = Xi\n",
    "        Y0, Du0 = self.net_u(t0, X0)\n",
    "        sigma0 = self.sigma_tf(t0, X0, Y0)\n",
    "        Z0 = tf.squeeze(tf.matmul(tf.expand_dims(Du0, 1), sigma0), axis=1)\n",
    "\n",
    "        X_list.append(X0)\n",
    "        Y_list.append(Y0)\n",
    "        Z_list.append(Z0)\n",
    "\n",
    "        for n in range(self.N):\n",
    "            t1 = t[:, n + 1, :]\n",
    "            W1 = W[:, n + 1, :]\n",
    "\n",
    "            dW = W1 - W0\n",
    "            sigma_dW = tf.squeeze(tf.matmul(sigma0, tf.expand_dims(dW, -1)), axis=[-1])\n",
    "            X1 = X0 + self.mu_tf(t0, X0, Y0, Z0) * (t1 - t0) + sigma_dW\n",
    "\n",
    "            Y1_tilde = Y0 + self.phi_tf(t0, X0, Y0, Z0) * (t1 - t0) + tf.reduce_sum(\n",
    "                Z0 * dW, axis=1, keepdims=True\n",
    "            )\n",
    "\n",
    "            Y1, Du1 = self.net_u(t1, X1)\n",
    "            sigma1 = self.sigma_tf(t1, X1, Y1)\n",
    "            Z1 = tf.squeeze(tf.matmul(tf.expand_dims(Du1, 1), sigma1), axis=1)\n",
    "\n",
    "            path_loss += tf.reduce_mean(tf.square(Y1 - Y1_tilde))\n",
    "\n",
    "            t0 = t1\n",
    "            W0 = W1\n",
    "            X0 = X1\n",
    "            Y0 = Y1\n",
    "            Z0 = Z1\n",
    "            sigma0 = sigma1\n",
    "\n",
    "            X_list.append(X0)\n",
    "            Y_list.append(Y0)\n",
    "            Z_list.append(Z0)\n",
    "\n",
    "        path_loss = path_loss / float(max(1, self.N))\n",
    "\n",
    "        Y_terminal_default = self.g_tf(X1)\n",
    "        Dg = self.Dg_tf(X1)\n",
    "        Z_terminal_default = tf.squeeze(tf.matmul(tf.expand_dims(Dg, 1), sigma1), axis=1)\n",
    "\n",
    "        alpha = tf.cast(self.use_external_terminal_tf, tf.float32)\n",
    "        Y_terminal_target = (1.0 - alpha) * Y_terminal_default + alpha * self.Y_terminal_tf\n",
    "        Z_terminal_target = (1.0 - alpha) * Z_terminal_default + alpha * self.Z_terminal_tf\n",
    "\n",
    "        terminal_y_loss = tf.reduce_mean(tf.square(Y1 - Y_terminal_target))\n",
    "        terminal_z_loss = tf.reduce_mean(tf.square(Z1 - Z_terminal_target))\n",
    "\n",
    "        loss = (\n",
    "            self.path_loss_weight_tf * path_loss\n",
    "            + self.terminal_y_loss_weight_tf * terminal_y_loss\n",
    "            + self.terminal_z_loss_weight_tf * terminal_z_loss\n",
    "        )\n",
    "\n",
    "        X = tf.stack(X_list, axis=1)\n",
    "        Y = tf.stack(Y_list, axis=1)\n",
    "        Z = tf.stack(Z_list, axis=1)\n",
    "        return loss, X, Y, Y[:, 0, :], Z\n",
    "\n",
    "    def fetch_minibatch(self):\n",
    "        Dt = np.zeros((self.M, self.N + 1, 1), dtype=np.float32)\n",
    "        DW = np.zeros((self.M, self.N + 1, self.D), dtype=np.float32)\n",
    "\n",
    "        dt = float(self.T / self.N)\n",
    "        Dt[:, 1:, :] = dt\n",
    "\n",
    "        if self.use_antithetic_sampling and self.M > 1:\n",
    "            half_M = self.M // 2\n",
    "            DW_half = np.sqrt(dt) * np.random.normal(size=(half_M, self.N, self.D)).astype(np.float32)\n",
    "            DW[:half_M, 1:, :] = DW_half\n",
    "            DW[half_M : 2 * half_M, 1:, :] = -DW_half\n",
    "            if self.M % 2 == 1:\n",
    "                DW[-1, 1:, :] = (\n",
    "                    np.sqrt(dt) * np.random.normal(size=(self.N, self.D)).astype(np.float32)\n",
    "                )\n",
    "        else:\n",
    "            DW[:, 1:, :] = (\n",
    "                np.sqrt(dt) * np.random.normal(size=(self.M, self.N, self.D)).astype(np.float32)\n",
    "            )\n",
    "\n",
    "        t = self.t_start + np.cumsum(Dt, axis=1)\n",
    "        W = np.cumsum(DW, axis=1)\n",
    "        return t.astype(np.float32), W.astype(np.float32)\n",
    "\n",
    "    def _check_batch_entry(self, entry):\n",
    "        Xi_entry = self._prepare_Xi(entry[\"Xi\"])\n",
    "        t_entry = np.asarray(entry[\"t\"], dtype=np.float32)\n",
    "        W_entry = np.asarray(entry[\"W\"], dtype=np.float32)\n",
    "        if t_entry.shape != (self.M, self.N + 1, 1):\n",
    "            raise ValueError(\n",
    "                f\"t_batch must have shape ({self.M},{self.N+1},1), got {t_entry.shape}\"\n",
    "            )\n",
    "        if W_entry.shape != (self.M, self.N + 1, self.D):\n",
    "            raise ValueError(\n",
    "                f\"W_batch must have shape ({self.M},{self.N+1},{self.D}), got {W_entry.shape}\"\n",
    "            )\n",
    "        term = entry.get(\"terminal_targets\", None)\n",
    "        return Xi_entry, t_entry, W_entry, term\n",
    "\n",
    "    def _build_feed_dict(\n",
    "        self,\n",
    "        Xi_feed,\n",
    "        t_batch,\n",
    "        W_batch,\n",
    "        learning_rate,\n",
    "        current_const,\n",
    "        terminal_targets,\n",
    "        loss_weights=None,\n",
    "    ):\n",
    "        use_external_terminal = np.float32(0.0)\n",
    "        Y_terminal_feed = np.zeros((self.M, 1), dtype=np.float32)\n",
    "        Z_terminal_feed = np.zeros((self.M, self.D), dtype=np.float32)\n",
    "\n",
    "        if terminal_targets is not None:\n",
    "            use_external_terminal = np.float32(1.0)\n",
    "            Y_terminal_feed, Z_terminal_feed = self._prepare_terminal_targets(\n",
    "                terminal_targets[0], terminal_targets[1]\n",
    "            )\n",
    "\n",
    "        tf_dict = {\n",
    "            self.Xi_tf: Xi_feed,\n",
    "            self.t_tf: t_batch,\n",
    "            self.W_tf: W_batch,\n",
    "            self.learning_rate: learning_rate,\n",
    "            self.const_tf: current_const,\n",
    "            self.use_external_terminal_tf: use_external_terminal,\n",
    "            self.Y_terminal_tf: Y_terminal_feed,\n",
    "            self.Z_terminal_tf: Z_terminal_feed,\n",
    "        }\n",
    "        if loss_weights is not None:\n",
    "            tf_dict[self.path_loss_weight_tf] = np.float32(\n",
    "                loss_weights.get(\"path\", self.path_loss_weight)\n",
    "            )\n",
    "            tf_dict[self.terminal_y_loss_weight_tf] = np.float32(\n",
    "                loss_weights.get(\"terminal_y\", self.terminal_y_loss_weight)\n",
    "            )\n",
    "            tf_dict[self.terminal_z_loss_weight_tf] = np.float32(\n",
    "                loss_weights.get(\"terminal_z\", self.terminal_z_loss_weight)\n",
    "            )\n",
    "        return tf_dict\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        N_Iter,\n",
    "        learning_rate,\n",
    "        const_value=None,\n",
    "        Xi_batch=None,\n",
    "        fixed_batch=None,\n",
    "        terminal_targets=None,\n",
    "        batch_pool=None,\n",
    "        loss_weights=None,\n",
    "    ):\n",
    "        Xi_feed_default = self.Xi if Xi_batch is None else self._prepare_Xi(Xi_batch)\n",
    "\n",
    "        fixed_t = None\n",
    "        fixed_W = None\n",
    "        if fixed_batch is not None:\n",
    "            fixed_t = np.asarray(fixed_batch[0], dtype=np.float32)\n",
    "            fixed_W = np.asarray(fixed_batch[1], dtype=np.float32)\n",
    "            if fixed_t.shape != (self.M, self.N + 1, 1):\n",
    "                raise ValueError(\n",
    "                    f\"t_batch must have shape ({self.M},{self.N+1},1), got {fixed_t.shape}\"\n",
    "                )\n",
    "            if fixed_W.shape != (self.M, self.N + 1, self.D):\n",
    "                raise ValueError(\n",
    "                    f\"W_batch must have shape ({self.M},{self.N+1},{self.D}), got {fixed_W.shape}\"\n",
    "                )\n",
    "\n",
    "        checked_pool = None\n",
    "        if batch_pool is not None:\n",
    "            if len(batch_pool) == 0:\n",
    "                raise ValueError(\"batch_pool cannot be empty\")\n",
    "            checked_pool = [self._check_batch_entry(entry) for entry in batch_pool]\n",
    "\n",
    "        start_time = time.time()\n",
    "        last_loss = None\n",
    "        last_y0 = None\n",
    "        current_const = np.float32(self.const if const_value is None else const_value)\n",
    "\n",
    "        for it in range(int(N_Iter)):\n",
    "            if checked_pool is not None:\n",
    "                idx = np.random.randint(len(checked_pool))\n",
    "                Xi_feed, t_batch, W_batch, term = checked_pool[idx]\n",
    "                term_use = term\n",
    "            else:\n",
    "                Xi_feed = Xi_feed_default\n",
    "                if fixed_t is None:\n",
    "                    t_batch, W_batch = self.fetch_minibatch()\n",
    "                else:\n",
    "                    t_batch, W_batch = fixed_t, fixed_W\n",
    "                term_use = terminal_targets\n",
    "\n",
    "            tf_dict = self._build_feed_dict(\n",
    "                Xi_feed=Xi_feed,\n",
    "                t_batch=t_batch,\n",
    "                W_batch=W_batch,\n",
    "                learning_rate=learning_rate,\n",
    "                current_const=current_const,\n",
    "                terminal_targets=term_use,\n",
    "                loss_weights=loss_weights,\n",
    "            )\n",
    "\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "\n",
    "            if it % 10 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                loss_value, y0_value = self.sess.run([self.loss, self.Y0_pred], tf_dict)\n",
    "                y0_mean = float(np.mean(y0_value))\n",
    "                y0_std = float(np.std(y0_value))\n",
    "                last_loss = float(loss_value)\n",
    "                last_y0 = y0_mean\n",
    "                print(\n",
    "                    \"It: %d, Loss: %.3e, Y0_mean: %.3f, Y0_std: %.3f, Time: %.2f, Learning Rate: %.3e\"\n",
    "                    % (it, loss_value, y0_mean, y0_std, elapsed, learning_rate)\n",
    "                )\n",
    "                start_time = time.time()\n",
    "\n",
    "        return {\n",
    "            \"const\": float(current_const),\n",
    "            \"learning_rate\": float(learning_rate),\n",
    "            \"n_iter\": int(N_Iter),\n",
    "            \"last_loss\": last_loss,\n",
    "            \"last_y0\": last_y0,\n",
    "            \"used_batch_pool\": checked_pool is not None,\n",
    "        }\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        const_value=None,\n",
    "        n_batches=5,\n",
    "        Xi_batch=None,\n",
    "        fixed_batch=None,\n",
    "        terminal_targets=None,\n",
    "        batch_pool=None,\n",
    "        loss_weights=None,\n",
    "    ):\n",
    "        current_const = np.float32(self.const if const_value is None else const_value)\n",
    "        Xi_feed_default = self.Xi if Xi_batch is None else self._prepare_Xi(Xi_batch)\n",
    "\n",
    "        fixed_t = None\n",
    "        fixed_W = None\n",
    "        if fixed_batch is not None:\n",
    "            fixed_t = np.asarray(fixed_batch[0], dtype=np.float32)\n",
    "            fixed_W = np.asarray(fixed_batch[1], dtype=np.float32)\n",
    "\n",
    "        checked_pool = None\n",
    "        if batch_pool is not None:\n",
    "            if len(batch_pool) == 0:\n",
    "                raise ValueError(\"batch_pool cannot be empty\")\n",
    "            checked_pool = [self._check_batch_entry(entry) for entry in batch_pool]\n",
    "\n",
    "        losses = []\n",
    "        y0s = []\n",
    "\n",
    "        if checked_pool is not None:\n",
    "            iterable = checked_pool\n",
    "        else:\n",
    "            iterable = range(int(n_batches))\n",
    "\n",
    "        for item in iterable:\n",
    "            if checked_pool is not None:\n",
    "                Xi_feed, t_batch, W_batch, term = item\n",
    "                term_use = term\n",
    "            else:\n",
    "                Xi_feed = Xi_feed_default\n",
    "                if fixed_t is None:\n",
    "                    t_batch, W_batch = self.fetch_minibatch()\n",
    "                else:\n",
    "                    t_batch, W_batch = fixed_t, fixed_W\n",
    "                term_use = terminal_targets\n",
    "\n",
    "            tf_dict = self._build_feed_dict(\n",
    "                Xi_feed=Xi_feed,\n",
    "                t_batch=t_batch,\n",
    "                W_batch=W_batch,\n",
    "                learning_rate=0.0,\n",
    "                current_const=current_const,\n",
    "                terminal_targets=term_use,\n",
    "                loss_weights=loss_weights,\n",
    "            )\n",
    "            tf_dict.pop(self.learning_rate)\n",
    "\n",
    "            loss_value, y0_value = self.sess.run([self.loss, self.Y0_pred], tf_dict)\n",
    "            losses.append(float(loss_value))\n",
    "            y0s.append(float(np.mean(y0_value)))\n",
    "\n",
    "        return {\n",
    "            \"const\": float(current_const),\n",
    "            \"mean_loss\": float(np.mean(losses)),\n",
    "            \"std_loss\": float(np.std(losses)),\n",
    "            \"mean_y0\": float(np.mean(y0s)),\n",
    "            \"std_y0\": float(np.std(y0s)),\n",
    "            \"n_batches\": int(len(losses)),\n",
    "            \"used_batch_pool\": checked_pool is not None,\n",
    "        }\n",
    "\n",
    "    def predict(self, Xi_star, t_star, W_star, const_value=None):\n",
    "        current_const = np.float32(self.const if const_value is None else const_value)\n",
    "        Xi_star = self._prepare_Xi(Xi_star)\n",
    "\n",
    "        tf_dict = {\n",
    "            self.Xi_tf: Xi_star,\n",
    "            self.t_tf: np.asarray(t_star, dtype=np.float32),\n",
    "            self.W_tf: np.asarray(W_star, dtype=np.float32),\n",
    "            self.const_tf: current_const,\n",
    "            self.use_external_terminal_tf: np.float32(0.0),\n",
    "        }\n",
    "\n",
    "        X_star = self.sess.run(self.X_pred, tf_dict)\n",
    "        Y_star = self.sess.run(self.Y_pred, tf_dict)\n",
    "        Z_star = self.sess.run(self.Z_pred, tf_dict)\n",
    "        return X_star, Y_star, Z_star\n",
    "\n",
    "    def export_parameters(self):\n",
    "        return self.sess.run(self.weights + self.biases)\n",
    "\n",
    "    def import_parameters(self, values):\n",
    "        if len(values) != len(self.weights) + len(self.biases):\n",
    "            raise ValueError(\"Invalid number of tensors for import_parameters\")\n",
    "        vars_all = self.weights + self.biases\n",
    "        assign_ops = [var.assign(val) for var, val in zip(vars_all, values)]\n",
    "        self.sess.run(assign_ops)\n",
    "\n",
    "    def get_output_bias(self):\n",
    "        b = self.sess.run(self.biases[-1])\n",
    "        return float(np.mean(b))\n",
    "\n",
    "    def set_output_bias(self, value):\n",
    "        b_shape = self.biases[-1].shape.as_list()\n",
    "        if len(b_shape) != 2 or b_shape[0] != 1:\n",
    "            raise ValueError(f\"Unexpected output bias shape: {b_shape}\")\n",
    "        b_new = np.full((1, b_shape[1]), np.float32(value), dtype=np.float32)\n",
    "        self.sess.run(self.biases[-1].assign(b_new))\n",
    "\n",
    "    @abstractmethod\n",
    "    def phi_tf(self, t, X, Y, Z):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def g_tf(self, X):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def mu_tf(self, t, X, Y, Z):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def sigma_tf(self, t, X, Y):\n",
    "        pass\n",
    "\n",
    "\n",
    "class NN(FBSNN):\n",
    "    def __init__(self, Xi, T, M, N, D, layers, parameters, t_start=0.0, **kwargs):\n",
    "        self.mu1 = parameters[\"mu1\"]\n",
    "        self.mu2 = parameters[\"mu2\"]\n",
    "        self.c1 = parameters[\"c1\"]\n",
    "        self.c2 = parameters[\"c2\"]\n",
    "        self.c3 = parameters[\"c3\"]\n",
    "        self.c4 = parameters[\"c4\"]\n",
    "        self.gamma = parameters[\"gamma\"]\n",
    "        self.s1 = parameters[\"s1\"]\n",
    "        self.s2 = parameters[\"s2\"]\n",
    "        self.s3 = parameters[\"s3\"]\n",
    "        self.x_max = parameters[\"x_max\"]\n",
    "        self.d = parameters[\"d\"]\n",
    "        self.const = parameters[\"const\"]\n",
    "        super().__init__(Xi, T, M, N, D, layers, t_start=t_start, **kwargs)\n",
    "\n",
    "    def psi(self, X_state):\n",
    "        return tf.maximum(\n",
    "            0.0,\n",
    "            tf.minimum(1.0, tf.minimum(X_state / self.d, (self.x_max - X_state) / self.d)),\n",
    "        )\n",
    "\n",
    "    def mu_tf(self, t, X, Y, Z):\n",
    "        S, H, V, X_state = tf.split(X, num_or_size_splits=4, axis=1)\n",
    "        mu1 = tf.cast(self.mu1, tf.float32)\n",
    "        mu2 = tf.cast(self.mu2, tf.float32)\n",
    "        c1 = tf.cast(self.c1, tf.float32)\n",
    "        c2 = tf.cast(self.c2, tf.float32)\n",
    "        c3 = tf.cast(self.c3, tf.float32)\n",
    "        c4 = tf.cast(self.c4, tf.float32)\n",
    "        x_max = tf.cast(self.x_max, tf.float32)\n",
    "\n",
    "        dS = mu1 * (c1 - S)\n",
    "        dH = mu2 * (c2 - H)\n",
    "        dV = c3 * self.psi(-X_state) - c4 * self.psi(X_state - x_max)\n",
    "        dX = V\n",
    "        return tf.concat([dS, dH, dV, dX], axis=1)\n",
    "\n",
    "    def g_tf(self, X):\n",
    "        S, H, V, X_state = tf.split(X, num_or_size_splits=4, axis=1)\n",
    "        gamma = tf.cast(self.gamma, tf.float32)\n",
    "        exp_S = tf.exp(S)\n",
    "        return -gamma * exp_S * X_state\n",
    "\n",
    "    def phi_tf(self, t, X, Y, Z):\n",
    "        S, H, V, X_state = tf.split(X, num_or_size_splits=4, axis=1)\n",
    "        mu1 = tf.cast(self.mu1, tf.float32)\n",
    "        c1 = tf.cast(self.c1, tf.float32)\n",
    "        s1 = tf.cast(self.s1, tf.float32)\n",
    "        gamma = tf.cast(self.gamma, tf.float32)\n",
    "\n",
    "        exp_S = tf.exp(S)\n",
    "        return -gamma * exp_S * (X_state * mu1 * (c1 - S) + V + 0.5 * X_state * s1**2)\n",
    "\n",
    "    def sigma_tf(self, t, X, Y):\n",
    "        S, H, V, X_state = tf.split(X, num_or_size_splits=4, axis=1)\n",
    "        s1 = tf.cast(self.s1, tf.float32)\n",
    "        s2 = tf.cast(self.s2, tf.float32)\n",
    "        s3 = tf.cast(self.s3, tf.float32)\n",
    "\n",
    "        zeros = tf.zeros_like(S)\n",
    "        ones = tf.ones_like(S)\n",
    "\n",
    "        r1 = tf.concat([s1 * ones, zeros, zeros, zeros], axis=1)\n",
    "        r2 = tf.concat([zeros, s2 * ones, zeros, zeros], axis=1)\n",
    "        r3 = tf.concat([zeros, zeros, s3 * ones, zeros], axis=1)\n",
    "        r4 = tf.concat([zeros, zeros, zeros, zeros], axis=1)\n",
    "        return tf.stack([r1, r2, r3, r4], axis=1)\n",
    "\n",
    "\n",
    "def psi_np(x_state, d, x_max):\n",
    "    return np.maximum(0.0, np.minimum(1.0, np.minimum(x_state / d, (x_max - x_state) / d)))\n",
    "\n",
    "\n",
    "def make_antithetic_dW(rng, M, N, D, dt, use_antithetic=True):\n",
    "    dW = np.zeros((M, N, D), dtype=np.float32)\n",
    "    if use_antithetic and M > 1:\n",
    "        half = M // 2\n",
    "        half_w = (np.sqrt(dt) * rng.normal(size=(half, N, D))).astype(np.float32)\n",
    "        dW[:half] = half_w\n",
    "        dW[half : 2 * half] = -half_w\n",
    "        if M % 2 == 1:\n",
    "            dW[-1] = (np.sqrt(dt) * rng.normal(size=(N, D))).astype(np.float32)\n",
    "    else:\n",
    "        dW[:, :, :] = (np.sqrt(dt) * rng.normal(size=(M, N, D))).astype(np.float32)\n",
    "    return dW\n",
    "\n",
    "\n",
    "def simulate_forward_paths_np(\n",
    "    Xi_batch,\n",
    "    params_local,\n",
    "    T_total,\n",
    "    N_total,\n",
    "    seed=1234,\n",
    "    use_antithetic=True,\n",
    "):\n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    M_local, D_local = Xi_batch.shape\n",
    "    dt = T_total / N_total\n",
    "\n",
    "    t_grid = np.linspace(0.0, T_total, N_total + 1, dtype=np.float32)\n",
    "    dW = make_antithetic_dW(rng, M_local, N_total, D_local, dt, use_antithetic)\n",
    "\n",
    "    X_paths = np.zeros((M_local, N_total + 1, D_local), dtype=np.float32)\n",
    "    X_paths[:, 0, :] = Xi_batch.astype(np.float32)\n",
    "\n",
    "    mu1 = float(params_local[\"mu1\"])\n",
    "    mu2 = float(params_local[\"mu2\"])\n",
    "    c1 = float(params_local[\"c1\"])\n",
    "    c2 = float(params_local[\"c2\"])\n",
    "    c3 = float(params_local[\"c3\"])\n",
    "    c4 = float(params_local[\"c4\"])\n",
    "    d_par = float(params_local[\"d\"])\n",
    "    x_max = float(params_local[\"x_max\"])\n",
    "    s1 = float(params_local[\"s1\"])\n",
    "    s2 = float(params_local[\"s2\"])\n",
    "    s3 = float(params_local[\"s3\"])\n",
    "\n",
    "    for n in range(N_total):\n",
    "        Xn = X_paths[:, n, :]\n",
    "        S = Xn[:, 0:1]\n",
    "        H = Xn[:, 1:2]\n",
    "        V = Xn[:, 2:3]\n",
    "        X_state = Xn[:, 3:4]\n",
    "\n",
    "        dS = mu1 * (c1 - S)\n",
    "        dH = mu2 * (c2 - H)\n",
    "        dV = c3 * psi_np(-X_state, d_par, x_max) - c4 * psi_np(X_state - x_max, d_par, x_max)\n",
    "        dX = V\n",
    "        drift = np.concatenate([dS, dH, dV, dX], axis=1)\n",
    "\n",
    "        diffusion = np.zeros_like(Xn)\n",
    "        diffusion[:, 0:1] = s1 * dW[:, n, 0:1]\n",
    "        diffusion[:, 1:2] = s2 * dW[:, n, 1:2]\n",
    "        diffusion[:, 2:3] = s3 * dW[:, n, 2:3]\n",
    "        diffusion[:, 3:4] = 0.0\n",
    "\n",
    "        X_paths[:, n + 1, :] = Xn + drift * dt + diffusion\n",
    "\n",
    "    return t_grid, dW, X_paths\n",
    "\n",
    "\n",
    "def build_block_batch_from_forward(t_grid, dW, i0, i1):\n",
    "    M_local = dW.shape[0]\n",
    "    D_local = dW.shape[2]\n",
    "    n_steps = i1 - i0\n",
    "\n",
    "    t_block = np.zeros((M_local, n_steps + 1, 1), dtype=np.float32)\n",
    "    W_block = np.zeros((M_local, n_steps + 1, D_local), dtype=np.float32)\n",
    "\n",
    "    t_block[:, :, 0] = t_grid[i0 : i1 + 1][None, :]\n",
    "    W_block[:, 1:, :] = np.cumsum(dW[:, i0:i1, :], axis=1)\n",
    "    return t_block, W_block\n",
    "\n",
    "\n",
    "def build_dummy_batch_for_model(model_local):\n",
    "    t_dummy = np.zeros((model_local.M, model_local.N + 1, 1), dtype=np.float32)\n",
    "    local_grid = np.linspace(\n",
    "        float(model_local.t_start),\n",
    "        float(model_local.t_start + model_local.T),\n",
    "        model_local.N + 1,\n",
    "        dtype=np.float32,\n",
    "    )\n",
    "    t_dummy[:, :, 0] = local_grid[None, :]\n",
    "    W_dummy = np.zeros((model_local.M, model_local.N + 1, model_local.D), dtype=np.float32)\n",
    "    return t_dummy, W_dummy\n",
    "\n",
    "\n",
    "def build_forward_scenarios(\n",
    "    Xi_batch,\n",
    "    params,\n",
    "    T_total,\n",
    "    N_total,\n",
    "    base_seed,\n",
    "    n_scenarios,\n",
    "    use_antithetic=True,\n",
    "):\n",
    "    scenarios = []\n",
    "    for k in range(int(n_scenarios)):\n",
    "        seed_k = int(base_seed + 7919 * k)\n",
    "        t_grid, dW, X_paths = simulate_forward_paths_np(\n",
    "            Xi_batch,\n",
    "            params,\n",
    "            T_total,\n",
    "            N_total,\n",
    "            seed=seed_k,\n",
    "            use_antithetic=use_antithetic,\n",
    "        )\n",
    "        scenarios.append(\n",
    "            {\n",
    "                \"seed\": seed_k,\n",
    "                \"t_grid\": t_grid,\n",
    "                \"dW\": dW,\n",
    "                \"X_paths\": X_paths,\n",
    "            }\n",
    "        )\n",
    "    return scenarios\n",
    "\n",
    "\n",
    "def build_block_pool(scenarios, i0, i1, next_model):\n",
    "    pool = []\n",
    "    for sc in scenarios:\n",
    "        Xi_block = sc[\"X_paths\"][:, i0, :]\n",
    "        X_terminal = sc[\"X_paths\"][:, i1, :]\n",
    "        t_block, W_block = build_block_batch_from_forward(sc[\"t_grid\"], sc[\"dW\"], i0, i1)\n",
    "\n",
    "        terminal_targets = None\n",
    "        if next_model is not None:\n",
    "            t_dummy_next, W_dummy_next = build_dummy_batch_for_model(next_model)\n",
    "            _, Y_next, Z_next = next_model.predict(\n",
    "                X_terminal, t_dummy_next, W_dummy_next, const_value=1.0\n",
    "            )\n",
    "            terminal_targets = (Y_next[:, 0, :], Z_next[:, 0, :])\n",
    "\n",
    "        pool.append(\n",
    "            {\n",
    "                \"Xi\": Xi_block,\n",
    "                \"t\": t_block,\n",
    "                \"W\": W_block,\n",
    "                \"terminal_targets\": terminal_targets,\n",
    "            }\n",
    "        )\n",
    "    return pool\n",
    "\n",
    "\n",
    "def compute_block_input_normalization(scenarios, i0, i1, std_floor=1.0e-3):\n",
    "    x_all = []\n",
    "    for sc in scenarios:\n",
    "        x_chunk = sc[\"X_paths\"][:, i0 : i1 + 1, :].reshape(-1, sc[\"X_paths\"].shape[2])\n",
    "        x_all.append(x_chunk)\n",
    "    x_all = np.concatenate(x_all, axis=0)\n",
    "    x_mean = np.mean(x_all, axis=0, keepdims=True).astype(np.float32)\n",
    "    x_std = np.std(x_all, axis=0, keepdims=True).astype(np.float32)\n",
    "    x_std = np.maximum(x_std, np.float32(std_floor)).astype(np.float32)\n",
    "    return x_mean, x_std\n",
    "\n",
    "\n",
    "def partition_blocks(N_total, cfg):\n",
    "    min_steps = int(cfg.get(\"min_steps_per_block\", 1))\n",
    "    requested_blocks = int(cfg[\"n_blocks\"])\n",
    "\n",
    "    target_block_steps = int(cfg.get(\"target_block_steps\", 0))\n",
    "    if target_block_steps > 0:\n",
    "        requested_blocks = max(requested_blocks, int(np.ceil(N_total / target_block_steps)))\n",
    "\n",
    "    max_blocks_from_steps = max(1, N_total // max(1, min_steps))\n",
    "    max_blocks_cfg = int(cfg.get(\"max_blocks\", max_blocks_from_steps))\n",
    "    effective_blocks = min(requested_blocks, max_blocks_from_steps, max_blocks_cfg)\n",
    "\n",
    "    terminal_min_steps = int(cfg.get(\"terminal_min_steps\", min_steps))\n",
    "\n",
    "    base = N_total // effective_blocks\n",
    "    rem = N_total % effective_blocks\n",
    "    steps = [base + (1 if i < rem else 0) for i in range(effective_blocks)]\n",
    "\n",
    "    deficit = max(0, terminal_min_steps - steps[-1])\n",
    "    protect_head_blocks = int(cfg.get(\"protect_head_blocks\", 0))\n",
    "    donor_candidates = [i for i in range(effective_blocks - 1) if i >= protect_head_blocks]\n",
    "    if len(donor_candidates) == 0:\n",
    "        donor_candidates = list(range(effective_blocks - 1))\n",
    "\n",
    "    donor_strategy = str(cfg.get(\"terminal_donor_strategy\", \"tail\")).lower()\n",
    "    if donor_strategy == \"head\":\n",
    "        donor_order = donor_candidates\n",
    "    else:\n",
    "        donor_order = list(reversed(donor_candidates))\n",
    "\n",
    "    for i_donor in donor_order:\n",
    "        if deficit <= 0:\n",
    "            break\n",
    "        spare = max(0, steps[i_donor] - min_steps)\n",
    "        take = min(spare, deficit)\n",
    "        steps[i_donor] -= take\n",
    "        steps[-1] += take\n",
    "        deficit -= take\n",
    "\n",
    "    block_edges = np.zeros(effective_blocks + 1, dtype=int)\n",
    "    for i_step in range(effective_blocks):\n",
    "        block_edges[i_step + 1] = block_edges[i_step] + steps[i_step]\n",
    "\n",
    "    if block_edges[-1] != N_total:\n",
    "        raise ValueError(\n",
    "            f\"invalid block partition: last edge {block_edges[-1]} != N_total {N_total}\"\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"requested_blocks\": requested_blocks,\n",
    "        \"effective_blocks\": effective_blocks,\n",
    "        \"steps\": steps,\n",
    "        \"block_edges\": block_edges,\n",
    "        \"min_steps\": min_steps,\n",
    "        \"terminal_min_steps\": terminal_min_steps,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_recursive_experiment(Xi_seed, N_total, T_total, cfg, label, params, M, D, layers):\n",
    "    Xi_batch_local = np.repeat(Xi_seed.astype(np.float32), M, axis=0)\n",
    "\n",
    "    n_train_scen = int(cfg.get(\"n_forward_train\", 1))\n",
    "    n_eval_scen = int(cfg.get(\"n_forward_eval\", 1))\n",
    "    forward_seed = int(cfg.get(\"seed_forward\", 1234))\n",
    "\n",
    "    train_scenarios = build_forward_scenarios(\n",
    "        Xi_batch=Xi_batch_local,\n",
    "        params=params,\n",
    "        T_total=T_total,\n",
    "        N_total=N_total,\n",
    "        base_seed=forward_seed,\n",
    "        n_scenarios=n_train_scen,\n",
    "        use_antithetic=bool(cfg.get(\"use_antithetic_forward\", True)),\n",
    "    )\n",
    "    eval_scenarios = build_forward_scenarios(\n",
    "        Xi_batch=Xi_batch_local,\n",
    "        params=params,\n",
    "        T_total=T_total,\n",
    "        N_total=N_total,\n",
    "        base_seed=forward_seed + 100000,\n",
    "        n_scenarios=n_eval_scen,\n",
    "        use_antithetic=bool(cfg.get(\"use_antithetic_forward\", True)),\n",
    "    )\n",
    "\n",
    "    part = partition_blocks(N_total=N_total, cfg=cfg)\n",
    "    block_edges = part[\"block_edges\"]\n",
    "    steps = part[\"steps\"]\n",
    "    effective_blocks = part[\"effective_blocks\"]\n",
    "\n",
    "    print(\n",
    "        f\"[RecursiveConfig] label={label}, N_total={N_total}, n_blocks={effective_blocks} (req={part['requested_blocks']}), \"\n",
    "        f\"min_steps_per_block={part['min_steps']}, terminal_min_steps={part['terminal_min_steps']}, \"\n",
    "        f\"iters_per_block={cfg['iters_per_block']}, lr={cfg['lr_per_block']:.1e}, warm_start={cfg['use_warm_start']}, \"\n",
    "        f\"train_scen={n_train_scen}, eval_scen={n_eval_scen}\"\n",
    "    )\n",
    "    print(f\"[RecursivePartition] label={label}, steps={steps}\")\n",
    "\n",
    "    next_model = None\n",
    "    next_params = None\n",
    "    local_logs = []\n",
    "    local_models = []\n",
    "\n",
    "    for b in range(effective_blocks - 1, -1, -1):\n",
    "        i0 = int(block_edges[b])\n",
    "        i1 = int(block_edges[b + 1])\n",
    "        n_steps_block = i1 - i0\n",
    "\n",
    "        t_grid_ref = train_scenarios[0][\"t_grid\"]\n",
    "        T_block = float(t_grid_ref[i1] - t_grid_ref[i0])\n",
    "        t_start_block = float(t_grid_ref[i0])\n",
    "\n",
    "        train_pool = build_block_pool(train_scenarios, i0, i1, next_model)\n",
    "        eval_pool = build_block_pool(eval_scenarios, i0, i1, next_model)\n",
    "\n",
    "        is_terminal_block = train_pool[0][\"terminal_targets\"] is None\n",
    "        Xi_block_ref = train_pool[0][\"Xi\"]\n",
    "        x_norm_mean, x_norm_std = compute_block_input_normalization(\n",
    "            train_scenarios,\n",
    "            i0,\n",
    "            i1,\n",
    "            std_floor=float(cfg.get(\"x_norm_std_floor\", 1.0e-3)),\n",
    "        )\n",
    "\n",
    "        base_loss_weights = {\n",
    "            \"path\": float(cfg.get(\"path_loss_weight\", 1.0)),\n",
    "            \"terminal_y\": float(cfg.get(\"terminal_y_loss_weight\", 10.0)),\n",
    "            \"terminal_z\": float(cfg.get(\"terminal_z_loss_weight\", 1.0)),\n",
    "        }\n",
    "        external_loss_weights = {\n",
    "            \"path\": float(cfg.get(\"external_path_loss_weight\", base_loss_weights[\"path\"])),\n",
    "            \"terminal_y\": float(\n",
    "                cfg.get(\"external_terminal_y_loss_weight\", base_loss_weights[\"terminal_y\"])\n",
    "            ),\n",
    "            \"terminal_z\": float(\n",
    "                cfg.get(\"external_terminal_z_loss_weight\", base_loss_weights[\"terminal_z\"])\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        block_model = NN(\n",
    "            Xi=Xi_block_ref,\n",
    "            T=T_block,\n",
    "            M=M,\n",
    "            N=n_steps_block,\n",
    "            D=D,\n",
    "            layers=layers,\n",
    "            parameters=params,\n",
    "            t_start=t_start_block,\n",
    "            activation=cfg.get(\"activation\", \"sin\"),\n",
    "            clip_grad_norm=cfg.get(\"clip_grad_norm\", 1.0),\n",
    "            use_antithetic_sampling=cfg.get(\"use_antithetic_sampling\", True),\n",
    "            path_loss_weight=base_loss_weights[\"path\"],\n",
    "            terminal_y_loss_weight=base_loss_weights[\"terminal_y\"],\n",
    "            terminal_z_loss_weight=base_loss_weights[\"terminal_z\"],\n",
    "            x_norm_mean=x_norm_mean,\n",
    "            x_norm_std=x_norm_std,\n",
    "            normalize_time=cfg.get(\"normalize_time\", True),\n",
    "        )\n",
    "\n",
    "        warm_start_skip_head_blocks = int(cfg.get(\"warm_start_skip_head_blocks\", 0))\n",
    "        if cfg[\"use_warm_start\"] and (next_params is not None):\n",
    "            if b >= warm_start_skip_head_blocks:\n",
    "                block_model.import_parameters(next_params)\n",
    "            elif bool(cfg.get(\"log_warm_start_skip\", True)):\n",
    "                print(\n",
    "                    f\"[WarmStartSkip] {label} b={b}, \"\n",
    "                    f\"skip_head_blocks={warm_start_skip_head_blocks}\"\n",
    "                )\n",
    "\n",
    "        init_output_bias = bool(cfg.get(\"init_output_bias_from_targets\", True))\n",
    "        init_output_bias_mode = str(cfg.get(\"init_output_bias_mode\", \"all\")).lower()\n",
    "        allow_bias_init = (\n",
    "            init_output_bias\n",
    "            and (\n",
    "                init_output_bias_mode == \"all\"\n",
    "                or (init_output_bias_mode in (\"terminal\", \"terminal_only\") and is_terminal_block)\n",
    "                or (init_output_bias_mode in (\"external\", \"external_only\") and (not is_terminal_block))\n",
    "            )\n",
    "        )\n",
    "        if allow_bias_init:\n",
    "            target_means = []\n",
    "            if is_terminal_block:\n",
    "                for sc in train_scenarios:\n",
    "                    X_terminal = sc[\"X_paths\"][:, i1, :]\n",
    "                    target_means.append(float(np.mean(u_exact_np(params, X_terminal))))\n",
    "            else:\n",
    "                for entry in train_pool:\n",
    "                    tt = entry[\"terminal_targets\"]\n",
    "                    if tt is not None:\n",
    "                        target_means.append(float(np.mean(tt[0])))\n",
    "            if len(target_means) > 0:\n",
    "                target_bias = float(np.mean(target_means))\n",
    "                if cfg.get(\"output_bias_min\", None) is not None:\n",
    "                    target_bias = max(float(cfg[\"output_bias_min\"]), target_bias)\n",
    "                if cfg.get(\"output_bias_max\", None) is not None:\n",
    "                    target_bias = min(float(cfg[\"output_bias_max\"]), target_bias)\n",
    "\n",
    "                default_blend = float(cfg.get(\"output_bias_blend\", 1.0 if is_terminal_block else 0.0))\n",
    "                blend_key = \"output_bias_blend_terminal\" if is_terminal_block else \"output_bias_blend_external\"\n",
    "                blend = float(cfg.get(blend_key, default_blend))\n",
    "                blend = min(1.0, max(0.0, blend))\n",
    "                current_bias = block_model.get_output_bias()\n",
    "                new_bias = (1.0 - blend) * current_bias + blend * target_bias\n",
    "                block_model.set_output_bias(new_bias)\n",
    "                if bool(cfg.get(\"log_output_bias_init\", True)):\n",
    "                    print(\n",
    "                        f\"[InitOutputBias] {label} b={b}, \"\n",
    "                        f\"bias_old={current_bias:.3f}, bias_target={target_bias:.3f}, bias_new={new_bias:.3f}\"\n",
    "                    )\n",
    "\n",
    "        block_iters = int(cfg[\"iters_per_block\"])\n",
    "        block_lr = float(cfg[\"lr_per_block\"])\n",
    "        if is_terminal_block:\n",
    "            block_iters = int(max(1, round(block_iters * float(cfg.get(\"terminal_iters_multiplier\", 1.0)))))\n",
    "            block_lr = float(block_lr * float(cfg.get(\"terminal_lr_multiplier\", 1.0)))\n",
    "\n",
    "        print(\n",
    "            f\"[RecursiveBlock] {label} b={b}, idx=[{i0},{i1}], steps={n_steps_block}, \"\n",
    "            f\"terminal={'external' if not is_terminal_block else 'g'}, iters={block_iters}, lr={block_lr:.1e}\"\n",
    "        )\n",
    "\n",
    "        terminal_stage2_weights = {\n",
    "            \"path\": float(cfg.get(\"terminal_stage2_path_loss_weight\", base_loss_weights[\"path\"])),\n",
    "            \"terminal_y\": float(\n",
    "                cfg.get(\"terminal_stage2_terminal_y_loss_weight\", base_loss_weights[\"terminal_y\"])\n",
    "            ),\n",
    "            \"terminal_z\": float(\n",
    "                cfg.get(\"terminal_stage2_terminal_z_loss_weight\", base_loss_weights[\"terminal_z\"])\n",
    "            ),\n",
    "        }\n",
    "        final_train_weights = external_loss_weights\n",
    "\n",
    "        if is_terminal_block and bool(cfg.get(\"terminal_two_stage\", False)):\n",
    "            stage1_frac = float(cfg.get(\"terminal_stage1_frac\", 0.3))\n",
    "            stage1_iters = int(max(1, round(block_iters * stage1_frac)))\n",
    "            stage2_iters = int(max(1, block_iters - stage1_iters))\n",
    "            stage1_lr = float(block_lr * float(cfg.get(\"terminal_stage1_lr_boost\", 1.0)))\n",
    "            stage2_lr = float(block_lr)\n",
    "            stage1_weights = {\n",
    "                \"path\": float(\n",
    "                    cfg.get(\"terminal_stage1_path_loss_weight\", terminal_stage2_weights[\"path\"])\n",
    "                ),\n",
    "                \"terminal_y\": float(\n",
    "                    cfg.get(\n",
    "                        \"terminal_stage1_terminal_y_loss_weight\",\n",
    "                        terminal_stage2_weights[\"terminal_y\"],\n",
    "                    )\n",
    "                ),\n",
    "                \"terminal_z\": float(\n",
    "                    cfg.get(\n",
    "                        \"terminal_stage1_terminal_z_loss_weight\",\n",
    "                        terminal_stage2_weights[\"terminal_z\"],\n",
    "                    )\n",
    "                ),\n",
    "            }\n",
    "            final_train_weights = terminal_stage2_weights\n",
    "\n",
    "            print(\n",
    "                f\"[TerminalSchedule] {label} b={b}: \"\n",
    "                f\"stage1(iters={stage1_iters}, lr={stage1_lr:.1e}), \"\n",
    "                f\"stage2(iters={stage2_iters}, lr={stage2_lr:.1e})\"\n",
    "            )\n",
    "\n",
    "            block_model.train(\n",
    "                N_Iter=stage1_iters,\n",
    "                learning_rate=stage1_lr,\n",
    "                const_value=1.0,\n",
    "                batch_pool=train_pool,\n",
    "                loss_weights=stage1_weights,\n",
    "            )\n",
    "            train_stats_block = block_model.train(\n",
    "                N_Iter=stage2_iters,\n",
    "                learning_rate=stage2_lr,\n",
    "                const_value=1.0,\n",
    "                batch_pool=train_pool,\n",
    "                loss_weights=terminal_stage2_weights,\n",
    "            )\n",
    "        else:\n",
    "            if is_terminal_block:\n",
    "                final_train_weights = terminal_stage2_weights\n",
    "            train_stats_block = block_model.train(\n",
    "                N_Iter=block_iters,\n",
    "                learning_rate=block_lr,\n",
    "                const_value=1.0,\n",
    "                batch_pool=train_pool,\n",
    "                loss_weights=final_train_weights,\n",
    "            )\n",
    "\n",
    "        exact_y0_vals = []\n",
    "        for sc in eval_scenarios:\n",
    "            exact_y0_vals.append(\n",
    "                np.mean(-params[\"gamma\"] * np.exp(sc[\"X_paths\"][:, i0, 0:1]) * sc[\"X_paths\"][:, i0, 3:4])\n",
    "            )\n",
    "        exact_y0_block = float(np.mean(exact_y0_vals))\n",
    "\n",
    "        eval_stats_block = block_model.evaluate(\n",
    "            const_value=1.0,\n",
    "            batch_pool=eval_pool,\n",
    "            loss_weights=final_train_weights,\n",
    "        )\n",
    "\n",
    "        n_refine_done = 0\n",
    "        if is_terminal_block and int(cfg.get(\"terminal_refine_rounds\", 0)) > 0:\n",
    "            rel_delta = abs(eval_stats_block[\"mean_y0\"] - exact_y0_block) / max(\n",
    "                1.0, abs(exact_y0_block)\n",
    "            )\n",
    "            for rr in range(int(cfg.get(\"terminal_refine_rounds\", 0))):\n",
    "                if rel_delta <= float(cfg.get(\"terminal_refine_delta_rel_threshold\", 0.25)):\n",
    "                    break\n",
    "\n",
    "                n_refine_done += 1\n",
    "                refine_iters = int(\n",
    "                    max(\n",
    "                        1,\n",
    "                        round(\n",
    "                            block_iters\n",
    "                            * float(cfg.get(\"terminal_refine_iters_multiplier\", 0.5))\n",
    "                        ),\n",
    "                    )\n",
    "                )\n",
    "                refine_lr = float(\n",
    "                    block_lr * float(cfg.get(\"terminal_refine_lr_multiplier\", 0.5))\n",
    "                )\n",
    "                refine_weights = {\n",
    "                    \"path\": float(cfg.get(\"terminal_refine_path_loss_weight\", 0.1)),\n",
    "                    \"terminal_y\": float(\n",
    "                        cfg.get(\"terminal_refine_terminal_y_loss_weight\", 120.0)\n",
    "                    ),\n",
    "                    \"terminal_z\": float(\n",
    "                        cfg.get(\"terminal_refine_terminal_z_loss_weight\", 2.0)\n",
    "                    ),\n",
    "                }\n",
    "                final_train_weights = refine_weights\n",
    "                print(\n",
    "                    f\"[TerminalRefine] {label} b={b}, round={rr+1}, \"\n",
    "                    f\"iters={refine_iters}, lr={refine_lr:.1e}, rel_delta={rel_delta:.3f}\"\n",
    "                )\n",
    "                train_stats_block = block_model.train(\n",
    "                    N_Iter=refine_iters,\n",
    "                    learning_rate=refine_lr,\n",
    "                    const_value=1.0,\n",
    "                    batch_pool=train_pool,\n",
    "                    loss_weights=refine_weights,\n",
    "                )\n",
    "                eval_stats_block = block_model.evaluate(\n",
    "                    const_value=1.0,\n",
    "                    batch_pool=eval_pool,\n",
    "                    loss_weights=refine_weights,\n",
    "                )\n",
    "                rel_delta = abs(eval_stats_block[\"mean_y0\"] - exact_y0_block) / max(\n",
    "                    1.0, abs(exact_y0_block)\n",
    "                )\n",
    "\n",
    "        local_logs.append(\n",
    "            {\n",
    "                \"block\": int(b),\n",
    "                \"i0\": int(i0),\n",
    "                \"i1\": int(i1),\n",
    "                \"steps\": int(n_steps_block),\n",
    "                \"train_last_loss\": train_stats_block[\"last_loss\"],\n",
    "                \"eval_loss\": eval_stats_block[\"mean_loss\"],\n",
    "                \"eval_y0\": eval_stats_block[\"mean_y0\"],\n",
    "                \"exact_eval_y0\": exact_y0_block,\n",
    "                \"terminal_mode\": \"external\" if not is_terminal_block else \"g\",\n",
    "                \"train_iters\": int(block_iters),\n",
    "                \"train_lr\": float(block_lr),\n",
    "                \"terminal_rel_delta\": float(\n",
    "                    abs(eval_stats_block[\"mean_y0\"] - exact_y0_block) / max(1.0, abs(exact_y0_block))\n",
    "                ),\n",
    "                \"terminal_refine_rounds_done\": int(n_refine_done),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        local_models.append(block_model)\n",
    "        next_model = block_model\n",
    "        next_params = block_model.export_parameters()\n",
    "\n",
    "    model_t0 = local_models[-1]\n",
    "    ordered_logs = sorted(local_logs, key=lambda x: x[\"block\"])\n",
    "\n",
    "    terminal_losses = [r[\"eval_loss\"] for r in ordered_logs if r[\"terminal_mode\"] == \"g\"]\n",
    "    non_terminal_losses = [\n",
    "        r[\"eval_loss\"] for r in ordered_logs if r[\"terminal_mode\"] != \"g\"\n",
    "    ]\n",
    "\n",
    "    summary = {\n",
    "        \"label\": label,\n",
    "        \"N_total\": int(N_total),\n",
    "        \"mean_eval_loss\": float(np.mean([r[\"eval_loss\"] for r in ordered_logs])),\n",
    "        \"std_eval_loss\": float(np.std([r[\"eval_loss\"] for r in ordered_logs])),\n",
    "        \"mean_rel_delta\": float(np.mean([r[\"terminal_rel_delta\"] for r in ordered_logs])),\n",
    "        \"mean_eval_loss_nonterminal\": float(np.mean(non_terminal_losses))\n",
    "        if len(non_terminal_losses) > 0\n",
    "        else float(\"nan\"),\n",
    "        \"terminal_eval_loss\": float(np.mean(terminal_losses))\n",
    "        if len(terminal_losses) > 0\n",
    "        else float(\"nan\"),\n",
    "        \"block0_eval_y0\": float(ordered_logs[0][\"eval_y0\"]),\n",
    "        \"block0_exact_y0\": float(ordered_logs[0][\"exact_eval_y0\"]),\n",
    "        \"n_blocks\": int(len(ordered_logs)),\n",
    "    }\n",
    "\n",
    "    for m in local_models[:-1]:\n",
    "        m.sess.close()\n",
    "\n",
    "    return summary, ordered_logs, model_t0\n",
    "\n",
    "\n",
    "def u_exact_np(params, X):\n",
    "    S = X[:, 0:1]\n",
    "    X_state = X[:, 3:4]\n",
    "    return -params[\"gamma\"] * np.exp(S) * X_state\n",
    "\n",
    "\n",
    "def z_exact_np(params, X):\n",
    "    S = X[:, 0:1]\n",
    "    X_state = X[:, 3:4]\n",
    "    s1 = params[\"s1\"]\n",
    "    z_s = -params[\"gamma\"] * np.exp(S) * X_state * s1\n",
    "    z_h = np.zeros_like(z_s)\n",
    "    z_v = np.zeros_like(z_s)\n",
    "    z_x = np.zeros_like(z_s)\n",
    "    return np.concatenate([z_s, z_h, z_v, z_x], axis=1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(1234)\n",
    "    tf.set_random_seed(1234)\n",
    "\n",
    "    M = 2048\n",
    "    D = 4\n",
    "\n",
    "    T = 48.0\n",
    "    target_dt = 0.1\n",
    "    N = int(max(10, round(T / target_dt)))\n",
    "\n",
    "    params = {\n",
    "        \"mu1\": np.float32(1.0),\n",
    "        \"mu2\": np.float32(1.0),\n",
    "        \"c1\": np.float32(1.0),\n",
    "        \"c2\": np.float32(1.0),\n",
    "        \"c3\": np.float32(1.0),\n",
    "        \"c4\": np.float32(1.0),\n",
    "        \"gamma\": np.float32(1.0),\n",
    "        \"d\": np.float32(1.0),\n",
    "        \"x_max\": np.float32(1.0),\n",
    "        \"s1\": np.float32(0.5),\n",
    "        \"s2\": np.float32(0.5),\n",
    "        \"s3\": np.float32(0.5),\n",
    "        \"const\": np.float32(0.0),\n",
    "    }\n",
    "\n",
    "    Xi = np.array([np.log(4.0), 0.5, 0.5, 2.0]).reshape(1, D)\n",
    "    layers = [D + 1] + 4 * [256] + [1]\n",
    "\n",
    "    recursive_profile = \"terminal_first\"  # 'quick', 'stable' oppure 'terminal_first'\n",
    "\n",
    "    if recursive_profile == \"quick\":\n",
    "        recursive_cfg = {\n",
    "            \"n_blocks\": 12,\n",
    "            \"target_block_steps\": 20,\n",
    "            \"max_blocks\": 32,\n",
    "            \"iters_per_block\": 120,\n",
    "            \"lr_per_block\": 2.0e-4,\n",
    "            \"seed_forward\": 1234,\n",
    "            \"use_warm_start\": True,\n",
    "            \"min_steps_per_block\": 12,\n",
    "            \"terminal_iters_multiplier\": 4.0,\n",
    "            \"terminal_lr_multiplier\": 0.5,\n",
    "            \"terminal_two_stage\": True,\n",
    "            \"terminal_stage1_frac\": 0.35,\n",
    "            \"terminal_stage1_lr_boost\": 2.0,\n",
    "            \"terminal_min_steps\": 24,\n",
    "            \"n_forward_train\": 3,\n",
    "            \"n_forward_eval\": 1,\n",
    "            \"use_antithetic_forward\": True,\n",
    "            \"activation\": \"sin\",\n",
    "            \"clip_grad_norm\": 2.0,\n",
    "            \"use_antithetic_sampling\": True,\n",
    "            \"normalize_time\": True,\n",
    "            \"x_norm_std_floor\": 1.0e-3,\n",
    "            \"path_loss_weight\": 1.0,\n",
    "            \"terminal_y_loss_weight\": 12.0,\n",
    "            \"terminal_z_loss_weight\": 0.5,\n",
    "            \"terminal_stage1_path_loss_weight\": 0.25,\n",
    "            \"terminal_stage1_terminal_y_loss_weight\": 24.0,\n",
    "            \"terminal_stage1_terminal_z_loss_weight\": 0.75,\n",
    "            \"terminal_stage2_path_loss_weight\": 0.75,\n",
    "            \"terminal_stage2_terminal_y_loss_weight\": 12.0,\n",
    "            \"terminal_stage2_terminal_z_loss_weight\": 0.5,\n",
    "            \"terminal_refine_rounds\": 0,\n",
    "        }\n",
    "    elif recursive_profile == \"stable\":\n",
    "        recursive_cfg = {\n",
    "            \"n_blocks\": 16,\n",
    "            \"target_block_steps\": 16,\n",
    "            \"max_blocks\": 40,\n",
    "            \"iters_per_block\": 220,\n",
    "            \"lr_per_block\": 1.2e-4,\n",
    "            \"seed_forward\": 1234,\n",
    "            \"use_warm_start\": True,\n",
    "            \"min_steps_per_block\": 10,\n",
    "            \"terminal_iters_multiplier\": 8.0,\n",
    "            \"terminal_lr_multiplier\": 0.4,\n",
    "            \"terminal_two_stage\": True,\n",
    "            \"terminal_stage1_frac\": 0.40,\n",
    "            \"terminal_stage1_lr_boost\": 2.0,\n",
    "            \"terminal_min_steps\": 24,\n",
    "            \"n_forward_train\": 6,\n",
    "            \"n_forward_eval\": 2,\n",
    "            \"use_antithetic_forward\": True,\n",
    "            \"activation\": \"swish\",\n",
    "            \"clip_grad_norm\": 3.0,\n",
    "            \"use_antithetic_sampling\": True,\n",
    "            \"normalize_time\": True,\n",
    "            \"x_norm_std_floor\": 1.0e-2,\n",
    "            \"path_loss_weight\": 1.0,\n",
    "            \"terminal_y_loss_weight\": 20.0,\n",
    "            \"terminal_z_loss_weight\": 0.5,\n",
    "            \"terminal_stage1_path_loss_weight\": 0.20,\n",
    "            \"terminal_stage1_terminal_y_loss_weight\": 50.0,\n",
    "            \"terminal_stage1_terminal_z_loss_weight\": 1.0,\n",
    "            \"terminal_stage2_path_loss_weight\": 0.60,\n",
    "            \"terminal_stage2_terminal_y_loss_weight\": 25.0,\n",
    "            \"terminal_stage2_terminal_z_loss_weight\": 0.5,\n",
    "            \"terminal_refine_rounds\": 1,\n",
    "            \"terminal_refine_delta_rel_threshold\": 0.30,\n",
    "            \"terminal_refine_iters_multiplier\": 0.40,\n",
    "            \"terminal_refine_lr_multiplier\": 0.60,\n",
    "            \"terminal_refine_path_loss_weight\": 0.10,\n",
    "            \"terminal_refine_terminal_y_loss_weight\": 80.0,\n",
    "            \"terminal_refine_terminal_z_loss_weight\": 1.5,\n",
    "        }\n",
    "    elif recursive_profile == \"terminal_first\":\n",
    "        recursive_cfg = {\n",
    "            \"n_blocks\": 24,\n",
    "            \"target_block_steps\": 20,\n",
    "            \"max_blocks\": 48,\n",
    "            \"iters_per_block\": 280,\n",
    "            \"lr_per_block\": 8.0e-5,\n",
    "            \"seed_forward\": 1234,\n",
    "            \"use_warm_start\": True,\n",
    "            \"warm_start_skip_head_blocks\": 4,\n",
    "            \"min_steps_per_block\": 12,\n",
    "            \"protect_head_blocks\": 3,\n",
    "            \"terminal_donor_strategy\": \"tail\",\n",
    "            \"terminal_iters_multiplier\": 6.0,\n",
    "            \"terminal_lr_multiplier\": 0.30,\n",
    "            \"terminal_two_stage\": True,\n",
    "            \"terminal_stage1_frac\": 0.45,\n",
    "            \"terminal_stage1_lr_boost\": 1.3,\n",
    "            \"terminal_min_steps\": 28,\n",
    "            \"n_forward_train\": 8,\n",
    "            \"n_forward_eval\": 3,\n",
    "            \"use_antithetic_forward\": True,\n",
    "            \"activation\": \"swish\",\n",
    "            \"clip_grad_norm\": 3.0,\n",
    "            \"use_antithetic_sampling\": True,\n",
    "            \"normalize_time\": True,\n",
    "            \"x_norm_std_floor\": 1.0e-2,\n",
    "            \"path_loss_weight\": 1.0,\n",
    "            \"terminal_y_loss_weight\": 22.0,\n",
    "            \"terminal_z_loss_weight\": 0.70,\n",
    "            \"external_path_loss_weight\": 2.5,\n",
    "            \"external_terminal_y_loss_weight\": 6.0,\n",
    "            \"external_terminal_z_loss_weight\": 0.35,\n",
    "            \"terminal_stage1_path_loss_weight\": 0.10,\n",
    "            \"terminal_stage1_terminal_y_loss_weight\": 80.0,\n",
    "            \"terminal_stage1_terminal_z_loss_weight\": 1.5,\n",
    "            \"terminal_stage2_path_loss_weight\": 0.60,\n",
    "            \"terminal_stage2_terminal_y_loss_weight\": 35.0,\n",
    "            \"terminal_stage2_terminal_z_loss_weight\": 0.8,\n",
    "            \"terminal_refine_rounds\": 1,\n",
    "            \"terminal_refine_delta_rel_threshold\": 0.18,\n",
    "            \"terminal_refine_iters_multiplier\": 0.30,\n",
    "            \"terminal_refine_lr_multiplier\": 0.50,\n",
    "            \"terminal_refine_path_loss_weight\": 0.20,\n",
    "            \"terminal_refine_terminal_y_loss_weight\": 60.0,\n",
    "            \"terminal_refine_terminal_z_loss_weight\": 1.2,\n",
    "            \"init_output_bias_from_targets\": True,\n",
    "            \"init_output_bias_mode\": \"terminal_only\",\n",
    "            \"output_bias_blend\": 0.35,\n",
    "            \"output_bias_blend_terminal\": 0.35,\n",
    "            \"output_bias_blend_external\": 0.0,\n",
    "            \"log_output_bias_init\": True,\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(\"recursive_profile must be 'quick', 'stable' or 'terminal_first'\")\n",
    "\n",
    "    print(\"\\n=== Recursive training (backward in time) ===\")\n",
    "    main_summary, recursive_logs, recursive_model_t0 = run_recursive_experiment(\n",
    "        Xi_seed=Xi,\n",
    "        N_total=N,\n",
    "        T_total=T,\n",
    "        cfg=recursive_cfg,\n",
    "        label=f\"main:{recursive_profile}\",\n",
    "        params=params,\n",
    "        M=M,\n",
    "        D=D,\n",
    "        layers=layers,\n",
    "    )\n",
    "\n",
    "    print(\"=== Recursive Log (compact) ===\")\n",
    "    for row in recursive_logs:\n",
    "        print(\n",
    "            f\"block={row['block']}, idx=[{row['i0']},{row['i1']}], steps={row['steps']}, \"\n",
    "            f\"terminal={row['terminal_mode']}, eval_loss={row['eval_loss']:.3e}, \"\n",
    "            f\"eval_y0={row['eval_y0']:.3f}, exact_y0={row['exact_eval_y0']:.3f}, \"\n",
    "            f\"delta={row['eval_y0'] - row['exact_eval_y0']:.3f}, \"\n",
    "            f\"rel_delta={row['terminal_rel_delta']:.3f}, \"\n",
    "            f\"refine={row['terminal_refine_rounds_done']}\"\n",
    "        )\n",
    "\n",
    "    model = recursive_model_t0\n",
    "\n",
    "    M_plot = model.M\n",
    "    N_plot = model.N\n",
    "\n",
    "    t_test, W_test = model.fetch_minibatch()\n",
    "    X_pred, Y_pred, Z_pred = model.predict(model.Xi, t_test, W_test, const_value=1.0)\n",
    "\n",
    "    Xi_reshaped = X_pred.reshape(-1, D)\n",
    "    Y_exact = u_exact_np(params, Xi_reshaped).reshape(M_plot, N_plot + 1, 1)\n",
    "    Z_exact = z_exact_np(params, Xi_reshaped).reshape(M_plot, N_plot + 1, D)\n",
    "\n",
    "    pred_y0_mean = float(np.mean(Y_pred[:, 0, 0]))\n",
    "    pred_y0_std = float(np.std(Y_pred[:, 0, 0]))\n",
    "    exact_y0 = float(u_exact_np(params, model.Xi[:1])[0, 0])\n",
    "\n",
    "    print(f\"\\nPredicted Y0 mean: {pred_y0_mean:.4f} (std {pred_y0_std:.4f})\")\n",
    "    print(f\"Predicted Y0 path0: {Y_pred[0,0,0]:.4f}\")\n",
    "    print(f\"Exact Y0:           {exact_y0:.4f}\")\n",
    "\n",
    "    n_show = min(5, M_plot)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(t_test[0, :, 0], Y_pred[0, :, 0], \"b\", label=\"Learned Y\")\n",
    "    plt.plot(t_test[0, :, 0], Y_exact[0, :, 0], \"r--\", label=\"Exact Y\")\n",
    "    if n_show > 1:\n",
    "        plt.plot(t_test[1:n_show, :, 0].T, Y_pred[1:n_show, :, 0].T, \"b\", alpha=0.7)\n",
    "        plt.plot(t_test[1:n_show, :, 0].T, Y_exact[1:n_show, :, 0].T, \"r--\", alpha=0.7)\n",
    "    plt.title(f\"Soluzione 4D - blocco iniziale modello (N={N_plot})\")\n",
    "    plt.xlabel(\"Tempo t\")\n",
    "    plt.ylabel(\"Y_t\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    eps = 1e-8\n",
    "    rel_err_Z = np.abs((Z_pred - Z_exact) / (np.abs(Z_exact) + eps))\n",
    "    abs_err_Z = np.abs(Z_pred - Z_exact)\n",
    "\n",
    "    mean_rel_err_Z = np.mean(rel_err_Z, axis=0)\n",
    "    mean_abs_err_Z = np.mean(abs_err_Z, axis=0)\n",
    "\n",
    "    z_scale = np.mean(np.abs(Z_exact), axis=(0, 1))\n",
    "    near_zero_target = z_scale < 1e-5\n",
    "\n",
    "    labels = [\"Z_S\", \"Z_H\", \"Z_V\", \"Z_X\"]\n",
    "    colors = [\"b\", \"g\", \"r\", \"m\"]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for d_idx in range(D):\n",
    "        if near_zero_target[d_idx]:\n",
    "            plt.plot(\n",
    "                t_test[0, :, 0],\n",
    "                mean_abs_err_Z[:, d_idx],\n",
    "                colors[d_idx],\n",
    "                label=f\"Mean Abs Error {labels[d_idx]}\",\n",
    "            )\n",
    "        else:\n",
    "            plt.plot(\n",
    "                t_test[0, :, 0],\n",
    "                mean_rel_err_Z[:, d_idx],\n",
    "                colors[d_idx],\n",
    "                label=f\"Mean Rel Error {labels[d_idx]}\",\n",
    "            )\n",
    "    plt.yscale(\"log\")\n",
    "    plt.title(\"Errore medio di Z (metrica mista rel/abs, scala log)\")\n",
    "    plt.xlabel(\"Tempo t\")\n",
    "    plt.ylabel(\"Errore medio\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    # Consiglio pratico: se il terminal block resta dominante, alza terminal_y_loss_weight\n",
    "    # e terminal_iters_multiplier nel profilo scelto.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
